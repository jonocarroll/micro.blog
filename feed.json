{
	"version": "https://jsonfeed.org/version/1",
	"title": "Jonathan Carroll's micro blog",
	"icon": "https://micro.blog/jonocarroll/avatar.jpg",
	"home_page_url": "https://jcarroll.xyz/",
	"feed_url": "https://jcarroll.xyz/feed.json",
	"items": [
		
			{
				"id": "http://jonocarroll.micro.blog/2024/09/18/bringing-type-checking.html",
				"title": "Bringing type checking to R",
				"content_html": "<p>Another &lsquo;toot that became too long&rsquo; post, this time about dynamically vs strictly typed languages. <a href=\"https://vapour.run/\">Vapour</a> is a new language that just dropped that has strict typing and transpiles to R (dynamic types) which means developers can write and refactor code with all the benefits of a type-checker and still end up with plain R code. I love the idea and have been playing around with it pretty much since I heard about it.</p>\n<p>Type checks aren&rsquo;t completely new to R - there are packages to include type annotations; <a href=\"https://github.com/moodymudskipper/typed\">{typed}</a>, <a href=\"https://github.com/jimhester/types\">{types}</a>, and others, but a new language layer (very much in the style of what Typescript brings to Javascript) is a cool new approach.</p>\n<p>The language is very new, so there&rsquo;s bugs, but a lot of things work (well) so there&rsquo;s more than enough to do some pondering about type safety.</p>\n<p>In <a href=\"https://jcarroll.com.au/2024/09/05/side-by-side-comparison-gleam-vs-r/\">this post</a> I compared a full toy project in a strictly typed language (Gleam) vs R and was left a little underwhelmed by the benefits of type safety - I couldn&rsquo;t identify a line of the code where I thought &ldquo;type-safety saves the day right here&rdquo; and in the <a href=\"https://vapour.run/blog/vapour-release/\">release blog post</a> for Vapour John spells out what I think I overlooked - the type safety helps you build the code properly, and connect the functions together in the correct way, and think about whether values might be missing or not, but a really significant benefit comes when you have already built something and want to <em>change</em> it.</p>\n<p>I didn&rsquo;t refactor anything in the Gleam vs R post, and if I did, I suspect <em>that</em> is where the &lsquo;saved the day&rsquo; moment would have come.</p>\n<p>A friend of mine presented a talk recently about functional programming and made the analogy of function return types to Lego bricks; the right combinations of studs (types) help you (the developer) understand which functions can be joined together. In terms of refactoring, replacing a piece of Lego with a particular shape means finding all the right connections on all the sides, otherwise it all falls apart.</p>\n<p>Looking back to the Gleam post, if I wanted to change the <code>Config</code> then all the places where that type is used would be checked to ensure that it still makes sense to have the structure I changed to. Without type checking, that could get quite involved, indeed.</p>\n<p>One thing that the Vapour approach <em>doesn&rsquo;t</em> give you (and neither does Typescript) is runtime type checking. The code generated by the transpiler has no type annotations (because R/Javascript don&rsquo;t have them) so, while the <em>internal</em> functions to a package can be validated to work together, once you allow a user (or third-party developer) to interact with that code, all bets are off. I initially wrote in my Gleam vs R post about the need for defensive programming - lots of assertions at the top of a function body to assert that the inputs are as expected - and I think this still applies because they need to be runtime checks. Now, that leads to the question of whether something like Vapour could <em>add those</em> given that it needs to have defined types to compile, so part of that could be to just insert a <code>stopifnot(is.character(arg1))</code> into the generated function body. Sure, you might prefer a <code>checkmate::assert_string(arg1)</code> but maybe that could be customisable.</p>\n<p>Lots of fun ideas to play with! John has done an amazing job with Vapour - I&rsquo;ve been digging through the Go code to play with things and there&rsquo;s a <em>lot</em> of work that&rsquo;s gone into this. Playing with this has really helped me understand specifically where the benefits of type safety are.</p>\n",
				"content_text": "Another 'toot that became too long' post, this time about dynamically vs strictly typed languages. [Vapour](https://vapour.run/) is a new language that just dropped that has strict typing and transpiles to R (dynamic types) which means developers can write and refactor code with all the benefits of a type-checker and still end up with plain R code. I love the idea and have been playing around with it pretty much since I heard about it.\r\n\r\nType checks aren't completely new to R - there are packages to include type annotations; [{typed}](https://github.com/moodymudskipper/typed), [{types}](https://github.com/jimhester/types), and others, but a new language layer (very much in the style of what Typescript brings to Javascript) is a cool new approach.\r\n\r\nThe language is very new, so there's bugs, but a lot of things work (well) so there's more than enough to do some pondering about type safety.\r\n\r\nIn [this post](https://jcarroll.com.au/2024/09/05/side-by-side-comparison-gleam-vs-r/) I compared a full toy project in a strictly typed language (Gleam) vs R and was left a little underwhelmed by the benefits of type safety - I couldn't identify a line of the code where I thought \"type-safety saves the day right here\" and in the [release blog post](https://vapour.run/blog/vapour-release/) for Vapour John spells out what I think I overlooked - the type safety helps you build the code properly, and connect the functions together in the correct way, and think about whether values might be missing or not, but a really significant benefit comes when you have already built something and want to _change_ it. \r\n\r\nI didn't refactor anything in the Gleam vs R post, and if I did, I suspect _that_ is where the 'saved the day' moment would have come.\r\n\r\nA friend of mine presented a talk recently about functional programming and made the analogy of function return types to Lego bricks; the right combinations of studs (types) help you (the developer) understand which functions can be joined together. In terms of refactoring, replacing a piece of Lego with a particular shape means finding all the right connections on all the sides, otherwise it all falls apart.\r\n\r\nLooking back to the Gleam post, if I wanted to change the `Config` then all the places where that type is used would be checked to ensure that it still makes sense to have the structure I changed to. Without type checking, that could get quite involved, indeed.\r\n\r\nOne thing that the Vapour approach _doesn't_ give you (and neither does Typescript) is runtime type checking. The code generated by the transpiler has no type annotations (because R/Javascript don't have them) so, while the _internal_ functions to a package can be validated to work together, once you allow a user (or third-party developer) to interact with that code, all bets are off. I initially wrote in my Gleam vs R post about the need for defensive programming - lots of assertions at the top of a function body to assert that the inputs are as expected - and I think this still applies because they need to be runtime checks. Now, that leads to the question of whether something like Vapour could _add those_ given that it needs to have defined types to compile, so part of that could be to just insert a `stopifnot(is.character(arg1))` into the generated function body. Sure, you might prefer a `checkmate::assert_string(arg1)` but maybe that could be customisable.\r\n\r\nLots of fun ideas to play with! John has done an amazing job with Vapour - I've been digging through the Go code to play with things and there's a *lot* of work that's gone into this. Playing with this has really helped me understand specifically where the benefits of type safety are.\r\n",
				"date_published": "2024-09-18T09:34:41+09:30",
				"url": "https://jcarroll.xyz/2024/09/18/bringing-type-checking.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2024/08/19/leap-years-with.html",
				"title": "Leap Years with GCD in various languages",
				"content_html": "<p>I saw <a href=\"https://fpilluminated.com/allSlides/238\">this deck</a> via <a href=\"https://discu.eu/weekly/haskell/2024/33/\">Haskell Weekly Recap</a> which noted an interesting property:</p>\n<blockquote>\n<p>&ldquo;x is a leap year if and only if <code>gcd(80, x) &gt; gcd(50, x)</code>&rdquo;</p>\n</blockquote>\n<p>(gcd = &lsquo;greatest common divisor&rsquo;) which I haven&rsquo;t fully appreciated the origin of, but wanted to try some implementations. The deck shows a Haskell version</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-haskell\" data-lang=\"haskell\"><span style=\"color:#a6e22e\">leapyear</span> <span style=\"color:#f92672\">=</span> liftA2 (<span style=\"color:#f92672\">&gt;</span>) (gcd <span style=\"color:#ae81ff\">80</span>) (gcd <span style=\"color:#ae81ff\">50</span>)\n<span style=\"color:#a6e22e\">filter</span> leapyear [<span style=\"color:#ae81ff\">1890</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">1920</span>]\n[<span style=\"color:#ae81ff\">1892</span>,<span style=\"color:#ae81ff\">1896</span>,<span style=\"color:#ae81ff\">1904</span>,<span style=\"color:#ae81ff\">1908</span>,<span style=\"color:#ae81ff\">1912</span>,<span style=\"color:#ae81ff\">1916</span>,<span style=\"color:#ae81ff\">1920</span>]\n</code></pre></div><p>and it took me a few minutes to fully grasp, but once I did, I&rsquo;m very happy with it. I recently worked through <a href=\"https://adueck.github.io/blog/functors-applicatives-and-monads-with-pictures-in-typescript/\">this fantastic post</a> explaining Functors, Applicatives, and Monads using a mix of Haskell and Typescript and it actually sunk in.</p>\n<p>What other languages could I try? Of course, I had to try R</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">leapyear <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(year) {\n  pracma<span style=\"color:#f92672\">::</span><span style=\"color:#a6e22e\">gcd</span>(<span style=\"color:#ae81ff\">80</span>, year) <span style=\"color:#f92672\">&gt;</span> pracma<span style=\"color:#f92672\">::</span><span style=\"color:#a6e22e\">gcd</span>(<span style=\"color:#ae81ff\">50</span>, year)\n}\n<span style=\"color:#a6e22e\">Filter</span>(leapyear, <span style=\"color:#ae81ff\">1890</span><span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">1920</span>)\n<span style=\"color:#75715e\">#&gt; [1] 1892 1896 1904 1908 1912 1916 1920</span>\n</code></pre></div><p>No point-free coding here, we need an actual argument to the function. It&rsquo;s also disappointing that R doesn&rsquo;t have a built-in greatest common divisor, so calling out to a package for that one. Otherwise, I&rsquo;m not disappointed by this.</p>\n<p>This brought me to wanting to try it in (Dylog) APL which <em>does</em> have a built-in GCD (<code>∨</code>) and does tacit/point-free quite nicely</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      leapyear <span style=\"color:#66d9ef\">←</span> (<span style=\"color:#ae81ff\">80</span><span style=\"color:#f92672\">∨⊣</span>) <span style=\"color:#f92672\">&gt;</span> <span style=\"color:#ae81ff\">50</span><span style=\"color:#f92672\">∨⊢</span>\n      years <span style=\"color:#66d9ef\">←</span> <span style=\"color:#ae81ff\">1890</span><span style=\"color:#f92672\">+⍳</span><span style=\"color:#ae81ff\">30</span>\n      (leapyear years)<span style=\"color:#a6e22e\">⌿</span>years\n<span style=\"color:#ae81ff\">1892</span> <span style=\"color:#ae81ff\">1896</span> <span style=\"color:#ae81ff\">1904</span> <span style=\"color:#ae81ff\">1908</span> <span style=\"color:#ae81ff\">1912</span> <span style=\"color:#ae81ff\">1916</span> <span style=\"color:#ae81ff\">1920</span>\n</code></pre></div><p>It took me a few attempts to get the tacit bits working but this is also quite nice, I feel.</p>\n<p>Fun stuff!!!</p>\n<p><em>Edit:</em> After a really interesting meetup with some J programmers, I realised I can use bind (<code>∘</code>) to make this properly tacit in APL</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">leapyear<span style=\"color:#66d9ef\">←</span>(<span style=\"color:#ae81ff\">80</span><span style=\"color:#a6e22e\">∘</span><span style=\"color:#f92672\">∨</span>)<span style=\"color:#f92672\">&gt;</span>(<span style=\"color:#ae81ff\">50</span><span style=\"color:#a6e22e\">∘</span><span style=\"color:#f92672\">∨</span>)\n</code></pre></div>",
				"content_text": "I saw [this deck](https://fpilluminated.com/allSlides/238) via [Haskell Weekly Recap](https://discu.eu/weekly/haskell/2024/33/) which noted an interesting property:\n\n> \"x is a leap year if and only if `gcd(80, x) > gcd(50, x)`\"\n\n(gcd = 'greatest common divisor') which I haven't fully appreciated the origin of, but wanted to try some implementations. The deck shows a Haskell version\n\n```haskell\nleapyear = liftA2 (>) (gcd 80) (gcd 50)\nfilter leapyear [1890..1920]\n[1892,1896,1904,1908,1912,1916,1920]\n```\nand it took me a few minutes to fully grasp, but once I did, I'm very happy with it. I recently worked through [this fantastic post](https://adueck.github.io/blog/functors-applicatives-and-monads-with-pictures-in-typescript/) explaining Functors, Applicatives, and Monads using a mix of Haskell and Typescript and it actually sunk in. \n\nWhat other languages could I try? Of course, I had to try R\n\n```r\nleapyear <- function(year) {\n  pracma::gcd(80, year) > pracma::gcd(50, year)\n}\nFilter(leapyear, 1890:1920)\n#> [1] 1892 1896 1904 1908 1912 1916 1920\n```\nNo point-free coding here, we need an actual argument to the function. It's also disappointing that R doesn't have a built-in greatest common divisor, so calling out to a package for that one. Otherwise, I'm not disappointed by this.\n\nThis brought me to wanting to try it in (Dylog) APL which _does_ have a built-in GCD (`∨`) and does tacit/point-free quite nicely\n\n```apl\n      leapyear ← (80∨⊣) > 50∨⊢\n      years ← 1890+⍳30\n      (leapyear years)⌿years\n1892 1896 1904 1908 1912 1916 1920\n``` \n\nIt took me a few attempts to get the tacit bits working but this is also quite nice, I feel.\n\nFun stuff!!!\n\n*Edit:* After a really interesting meetup with some J programmers, I realised I can use bind (`∘`) to make this properly tacit in APL\n\n```apl\nleapyear←(80∘∨)>(50∘∨)\n``` \n",
				"date_published": "2024-08-19T18:50:21+09:30",
				"url": "https://jcarroll.xyz/2024/08/19/leap-years-with.html",
				"tags": ["R","APL","Haskell"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2024/08/04/r-function-call.html",
				"title": "R function call overhead",
				"content_html": "<p>I&rsquo;m entirely perplexed by this - I was exploring an idea I saw discussed in a blog post (that will have to wait for now - I&rsquo;ll come back to it) and found some interesting behaviour I don&rsquo;t understand.</p>\n<p>I managed to boil the issue down to the following: let&rsquo;s say I have a function that does something with all the values of an array, in this case adds 1 to each element (not returning it). I might define that like this</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">add_inline <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) { <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#a6e22e\">seq_along</span>(z)) z[i] <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span> }\n</code></pre></div><p>Sure, I could do something more &ldquo;functional&rdquo; but that&rsquo;s the point and I&rsquo;ll come back to that.</p>\n<p>Now, what if I add a single layer of abstraction - the adding of 1 is defined in a function</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">f <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(x) x <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span>\nadd_call <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) { <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#a6e22e\">seq_along</span>(z)) <span style=\"color:#a6e22e\">f</span>(z[i]) }\n</code></pre></div><p>I thought perhaps that the scoping of that function, in that R needs to look through all the loaded packages before finding it in the global scope might matter, so I also added a version where <code>f</code> is defined more locally</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">add_call_local <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) {\n  localf <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(x) x <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span>\n  <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#a6e22e\">seq_along</span>(z)) {\n    <span style=\"color:#a6e22e\">localf</span>(z[i])\n  }\n}\n</code></pre></div><p>Just in case there was any funny business with named function lookup, how about an anonymous function version?</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">add_call_anon <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) {\n  <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#a6e22e\">seq_along</span>(z)) {\n    (<span style=\"color:#a6e22e\">\\</span>(x) x <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span>)(z[i])\n  }\n}\n</code></pre></div><p>Okay, now let&rsquo;s look at the comparative performance of each of these</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">n <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#ae81ff\">1e4</span>\nbm <span style=\"color:#f92672\">&lt;-</span> bench<span style=\"color:#f92672\">::</span><span style=\"color:#a6e22e\">mark</span>(\n  inline <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add_inline</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  call <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add_call</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  local <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add_call_local</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  anon <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add_call_anon</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  iterations <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">100</span>\n)\n<span style=\"color:#a6e22e\">plot</span>(bm)\n</code></pre></div><p><img src=\"https://i.imgur.com/B5jRwTO.png\" alt=\"\"><!-- raw HTML omitted --></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">bm[, <span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">7</span>]\n<span style=\"color:#75715e\">#&gt; # A tibble: 4 × 6</span>\n<span style=\"color:#75715e\">#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`</span>\n<span style=\"color:#75715e\">#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;</span>\n<span style=\"color:#75715e\">#&gt; 1 inline     414.73µs 444.43µs     2178.    3.94MB      0  </span>\n<span style=\"color:#75715e\">#&gt; 2 call         5.38ms   5.83ms      169.  113.74KB     20.8</span>\n<span style=\"color:#75715e\">#&gt; 3 local        5.22ms    5.6ms      174.   76.47KB     19.3</span>\n<span style=\"color:#75715e\">#&gt; 4 anon         5.85ms   6.29ms      157.   35.06KB     23.4</span>\n</code></pre></div><p>What? There&rsquo;s a 10x difference between inlining <code>x + 1</code> and calling out to a function.</p>\n<p>How does that scale? I made a similar comparison with the addition five times</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">add5_inline <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) { <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#a6e22e\">seq_along</span>(z)) z[i] <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span>}\nadd5_call <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) { <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#a6e22e\">seq_along</span>(z)) <span style=\"color:#a6e22e\">f</span>(<span style=\"color:#a6e22e\">f</span>(<span style=\"color:#a6e22e\">f</span>(<span style=\"color:#a6e22e\">f</span>(<span style=\"color:#a6e22e\">f</span>(z[i]))))) }\n\nn <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#ae81ff\">1e4</span>\nbm5 <span style=\"color:#f92672\">&lt;-</span> bench<span style=\"color:#f92672\">::</span><span style=\"color:#a6e22e\">mark</span>(\n  inline <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add_inline</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  call <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add_call</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  inline5 <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add5_inline</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  call5 <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add5_call</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  iterations <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">100</span>\n)\n<span style=\"color:#a6e22e\">plot</span>(bm5)\n</code></pre></div><p><img src=\"https://i.imgur.com/esmKs7U.png\" alt=\"\"><!-- raw HTML omitted --></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">bm5[, <span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">7</span>]\n<span style=\"color:#75715e\">#&gt; # A tibble: 4 × 6</span>\n<span style=\"color:#75715e\">#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`</span>\n<span style=\"color:#75715e\">#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;</span>\n<span style=\"color:#75715e\">#&gt; 1 inline      509.3µs   2.06ms     511.         0B     0   </span>\n<span style=\"color:#75715e\">#&gt; 2 call          5.3ms    5.6ms     125.         0B     8.00</span>\n<span style=\"color:#75715e\">#&gt; 3 inline5       746µs 773.44µs    1237.     38.1KB     0   </span>\n<span style=\"color:#75715e\">#&gt; 4 call5        20.5ms  21.53ms      46.2    18.5KB    13.8</span>\n</code></pre></div><p>That&rsquo;s nearly 30x difference.</p>\n<p>Now, that&rsquo;s over 10,000 calls, so it&rsquo;s not a whole lot for a <em>single</em> call, but for many, it adds up! Especially inside a hot loop.</p>\n<p>What if we actually do something with the values? I added in a <code>sapply</code> version but it <em>has</em> to take a function, so there&rsquo;s no strict comparison to inlining it</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">add3_inline <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) { a <span style=\"color:#f92672\">&lt;-</span> z[NA]; <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#a6e22e\">seq_along</span>(z)) a[i] <span style=\"color:#f92672\">&lt;-</span> z[i] <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#f92672\">+</span> <span style=\"color:#ae81ff\">1</span>; a }\nf3 <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(x) <span style=\"color:#a6e22e\">f</span>(<span style=\"color:#a6e22e\">f</span>(<span style=\"color:#a6e22e\">f</span>(x)))\nadd3_call <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) { a <span style=\"color:#f92672\">&lt;-</span> z[NA]; <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#a6e22e\">seq_along</span>(z)) a[i] <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">f3</span>(z[i]); a }\nadd3_sapply_call <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(z) { <span style=\"color:#a6e22e\">sapply</span>(z, f3) }\n\nn <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#ae81ff\">1e4</span>\nbms <span style=\"color:#f92672\">&lt;-</span> bench<span style=\"color:#f92672\">::</span><span style=\"color:#a6e22e\">mark</span>(\n  inline <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add3_inline</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  call <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add3_call</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  sapply_call <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add3_sapply_call</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  iterations <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">100</span>\n)\n<span style=\"color:#a6e22e\">plot</span>(bms)\n</code></pre></div><p><img src=\"https://i.imgur.com/7wWxAWp.png\" alt=\"\"><!-- raw HTML omitted --></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">bms[, <span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">7</span>]\n<span style=\"color:#75715e\">#&gt; # A tibble: 3 × 6</span>\n<span style=\"color:#75715e\">#&gt;   expression       min   median `itr/sec` mem_alloc `gc/sec`</span>\n<span style=\"color:#75715e\">#&gt;   &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;</span>\n<span style=\"color:#75715e\">#&gt; 1 inline       991.8µs   1.08ms     897.      186KB      0  </span>\n<span style=\"color:#75715e\">#&gt; 2 call            18ms  18.73ms      52.4     184KB     12.3</span>\n<span style=\"color:#75715e\">#&gt; 3 sapply_call   19.2ms  21.19ms      46.8     363KB     13.2</span>\n</code></pre></div><p>Still, 18x?</p>\n<p>I don&rsquo;t know how to explain this apart from &ldquo;there&rsquo;s a not-insignificant overhead to calling a function vs inlining the body&rdquo;. Have I missed something?</p>\n<p>R is usually &ldquo;fast enough&rdquo; for everything I do - I&rsquo;m rarely trying to cram a lot of evaluations inside a given millisecond, but I was surprised that this single abstraction added so much. I&rsquo;m imagining a heavy calculation inside a hot loop involving a lot of composed function calls and wondering how much could be saved by consolidating the composition into a single function body?</p>\n<p>Just for context of how &ldquo;slow&rdquo; this is, I wanted to see how Rust performs and &hellip; ouch.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">rextendr<span style=\"color:#f92672\">::</span><span style=\"color:#a6e22e\">rust_function</span>(<span style=\"color:#e6db74\">&#34;fn add3_rust(z:Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { z.iter().map(|x| x + 3).collect() }&#34;</span>, profile <span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">&#34;release&#34;</span>)\n<span style=\"color:#75715e\">#&gt; ℹ build directory: &#39;/tmp/RtmpqsZ3jP/file73e7742716b8f&#39;</span>\n<span style=\"color:#75715e\">#&gt; ✔ Writing &#39;/tmp/RtmpqsZ3jP/file73e7742716b8f/target/extendr_wrappers.R&#39;</span>\n\nn <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#ae81ff\">1e4</span>\nbmr <span style=\"color:#f92672\">&lt;-</span> bench<span style=\"color:#f92672\">::</span><span style=\"color:#a6e22e\">mark</span>(\n  inline <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add3_inline</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  call <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add3_call</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  sapply_call <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add3_sapply_call</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  rust <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">add3_rust</span>(<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span>n),\n  iterations <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">100</span>\n)\n<span style=\"color:#a6e22e\">plot</span>(bmr)\n</code></pre></div><p><img src=\"https://i.imgur.com/ElECsgG.png\" alt=\"\"><!-- raw HTML omitted --></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">bmr[, <span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">7</span>]\n<span style=\"color:#75715e\">#&gt; # A tibble: 4 × 6</span>\n<span style=\"color:#75715e\">#&gt;   expression       min   median `itr/sec` mem_alloc `gc/sec`</span>\n<span style=\"color:#75715e\">#&gt;   &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;</span>\n<span style=\"color:#75715e\">#&gt; 1 inline        1.03ms   1.08ms     925.    156.4KB      0  </span>\n<span style=\"color:#75715e\">#&gt; 2 call         18.52ms  21.53ms      41.7   156.4KB     11.8</span>\n<span style=\"color:#75715e\">#&gt; 3 sapply_call  19.79ms   23.6ms      41.8   362.6KB     13.2</span>\n<span style=\"color:#75715e\">#&gt; 4 rust         18.37µs  24.21µs   14165.     78.2KB      0</span>\n</code></pre></div><p>Yeah, my best R is 40x slower than Rust (the <code>f3(x)</code> R version 875x slower than Rust) at this particular example. I love the idea of calling out to Rust for performance, and will definitely be doing that a bit more.</p>\n<p>If you have an explanation about the function calling overhead, please let me know here or on <a href=\"https://fosstodon.org/@jonocarroll\">Mastodon</a>.</p>\n",
				"content_text": "I'm entirely perplexed by this - I was exploring an idea I saw discussed in a blog post (that will have to wait for now - I'll come back to it) and found some interesting behaviour I don't understand. \n\nI managed to boil the issue down to the following: let's say I have a function that does something with all the values of an array, in this case adds 1 to each element (not returning it). I might define that like this\n\n```r\nadd_inline <- function(z) { for (i in seq_along(z)) z[i] + 1 }\n```\n\nSure, I could do something more \"functional\" but that's the point and I'll come back to that.\n\nNow, what if I add a single layer of abstraction - the adding of 1 is defined in a function\n\n```r\nf <- function(x) x + 1\nadd_call <- function(z) { for (i in seq_along(z)) f(z[i]) }\n```\n\nI thought perhaps that the scoping of that function, in that R needs to look through all the loaded packages before finding it in the global scope might matter, so I also added a version where `f` is defined more locally\n\n```r\nadd_call_local <- function(z) {\n  localf <- function(x) x + 1\n  for (i in seq_along(z)) {\n    localf(z[i])\n  }\n}\n```\n\nJust in case there was any funny business with named function lookup, how about an anonymous function version?\n\n```r\nadd_call_anon <- function(z) {\n  for (i in seq_along(z)) {\n    (\\(x) x + 1)(z[i])\n  }\n}\n```\n\nOkay, now let's look at the comparative performance of each of these\n\n```r\nn <- 1e4\nbm <- bench::mark(\n  inline = add_inline(1:n),\n  call = add_call(1:n),\n  local = add_call_local(1:n),\n  anon = add_call_anon(1:n),\n  iterations = 100\n)\nplot(bm)\n```\n\n![](https://i.imgur.com/B5jRwTO.png)<!-- -->\n\n``` r\nbm[, 1:7]\n#> # A tibble: 4 × 6\n#>   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#>   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n#> 1 inline     414.73µs 444.43µs     2178.    3.94MB      0  \n#> 2 call         5.38ms   5.83ms      169.  113.74KB     20.8\n#> 3 local        5.22ms    5.6ms      174.   76.47KB     19.3\n#> 4 anon         5.85ms   6.29ms      157.   35.06KB     23.4\n```\n\nWhat? There's a 10x difference between inlining `x + 1` and calling out to a function. \n\nHow does that scale? I made a similar comparison with the addition five times\n\n```r\nadd5_inline <- function(z) { for (i in seq_along(z)) z[i] + 1 + 1 + 1 + 1 + 1}\nadd5_call <- function(z) { for (i in seq_along(z)) f(f(f(f(f(z[i]))))) }\n\nn <- 1e4\nbm5 <- bench::mark(\n  inline = add_inline(1:n),\n  call = add_call(1:n),\n  inline5 = add5_inline(1:n),\n  call5 = add5_call(1:n),\n  iterations = 100\n)\nplot(bm5)\n```\n\n![](https://i.imgur.com/esmKs7U.png)<!-- -->\n\n``` r\nbm5[, 1:7]\n#> # A tibble: 4 × 6\n#>   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#>   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n#> 1 inline      509.3µs   2.06ms     511.         0B     0   \n#> 2 call          5.3ms    5.6ms     125.         0B     8.00\n#> 3 inline5       746µs 773.44µs    1237.     38.1KB     0   \n#> 4 call5        20.5ms  21.53ms      46.2    18.5KB    13.8\n```\n\nThat's nearly 30x difference.\n\nNow, that's over 10,000 calls, so it's not a whole lot for a _single_ call, but for many, it adds up! Especially inside a hot loop.\n\nWhat if we actually do something with the values? I added in a `sapply` version but it _has_ to take a function, so there's no strict comparison to inlining it\n\n```r\nadd3_inline <- function(z) { a <- z[NA]; for (i in seq_along(z)) a[i] <- z[i] + 1 + 1 + 1; a }\nf3 <- function(x) f(f(f(x)))\nadd3_call <- function(z) { a <- z[NA]; for (i in seq_along(z)) a[i] <- f3(z[i]); a }\nadd3_sapply_call <- function(z) { sapply(z, f3) }\n\nn <- 1e4\nbms <- bench::mark(\n  inline = add3_inline(1:n),\n  call = add3_call(1:n),\n  sapply_call = add3_sapply_call(1:n),\n  iterations = 100\n)\nplot(bms)\n```\n\n![](https://i.imgur.com/7wWxAWp.png)<!-- -->\n\n``` r\nbms[, 1:7]\n#> # A tibble: 3 × 6\n#>   expression       min   median `itr/sec` mem_alloc `gc/sec`\n#>   <bch:expr>  <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n#> 1 inline       991.8µs   1.08ms     897.      186KB      0  \n#> 2 call            18ms  18.73ms      52.4     184KB     12.3\n#> 3 sapply_call   19.2ms  21.19ms      46.8     363KB     13.2\n```\n\nStill, 18x?\n\nI don't know how to explain this apart from \"there's a not-insignificant overhead to calling a function vs inlining the body\". Have I missed something?\n\nR is usually \"fast enough\" for everything I do - I'm rarely trying to cram a lot of evaluations inside a given millisecond, but I was surprised that this single abstraction added so much. I'm imagining a heavy calculation inside a hot loop involving a lot of composed function calls and wondering how much could be saved by consolidating the composition into a single function body?\n\nJust for context of how \"slow\" this is, I wanted to see how Rust performs and ... ouch.\n\n```r\nrextendr::rust_function(\"fn add3_rust(z:Vec<i32>) -> Vec<i32> { z.iter().map(|x| x + 3).collect() }\", profile = \"release\")\n#> ℹ build directory: '/tmp/RtmpqsZ3jP/file73e7742716b8f'\n#> ✔ Writing '/tmp/RtmpqsZ3jP/file73e7742716b8f/target/extendr_wrappers.R'\n\nn <- 1e4\nbmr <- bench::mark(\n  inline = add3_inline(1:n),\n  call = add3_call(1:n),\n  sapply_call = add3_sapply_call(1:n),\n  rust = add3_rust(1:n),\n  iterations = 100\n)\nplot(bmr)\n```\n\n![](https://i.imgur.com/ElECsgG.png)<!-- -->\n\n``` r\nbmr[, 1:7]\n#> # A tibble: 4 × 6\n#>   expression       min   median `itr/sec` mem_alloc `gc/sec`\n#>   <bch:expr>  <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n#> 1 inline        1.03ms   1.08ms     925.    156.4KB      0  \n#> 2 call         18.52ms  21.53ms      41.7   156.4KB     11.8\n#> 3 sapply_call  19.79ms   23.6ms      41.8   362.6KB     13.2\n#> 4 rust         18.37µs  24.21µs   14165.     78.2KB      0\n```\n\nYeah, my best R is 40x slower than Rust (the `f3(x)` R version 875x slower than Rust) at this particular example. I love the idea of calling out to Rust for performance, and will definitely be doing that a bit more.\n\nIf you have an explanation about the function calling overhead, please let me know here or on [Mastodon](https://fosstodon.org/@jonocarroll). \n",
				"date_published": "2024-08-04T17:01:30+09:30",
				"url": "https://jcarroll.xyz/2024/08/04/r-function-call.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2024/08/01/book-review-data.html",
				"title": "Book review: Data Analysis with AI and R",
				"content_html": "<p>I published <a href=\"https://www.manning.com/books/beyond-spreadsheets-with-r\">Beyond Spreadsheets with R</a> with Manning Publications partly because they approached me, but a lot because they seem to do a good job of publishing technical books and making them DRM free - if you buy a physical copy you get access to the eBooks to download (pdf, epub, &hellip;) and use as you see fit. I&rsquo;ve received free (well-known) ebooks from other large publishers that I just don&rsquo;t read because it involves installing and opening up a proprietary piece of software just to read the book, and I can&rsquo;t integrate it with any of my notetaking tools.</p>\n<p>After going through the process, I can say that Manning also do a really good job of getting eyes on drafts and providing technical proofreading. I had a panel of reviewers who gave me great feedback on draft chapters and the book was better for it. I&rsquo;m learning a lot of languages and deep diving into more structured programming recently, so I find myself buying a lot of their technical books - it&rsquo;s possible that I&rsquo;ve given back a lot of what I made on Beyond Spreadsheets with R and I&rsquo;m not necessarily disappointed by that.</p>\n<p>I was recently offered a draft (Manning calls these Manning Early Access Program or MEAP editions) of <a href=\"https://www.manning.com/books/data-analysis-with-ai-and-r\">Data Analysis with AI and R</a> by Ulirich Matter and asked if I could share my thoughts. Word of mouth can do great things, and many eyes are very valuable for early drafts. What follows wasn&rsquo;t paid for; I was given an ebook copy of the book to review.</p>\n<p>I&rsquo;m apprehensive but optimistic when it comes to what we&rsquo;re calling AI now (whatever definition we&rsquo;ve had since the 1960s is now irrelevant) and I do wish we&rsquo;d come up with a better term that didn&rsquo;t use the word &ldquo;intelligence&rdquo; so early in the game. I&rsquo;m apprehensive because it&rsquo;s almost certainly being rushed into places it doesn&rsquo;t need to be at great cost to users, companies, and the environment, with results that overlook the human aspect of the problem being solved (the human aspect typically being the most complex). I&rsquo;m optimistic because in the few places where it can be used somewhat &ldquo;safely&rdquo; (of low impact if it&rsquo;s entirely wrong) it&rsquo;s capable of doing some things that a text-matching search can&rsquo;t - I treat a chat LLM like an overconfident coworker and ask it to explain how something I don&rsquo;t understand works. I still need to check to make sure it isn&rsquo;t wrong, but a web search engine can&rsquo;t answer &ldquo;what is this snippet of code doing?&rdquo;</p>\n<p>The first example of Data Analysis with AI and R demonstrates having an LLM try to fix some R code - a simple function that adds two input parameters as <code>a + b</code> that fails when called with a character argument <code>b</code> e.g. <code>b=&quot;5&quot;</code>. The demonstrated LLM offers a new version of the function that has <code>a + as.numeric(b)</code> in the body with a decent explanation of why it failed</p>\n<blockquote>\n<p>Your code is attempting to add a numeric value to a character string, which is causing\nthe error. In R, you need to convert the string to a numeric type first. Here’s the\ncorrected code:</p>\n</blockquote>\n<p>I think this is a fantastic teachable moment. Keep in mind that we&rsquo;re using R, a dynamically typed language, so preventing this sort of thing at the type level doesn&rsquo;t exist. Someone bitten by this enough times will start to do defensive programming and add assertions to the top of their functions to enforce type constraints according to how their function works internally plus any business logic of how it expects to be used. LLMs don&rsquo;t (typically) have any of that context, and sometimes that&rsquo;s a human component.</p>\n<p>An experienced programmer will recognise the symmetry in the arguments as presented - two values, to be used on either side of a <code>+</code> infix function. The LLM &ldquo;fixes&rdquo; the issue as presented, which is great, but if you stop there and move on you&rsquo;re leaving the potential bug that will bite you when someone tries to use the function with <code>a=&quot;5&quot;</code> and <code>b=5</code>. If you&rsquo;re stuck on something, an LLM can potentially get you unstuck, but it&rsquo;s not a magic bug eraser. It can generate some code that appears suitable as a response and that code may solve the immediate problem, but it&rsquo;s a tool to help you improve, not an actual coworker.</p>\n<p>With that said, I <a href=\"https://chatgpt.com/share/f2a1fc26-6b57-4d01-88c3-64f0393b5b64\">tried the example verbatim</a> into the latest (free) chatGPT and it identified the benefit of converting both arguments, but said in its explanation</p>\n<blockquote>\n<p>In R, you need to ensure that both arguments are of the same type</p>\n</blockquote>\n<p>which isn&rsquo;t strictly true&hellip;</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\"><span style=\"color:#a6e22e\">typeof</span>(a <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#ae81ff\">5</span>)\n<span style=\"color:#75715e\"># [1] &#34;double&#34;</span>\n<span style=\"color:#a6e22e\">typeof</span>(b <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#ae81ff\">5L</span>)\n<span style=\"color:#75715e\"># [1] &#34;integer&#34;</span>\n<span style=\"color:#a6e22e\">typeof</span>(c <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#ae81ff\">5+5i</span>)\n<span style=\"color:#75715e\"># [1] &#34;complex&#34;</span>\na<span style=\"color:#f92672\">+</span>b<span style=\"color:#f92672\">+</span>c\n<span style=\"color:#75715e\"># [1] 15+5i</span>\n</code></pre></div><p>Again, ask for advice, but treat it with suspicion.</p>\n<p>The rest of the available 3 chapters covers a lot of interesting topics, many of which I wasn&rsquo;t familiar with such as setting system messages (the internal prompt) via the API, the PAL (Programming-Aided Language Models) approach, and generating embeddings of some input text.</p>\n<p>I had a handful of suggestions based on what I&rsquo;d read which, for a regular, already published book, <em>might</em> get sent to the author to be listed on an errata page somewhere. For a MEAP book, Manning has a discussion forum where readers can ask questions, submit suggestions/corrections, etc&hellip; so that the book can be improved <em>before</em> it&rsquo;s published. I was very happy to have this available for Beyond Spreadsheets with R (<a href=\"https://livebook.manning.com/forum?product=carroll&amp;p=1&amp;page=1\">forum</a>) so I <a href=\"https://livebook.manning.com/forum?product=matter&amp;comment=571431\">submitted my notes/questions</a> there and hoped the author might have had a chance to respond before I wrote this, but not yet it seems.</p>\n<p>Overall, I&rsquo;m pleased with what I read so far - there&rsquo;s a lot in there about how to interact with the models via the API in an R way, including having it help write tests, documentation, and entire functions. I&rsquo;m looking forward to the rest of the book.</p>\n<p>I think it&rsquo;s brave to try to write a book about a field that&rsquo;s changing quite so fast, but most of the principles should still apply for at least a while, even if the screenshots of website menus are quickly outdated.</p>\n<p>So far, this one has my recommendation. I don&rsquo;t think I&rsquo;ve seen any R-related resources that have anywhere near this level of detail. If you&rsquo;re interested in a copy, I believe Manning has a sale at the moment and <a href=\"https://deals.manning.com/save-half-sitewide-for-the-end-of-july/\">all products sitewide are 50% off</a> (including Beyond Spreadsheets with R, FYI).</p>\n",
				"content_text": "I published [Beyond Spreadsheets with R](https://www.manning.com/books/beyond-spreadsheets-with-r) with Manning Publications partly because they approached me, but a lot because they seem to do a good job of publishing technical books and making them DRM free - if you buy a physical copy you get access to the eBooks to download (pdf, epub, ...) and use as you see fit. I've received free (well-known) ebooks from other large publishers that I just don't read because it involves installing and opening up a proprietary piece of software just to read the book, and I can't integrate it with any of my notetaking tools. \n\nAfter going through the process, I can say that Manning also do a really good job of getting eyes on drafts and providing technical proofreading. I had a panel of reviewers who gave me great feedback on draft chapters and the book was better for it. I'm learning a lot of languages and deep diving into more structured programming recently, so I find myself buying a lot of their technical books - it's possible that I've given back a lot of what I made on Beyond Spreadsheets with R and I'm not necessarily disappointed by that.\n\nI was recently offered a draft (Manning calls these Manning Early Access Program or MEAP editions) of [Data Analysis with AI and R](https://www.manning.com/books/data-analysis-with-ai-and-r) by Ulirich Matter and asked if I could share my thoughts. Word of mouth can do great things, and many eyes are very valuable for early drafts. What follows wasn't paid for; I was given an ebook copy of the book to review.\n\nI'm apprehensive but optimistic when it comes to what we're calling AI now (whatever definition we've had since the 1960s is now irrelevant) and I do wish we'd come up with a better term that didn't use the word \"intelligence\" so early in the game. I'm apprehensive because it's almost certainly being rushed into places it doesn't need to be at great cost to users, companies, and the environment, with results that overlook the human aspect of the problem being solved (the human aspect typically being the most complex). I'm optimistic because in the few places where it can be used somewhat \"safely\" (of low impact if it's entirely wrong) it's capable of doing some things that a text-matching search can't - I treat a chat LLM like an overconfident coworker and ask it to explain how something I don't understand works. I still need to check to make sure it isn't wrong, but a web search engine can't answer \"what is this snippet of code doing?\"\n\nThe first example of Data Analysis with AI and R demonstrates having an LLM try to fix some R code - a simple function that adds two input parameters as `a + b` that fails when called with a character argument `b` e.g. `b=\"5\"`. The demonstrated LLM offers a new version of the function that has `a + as.numeric(b)` in the body with a decent explanation of why it failed\n\n>Your code is attempting to add a numeric value to a character string, which is causing\nthe error. In R, you need to convert the string to a numeric type first. Here’s the\ncorrected code:\n\nI think this is a fantastic teachable moment. Keep in mind that we're using R, a dynamically typed language, so preventing this sort of thing at the type level doesn't exist. Someone bitten by this enough times will start to do defensive programming and add assertions to the top of their functions to enforce type constraints according to how their function works internally plus any business logic of how it expects to be used. LLMs don't (typically) have any of that context, and sometimes that's a human component. \n\nAn experienced programmer will recognise the symmetry in the arguments as presented - two values, to be used on either side of a `+` infix function. The LLM \"fixes\" the issue as presented, which is great, but if you stop there and move on you're leaving the potential bug that will bite you when someone tries to use the function with `a=\"5\"` and `b=5`. If you're stuck on something, an LLM can potentially get you unstuck, but it's not a magic bug eraser. It can generate some code that appears suitable as a response and that code may solve the immediate problem, but it's a tool to help you improve, not an actual coworker.\n\nWith that said, I [tried the example verbatim](https://chatgpt.com/share/f2a1fc26-6b57-4d01-88c3-64f0393b5b64) into the latest (free) chatGPT and it identified the benefit of converting both arguments, but said in its explanation \n\n> In R, you need to ensure that both arguments are of the same type\n\nwhich isn't strictly true...\n\n```r\ntypeof(a <- 5)\n# [1] \"double\"\ntypeof(b <- 5L)\n# [1] \"integer\"\ntypeof(c <- 5+5i)\n# [1] \"complex\"\na+b+c\n# [1] 15+5i\n```\n\nAgain, ask for advice, but treat it with suspicion.\n\nThe rest of the available 3 chapters covers a lot of interesting topics, many of which I wasn't familiar with such as setting system messages (the internal prompt) via the API, the PAL (Programming-Aided Language Models) approach, and generating embeddings of some input text.\n\nI had a handful of suggestions based on what I'd read which, for a regular, already published book, _might_ get sent to the author to be listed on an errata page somewhere. For a MEAP book, Manning has a discussion forum where readers can ask questions, submit suggestions/corrections, etc... so that the book can be improved _before_ it's published. I was very happy to have this available for Beyond Spreadsheets with R ([forum](https://livebook.manning.com/forum?product=carroll&p=1&page=1)) so I [submitted my notes/questions](https://livebook.manning.com/forum?product=matter&comment=571431) there and hoped the author might have had a chance to respond before I wrote this, but not yet it seems.\n\nOverall, I'm pleased with what I read so far - there's a lot in there about how to interact with the models via the API in an R way, including having it help write tests, documentation, and entire functions. I'm looking forward to the rest of the book. \n\nI think it's brave to try to write a book about a field that's changing quite so fast, but most of the principles should still apply for at least a while, even if the screenshots of website menus are quickly outdated. \n\nSo far, this one has my recommendation. I don't think I've seen any R-related resources that have anywhere near this level of detail. If you're interested in a copy, I believe Manning has a sale at the moment and [all products sitewide are 50% off](https://deals.manning.com/save-half-sitewide-for-the-end-of-july/) (including Beyond Spreadsheets with R, FYI).\n",
				"date_published": "2024-08-01T09:22:24+09:30",
				"url": "https://jcarroll.xyz/2024/08/01/book-review-data.html",
				"tags": ["R","Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2024/06/06/enums-in-r.html",
				"title": "Enums in R",
				"content_html": "<p>I&rsquo;m looping back to reading some of <a href=\"https://design.tidyverse.org/\">Tidy Design Principles</a> since it hasn&rsquo;t been touched in a while (indicating some stability, at least in the short term). I jumped ahead to a <a href=\"https://design.tidyverse.org/boolean-strategies.html\">section on enums</a> out of curiosity and pondered on them a bit more - they&rsquo;re <em>really</em> useful in Rust, maybe there&rsquo;s a good way to implement them in R.</p>\n<p>I did a quick search around the web and found some okay-ish Stack Overflow posts, then stumbled across <a href=\"https://josiahparry.com/posts/2023-11-10-enums-in-r/\">this post</a> which is just - chef&rsquo;s kiss - precisely what I wanted. I clearly haven&rsquo;t been notetaking for long, because I have <em>definitely</em> read this post&hellip; and left a comment on it at the time.</p>\n<p>Revisiting it again, it&rsquo;s still a brilliant post that has a really nice modern implementation of enums in R that boils down to</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\"><span style=\"color:#75715e\"># remotes::install_github(&#34;RConsortium/S7&#34;)</span>\n<span style=\"color:#75715e\"># or </span>\n<span style=\"color:#75715e\"># install.packages(&#34;S7&#34;)</span>\n<span style=\"color:#a6e22e\">library</span>(S7)\n\n<span style=\"color:#75715e\"># create a new Enum abstract class</span>\nEnum <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">new_class</span>(\n  <span style=\"color:#e6db74\">&#34;Enum&#34;</span>,\n  properties <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">list</span>(\n    Value <span style=\"color:#f92672\">=</span> class_character,\n    Variants <span style=\"color:#f92672\">=</span> class_character\n  ),\n  validator <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">function</span>(self) { \n    <span style=\"color:#a6e22e\">if </span>(<span style=\"color:#a6e22e\">length</span>(self<span style=\"color:#f92672\">@</span>Value) <span style=\"color:#f92672\">!=</span> <span style=\"color:#ae81ff\">1L</span>) {\n      <span style=\"color:#e6db74\">&#34;enum value&#39;s are length 1&#34;</span>\n    } else <span style=\"color:#a6e22e\">if </span>(<span style=\"color:#f92672\">!</span>(self<span style=\"color:#f92672\">@</span>Value <span style=\"color:#f92672\">%in%</span> self<span style=\"color:#f92672\">@</span>Variants)) {\n      <span style=\"color:#e6db74\">&#34;enum value must be one of possible variants&#34;</span>\n    }\n  }, \n  abstract <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">TRUE</span>\n)\n\n<span style=\"color:#75715e\"># create a new enum constructor </span>\nnew_enum_class <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(enum_class, variants) {\n  <span style=\"color:#a6e22e\">new_class</span>(\n    enum_class,\n    parent <span style=\"color:#f92672\">=</span> Enum,\n    properties <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">list</span>(\n      Value <span style=\"color:#f92672\">=</span> class_character,\n      Variants <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">new_property</span>(class_character, default <span style=\"color:#f92672\">=</span> variants)\n    ),\n    constructor <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">function</span>(Value) {\n      <span style=\"color:#a6e22e\">new_object</span>(<span style=\"color:#a6e22e\">S7_object</span>(), Value <span style=\"color:#f92672\">=</span> Value, Variants <span style=\"color:#f92672\">=</span> variants)\n    }\n  )\n}\n\nGridShape <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">new_enum_class</span>(\n  <span style=\"color:#e6db74\">&#34;GridShape&#34;</span>,\n  <span style=\"color:#a6e22e\">c</span>(<span style=\"color:#e6db74\">&#34;Square&#34;</span>, <span style=\"color:#e6db74\">&#34;Hexagon&#34;</span>)\n)\n\nGridShape\n</code></pre></div><p>so now I&rsquo;m left wondering &ldquo;where to next&rdquo; for this idea?</p>\n<p>One potential downside I can see of capturing argument options in an enum is that, depending on the implementation and how documentation is generated, the documentation may then live in the enum itself.</p>\n<p>I&rsquo;m not sure how one would document the S7 enum above, since it&rsquo;s the result of a function call, not a definition.</p>\n<p>With the current idiom of stringy enum arguments the variants are all spelled out in the function signature, and probably documented there, too. If, instead, the function took a <code>GridShape</code> enum, it might result in the variants being documented in the enum instead. This could lead to disparate implementations (add an unsupported enum variant) or just having to look in two places.</p>\n<p>On the other hand, it may also lead to more <em>consistent</em> documentation if that enum is used in multiple places.</p>\n<p>There&rsquo;s a certain efficiency to having users be able to call a function as <code>move(direction = &quot;north&quot;)</code> over <code>move(direction = Compass(&quot;north&quot;))</code> but maybe that can be solved with syntax, e.g. having the package define &amp; export a symbol <code>North &lt;- Compass(&quot;north&quot;)</code>. But then again, that obfuscates the origin of <code>North</code>. Having a <code>Compass::North</code> syntax like Rust would be great if the <code>::</code> wasn&rsquo;t already signalling a package namespace.</p>\n<p>We do already have a syntax for taking a subset; <code>Compass$North</code> would be the most consistent. That doesn&rsquo;t currently work for the S7 definition</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">GridShape<span style=\"color:#f92672\">$</span>Square\n<span style=\"color:#75715e\"># Error: Can&#39;t get S7 properties with `$`. Did you mean `GridShape@Square`?</span>\n\nGridShape<span style=\"color:#f92672\">@</span>Square\n<span style=\"color:#75715e\"># Error: Can&#39;t find property &lt;S7_class&gt;[@Square](https://micro.blog/Square)</span>\n</code></pre></div><p>but maybe that can be solved with some getter (?).</p>\n<p>I see some discussions in the S7 repo about support for enums / sum types / literal classes e.g. <a href=\"https://github.com/RConsortium/S7/issues/32\">https://github.com/RConsortium/S7/issues/32</a> and <a href=\"https://github.com/RConsortium/S7/issues/267\">https://github.com/RConsortium/S7/issues/267</a> but they predate Josiah&rsquo;s post.</p>\n<p>Has anyone been thinking about enums in R lately?</p>\n",
				"content_text": "I'm looping back to reading some of [Tidy Design Principles](https://design.tidyverse.org/) since it hasn't been touched in a while (indicating some stability, at least in the short term). I jumped ahead to a [section on enums](https://design.tidyverse.org/boolean-strategies.html) out of curiosity and pondered on them a bit more - they're *really* useful in Rust, maybe there's a good way to implement them in R.\r\n\r\nI did a quick search around the web and found some okay-ish Stack Overflow posts, then stumbled across [this post](https://josiahparry.com/posts/2023-11-10-enums-in-r/) which is just - chef's kiss - precisely what I wanted. I clearly haven't been notetaking for long, because I have *definitely* read this post... and left a comment on it at the time.\r\n\r\nRevisiting it again, it's still a brilliant post that has a really nice modern implementation of enums in R that boils down to\r\n\r\n```r\r\n# remotes::install_github(\"RConsortium/S7\")\r\n# or \r\n# install.packages(\"S7\")\r\nlibrary(S7)\r\n\r\n# create a new Enum abstract class\r\nEnum <- new_class(\r\n  \"Enum\",\r\n  properties = list(\r\n    Value = class_character,\r\n    Variants = class_character\r\n  ),\r\n  validator = function(self) { \r\n    if (length(self@Value) != 1L) {\r\n      \"enum value's are length 1\"\r\n    } else if (!(self@Value %in% self@Variants)) {\r\n      \"enum value must be one of possible variants\"\r\n    }\r\n  }, \r\n  abstract = TRUE\r\n)\r\n\r\n# create a new enum constructor \r\nnew_enum_class <- function(enum_class, variants) {\r\n  new_class(\r\n    enum_class,\r\n    parent = Enum,\r\n    properties = list(\r\n      Value = class_character,\r\n      Variants = new_property(class_character, default = variants)\r\n    ),\r\n    constructor = function(Value) {\r\n      new_object(S7_object(), Value = Value, Variants = variants)\r\n    }\r\n  )\r\n}\r\n\r\nGridShape <- new_enum_class(\r\n  \"GridShape\",\r\n  c(\"Square\", \"Hexagon\")\r\n)\r\n\r\nGridShape\r\n```\r\n\r\n\r\nso now I'm left wondering \"where to next\" for this idea?\r\n\r\nOne potential downside I can see of capturing argument options in an enum is that, depending on the implementation and how documentation is generated, the documentation may then live in the enum itself.\r\n\r\nI'm not sure how one would document the S7 enum above, since it's the result of a function call, not a definition.\r\n\r\nWith the current idiom of stringy enum arguments the variants are all spelled out in the function signature, and probably documented there, too. If, instead, the function took a `GridShape` enum, it might result in the variants being documented in the enum instead. This could lead to disparate implementations (add an unsupported enum variant) or just having to look in two places.\r\n\r\nOn the other hand, it may also lead to more _consistent_ documentation if that enum is used in multiple places.\r\n\r\nThere's a certain efficiency to having users be able to call a function as `move(direction = \"north\")` over `move(direction = Compass(\"north\"))` but maybe that can be solved with syntax, e.g. having the package define & export a symbol `North <- Compass(\"north\")`. But then again, that obfuscates the origin of `North`. Having a `Compass::North` syntax like Rust would be great if the `::` wasn't already signalling a package namespace. \r\n\r\nWe do already have a syntax for taking a subset; `Compass$North` would be the most consistent. That doesn't currently work for the S7 definition\r\n\r\n```r\r\nGridShape$Square\r\n# Error: Can't get S7 properties with `$`. Did you mean `GridShape@Square`?\r\n\r\nGridShape@Square\r\n# Error: Can't find property <S7_class>[@Square](https://micro.blog/Square)\r\n```\r\n\r\nbut maybe that can be solved with some getter (?).\r\n\r\nI see some discussions in the S7 repo about support for enums / sum types / literal classes e.g. https://github.com/RConsortium/S7/issues/32 and https://github.com/RConsortium/S7/issues/267 but they predate Josiah's post.\r\n\r\nHas anyone been thinking about enums in R lately?\n",
				"date_published": "2024-06-06T11:23:02+09:30",
				"url": "https://jcarroll.xyz/2024/06/06/enums-in-r.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2024/06/01/how-do-you.html",
				"title": "How do you do personal knowledge management?",
				"content_html": "<p>This post is a bit of a brain dump / working-out-loud thing. The posts here on <a href=\"https://jcarroll.xyz\">jcarroll.xyz</a> are less formal than what I put on my main blog <a href=\"https://jcarroll.com.au\">jcarroll.com.au </a> which is more for technical articles on coding. There won&rsquo;t be any code in this post. This was built up from a series of notes captured over a couple of weeks or so, which makes for a somewhat longer, bordering on rambling post, so &hellip; sorry, I guess?</p>\n<p>I&rsquo;ll also point out from the get-go that I&rsquo;m not particularly looking for tool suggestions in this post - I&rsquo;ve committed to using Obsidian as my note-capturing tool for at least the next couple of years to give it a fair chance.</p>\n<p>I just finished reading <a href=\"https://www.buildingasecondbrain.com/book\">Building a Second Brain</a> by Tiago Forte and even early into that book I was convinced that I wanted to start better capturing my notes when I&rsquo;m reading something. That&rsquo;s not to say it was the <em>only</em> resource I&rsquo;ve read on the topic recently - I haven&rsquo;t quite finished <a href=\"https://en.wikipedia.org/wiki/How_to_Read_a_Book\">How to Read a Book</a> (the first edition of which was published in the 1960s, and the second edition - the one I&rsquo;m reading - in the 1970s) which covers the concept of progressive summarisation in great detail, and was the <em>actual</em> first prompt for me to start taking notes, but I only did it while reading <em>that</em> book.</p>\n<p>The idea of progressive summarisation is to distil the essential message of a given paragraph/chapter/article (at some scale) so that if you do ever need to refer to it again you can read <em>that</em> rather than re-reading literally everything in the book. As someone who does a fair amount of knowledge work, that can be important. As a scientist I was trained to use a notebook frequently, but in any process I tried to implement myself I created too much friction to use it regularly in my non-academic life.</p>\n<p>I began by taking a lot of notes in paper notebooks - I enjoy the experience of writing and did find that on re-reading what I had written I found a lot of things that I had otherwise forgotten about, even just a week later. Paper is nice, my fountain pens are lovely to write with, and a notebook makes for a targeted place to store those notes, but it wasn&rsquo;t searchable or necessarily legible all the time. &ldquo;What good are notes you never read&rdquo; comes up a few times around the place. I needed a digital solution both for storage and for searching/organising the notes.</p>\n<p>As it turns out, AI LLMs are <em>pretty good</em> at extracting text from my a photo of my handwriting - not perfect, but way better than what I remember of using a Palm Pilot with a very specific (and temperamental) handwriting recognition system. So, I could &ldquo;scan&rdquo; all my handwritten notes and&hellip; file them? I&rsquo;d still need to text search through them every time I wanted to look up something.</p>\n<p>Enter <a href=\"https://obsidian.md/\">Obsidian</a> - a free (but not open-source) digital note organisation tool. It&rsquo;s probably familiar to users of Evernote, Notion, etc&hellip; but extremely configurable; stores <em>everything</em> in plaintext (mostly markdown) locally; and works on pretty much every platform. Even better, they&rsquo;ve very recently dropped the price of their sync offering to US$4/month which seems very reasonable given how well it works. I briefly flirted with the idea of the other free sync options (put it all in git, gDrive, etc&hellip;) but getting these to work properly across multiple devices including a phone didn&rsquo;t seem like a saving over the very reasonable price of sync. I&rsquo;d used Obsidian before for notes, but just for capturing something daily, which I rarely ever referred back to.</p>\n<p>Building a Second Brain is agnostic to the tool being used, so I figured I&rsquo;d do what a <em>lot</em> of other people on the internet appear to have done and implement that system in Obsidian. I&rsquo;m learning a lot as I go and trying not to over-complicate things, but I&rsquo;m interested in hearing about how other people do this.</p>\n<p>One piece I&rsquo;m still trying to balance is the physical act of notetaking on paper and transferring to a digital tool. My understanding is that there&rsquo;s a nontrivial difference in retention between writing a note and just highlighting something. My notes on Building a Second Brain were physical, while I bought the eBook for the follow-up <a href=\"https://www.buildingasecondbrain.com/para\">The PARA Method</a> and I&rsquo;ll do highlighting on my Kindle Scribe which I can export to text. I can also physically write on the Scribe and export that converted text (and it still does a pretty good job of the conversion of my handwriting). I suppose we&rsquo;ll see if these notes feel better captured.</p>\n<p>To set up some context about who <em>I</em> am and why I want to take notes at all: I&rsquo;m not an academic any more but I still do research, both for work and fun. I&rsquo;m a PhD theoretical physicist who transitioned to software / data engineering in cancer immunology and now autoimmune diseases doing statistics, software development, data curation/parsing/transformation/storage, visualisation, infrastructure, API wrappers and more, primarily using R. I work on a handful of open-source projects including {ggeasy}. I&rsquo;m learning as many <a href=\"https://github.com/jonocarroll/exercism-solutions\">programming languages as I can</a> and enjoy polyglot challenges. I&rsquo;m interested in personal knowledge management and knowledge graphs (and the intersection of these).</p>\n<p>&laquo; self-promotion warning &raquo;</p>\n<p>For what it&rsquo;s worth, if that&rsquo;s the profile of someone you&rsquo;re looking for in your organisation, I occasionally have some short-term capacity to take on projects. I can&rsquo;t recall on which podcast I heard it discussed (because I wasn&rsquo;t taking notes&hellip; grrr) but there was a point about the fact that orgs don&rsquo;t hire physicists for their physics knowledge; they hire them because getting a PhD in physics is a proxy for &ldquo;I can do this work&rdquo;. I connect up data sources via APIs and create tools to help scientists interrogate their data faster and more robustly. If you&rsquo;re interested, feel free to get in touch with me.</p>\n<p>So, with that context in mind&hellip; &ldquo;how do you use personal knowledge management?&rdquo; Not which tools do you use - I&rsquo;m less interested in how you ingest notes and more interested in how you <em>extract knowledge</em> from them. We don&rsquo;t insert knowledge into these tools; we use them to offload the heavy lift of storing information so that we can link things up and retrieve them on demand.</p>\n<p>The notes I took for Building a Second Brain were a bit meta - but there were tidbits in there I probably want to refer back to at some point. I work on a set of projects for different clients and need to keep track of certain details, ideas, snippets, and notes in general for those, so a notetaking tool of some sort is essential.</p>\n<p>I started expanding my notetaking to the rest of my life and created notes for a project I&rsquo;m currently in the middle of - building some more storage for my workshop/shed. Then I realised that there were a lot of other &lsquo;areas&rsquo; (not &lsquo;projects&rsquo;) for which I could be capturing information. I started capturing all the IT infrastructure in my house and realised how scattered all the information actually is. I remembered this post about &lsquo;<a href=\"https://luke.hsiao.dev/blog/housing-documentation/\">documenting your house</a>&rsquo; and managed to dig it out of Pocket - my current &lsquo;read later&rsquo; tool. I realised I could store the product manuals for all the devices right alongside my config notes, model numbers, purchased date (year at least). It&rsquo;s starting to feel organised, and that&rsquo;s great. These bits aren&rsquo;t linked in any way to my research, but that&rsquo;s fine - I can have disconnected &lsquo;landmasses&rsquo; of notes.</p>\n<p>I started capturing the blog posts I found useful/interesting - Obsidian connects up to a web clipping tool that can capture the entire content of a post and insert it as markdown into a new note. I attended a Meetup and noted some things I wanted to look into more later. I now have a note for each Meetup group linking to notes for each of the people I frequently meet. I can write the link for anything and even if the note it points to doesn&rsquo;t exist, if I ever create it the link will be made.</p>\n<p>I figured I could probably also capture all the books on my bookshelf, but I really didn&rsquo;t want to sit there and type out all the titles manually. Thankfully, these new fangled AIs are quite good at extracting them from (rotated) images. I had a lot of success with claude.ai (once I got the prompt working). I have a note containing a bullet list of all the books on my shelf, sorted into rough categories. I did the same with my vinyl albums. Fantastic! So organised! Now if I want to refer to those in my notes, it&rsquo;s easy. Plus, I can see what I do and don&rsquo;t already have.</p>\n<p>At this point I tooted on Mastodon about how much I was enjoying Obsidian</p>\n<!-- raw HTML omitted -->\n<p>and was pointed to a convenient plugin to pull in metadata about books/music/etc&hellip; from a database</p>\n<!-- raw HTML omitted -->\n<p>which a) is extremely cool! b) made me stop and think about what I was actually doing with this data. Yes, I could pull in all that additional information, but would I ever use it?</p>\n<p>One of the points in Building a Second Brain was about &lsquo;data hoarders&rsquo; - people who note everything in case they&rsquo;ll ever need it, but with no plan to get any of it out again. I&rsquo;m not accusing Lou here of anything like that - a good suggestion for a plugin is most welcome - but it got me thinking about how I want to landscape my knowledge garden; do I want ornamentals (nice to look at but not productive) or fruiting plants? I&rsquo;ve since seen other people with curated vaults full of movies they&rsquo;ve seen/reviewed and all that metadata pulled in automatically, so it has a place, but not necessarily for what I want to get out of my own vault.</p>\n<p>I&rsquo;ve dealt with a similar delicate balance before - I&rsquo;m frequently tasked with annotating data (most recently of the genomic variety) such as complementing the metadata for samples from a clinical trial with metadata regarding the source subject. Where I&rsquo;ve seen this done less reliably, all of that has been shoehorned into the data of the sample itself, but in more sophisticated systems it&rsquo;s a lookup from sample to subject to create a join. Not every piece of metadata is always relevant, though - it depends on what you want to do with it. Some downstream users of the data I curate might be interested in the medical history or concomitant medications of the source subject when analysing a sample, but others may only be interested in the count method used for sequencing.</p>\n<p>I <em>could</em> link all sorts of metadata to the entries I&rsquo;ve created - I could add the artist to my albums and add a note for them. I could add the record label and the city in which they&rsquo;re headquartered, and the individual band members and the other bands they&rsquo;ve been in and their albums&hellip; but where does it end, and why do I want that?</p>\n<p>Someone recently created a (knowledge) graph of <a href=\"https://youtu.be/JheGL6uSF-4?si=Hx7_NQXlYgpd9WaP\">&ldquo;all of Wikipedia&rdquo;</a> which is both &ldquo;trivial&rdquo; to do and impressive - trivial because it&rsquo;s just capturing the links between pages, and impressive because it&rsquo;s nearly 200 million links between 63 million (English) articles. This is interesting because it shows which pages link to each other, but not <em>why</em> they link to each other - one of the examples in the section on paths between articles shows that to get from &lsquo;Pokemon&rsquo; to &lsquo;Ancient Egypt&rsquo; it&rsquo;s only one stop along the way - &lsquo;pet&rsquo; appears in the &lsquo;Pokemon&rsquo; article and has a link to &lsquo;Ancient Egypt&rsquo;&hellip; but that graph traversal isn&rsquo;t particularly useful or meaningful.</p>\n<p>Similarly, I&rsquo;m trying to figure out which links <em>are</em> useful to capture, and part of that is figuring out what a &lsquo;note&rsquo; looks like. I think I&rsquo;m starting to get the idea that a note should capture an atomic &lsquo;idea&rsquo; so that links between <em>those</em> are useful. Finding a link to the entire Building a Second Brain book note isn&rsquo;t helpful, but finding one to a particular quote from there might be. With that in mind, do I need a note capturing the artist, year, runtime, etc&hellip; for every album I own? Am I ever going to reference that? More importantly, perhaps, can&rsquo;t I just search the web for that information if I need it?</p>\n<p>In theory, I may want to search my knowledge garden for an artist or an author, but more likely I&rsquo;ll want to create links between <em>ideas</em> in their works, and I should capture <em>those</em>.</p>\n<p>More broadly, I&rsquo;m starting to capture things that I&rsquo;ve found troublesome to find via a web search - getting harder and harder these days with all the LLM content, SEO-optimised trash, and even human-generated content that&rsquo;s only purpose is engagement farming. One of the points that stuck with me from Building a Second Brain was that a resource such as one&rsquo;s notes should &ldquo;do the heavy lifting ahead of time so that the actual project is easier&rdquo;. I&rsquo;m still figuring out what a &ldquo;project&rdquo; is in my world (sometimes literally a client project, but other times a less-well-defined thing) but having notes to get the ball rolling sounds like a great idea.</p>\n<p>Another salient point that I believe will guide me in the right direction was the idea that &ldquo;the notes should form a working environment, not a storage environment&rdquo; - notes should be &ldquo;actionable&rdquo; in the sense that you can do something with them. I think this comes back to the &ldquo;ornamental vs productive fruit&rdquo; garden framing.</p>\n<p>When creating a new note, a question I&rsquo;m trying to keep front-of-mind is &ldquo;can I get out what I want from this via a query?&rdquo; - that query could be a simple text search, a search for a specific tag, a backlink, or some more sophisticated dataview, but I do want it to be possible to get to the information, and would like to structure my notes to make them more amenable to those queries.</p>\n<p>So far, when reading a paper/blog post, I&rsquo;m copying in the entire text and bolding/highlighting as a means of progressive summarisation, but I believe I need to (later, not immediately, after reflection) turn any salient points from that source into their own notes. I <em>think</em> the name for the former is &ldquo;fleeting notes&rdquo; (which don&rsquo;t need to be kept) and/or &ldquo;literature notes&rdquo; (direct highlights from the source) but these need to be distilled into their own notes at some point. I&rsquo;m not really looking for the &ldquo;right&rdquo; solution here, more trying to understand what will be helpful in achieving success.</p>\n<p>I&rsquo;m definitely starting to appreciate the value of notes - all of the ideas for this post were captured in a note as short bullet points, getting as many down as possible, ignoring spelling. When I have a good idea it tends to flow all-too-freely and I worry that I&rsquo;ll forget some interesting point I wanted to make. I feel that most (if not all) of those got captured this time, so if this post is still boring then bad luck - that was 100% of what I&rsquo;ve got to offer. All that was left to do was the wordcrafting around the bullet points. I still enjoy that part, but it was reassuring to have the bullet points to guide my thoughts. It was also extremely helpful to have a bunch of recent resources at my disposal to link to - there are hopefully more useful links scattered throughout this post than usual. If you find them overwhelming, though - I made <a href=\"https://jcarroll.com.au/2023/12/17/making-links-a-little-less-hyper/\">this javascript snippet</a> to help you focus.</p>\n<p>I found some comfort in the idea that my notes are for my eyes only - they don&rsquo;t need to be any more polished than I want them to be because they&rsquo;re not to be seen by anyone else. This also meant that I felt more comfortable leaving these &lsquo;draft&rsquo; notes in a &lsquo;draft&rsquo; state - I added to the notes for this post several times over a week (part of why this post is so long). The original reason for using this hosting service (<a href=\"https://micro.blog\">micro.blog</a>) was to be able to write <em>faster</em> - to get the ideas I had from my head to the world wide web as fast as possible, without going through the pull - write - build - push -check - merge cycle that my main blog requires (Rmarkdown to markdown to html to Netlify via git).</p>\n<p>I no longer need to worry (not that I stress over things) that I&rsquo;ll forget a useful point - it&rsquo;s stored in my second brain. I&rsquo;ve started doing this for all sorts of things I previously would simply &ldquo;hope to remember later&rdquo;. Thanks to the sync functionality of Obsidian, my second brain is always in my pocket and on all my desktops. I wrote this post in Obsidian because I know it will be synced to all my devices.</p>\n<p>I&rsquo;ve had more than a couple of instances since I started focusing on notetaking where I&rsquo;ve dropped everything to add a note or braindump. Sometimes it&rsquo;s been directly into Obsidian, other times on paper or my Scribe. One venue I haven&rsquo;t quite figured out yet is the shower - I have a large fraction of my best ideas or debugging solutions come to me in the shower or on a walk. This isn&rsquo;t uncommon, I believe - in <a href=\"https://dl.acm.org/doi/10.1145/3584859\">10 Things Software Developers Should Learn about Learning</a> they discuss &ldquo;spreading activation&rdquo; and the likelihood of triggering linkages when not actively thinking about a topic. Maybe I need a waterproof tablet device? If you ask me, workplaces enforcing a &lsquo;return to office&rsquo; policy should encourage &ldquo;debugging showers&rdquo;; it&rsquo;s certainly useful for me working from home. Actually, it looks like I&rsquo;m not the first to have this idea; there is a <a href=\"https://www.amazon.com.au/Aqua-Notes-Water-Proof-Note/dp/B01AS5I0ZS/\">waterproof notepad</a> that I might have to try out.</p>\n<p>I likely need to get more comfortable taking audio notes both at home and on-the-go. I&rsquo;ve figured out how to get my smartwatch to record these, but Obsidian has a native recording facility that I am yet to try out. I haven&rsquo;t explored if it can transcribe those notes, either.</p>\n<p>There are a few other plugins I&rsquo;m keen to see developed - top of my list is a workable LLM to enable me to &ldquo;ask&rdquo; questions of my notes. For a while now I&rsquo;ve wondered if it&rsquo;s possible for an &ldquo;AI&rdquo; to find links between ideas I <em>haven&rsquo;t</em> identified. I&rsquo;m not quite sure what I anticipate that to look like, but perhaps &ldquo;you have notes on both iGAN and obesity - a note on <a href=\"https://en.wikipedia.org/wiki/Akkermansia_muciniphila\">Akkermansia_muciniphila</a> would connect these two.&rdquo;</p>\n<p>So far, all of the local LLM plugins I&rsquo;ve tried which are able to serve up all the markdown files are either ridiculously slow or fail at the simplest requests. I&rsquo;m hoping this improves soon.</p>\n<p>I&rsquo;m curious if it&rsquo;s useful to do spaced repetition on some of the notes I create - certainly revisiting them at a later point in time - with added life experience, new takes/perspectives, new opinions - would lead to a refined/expanded summary, but how to invoke that? I&rsquo;ve managed to build a dataview search for &lsquo;notes created in the last 7 days&rsquo; which I&rsquo;ll use for a weekly review, but beyond that I may never see any given note again. I added the &lsquo;open a random note&rsquo; tool and occasionally see what it surfaces.</p>\n<p>All of the above was a roundabout way of asking the people who have been using these tools for longer - how do you extract knowledge from your &ldquo;second brain&rdquo;? with a follow-up of &ldquo;what did you put in place that best enabled that?&rdquo; What <em>didn&rsquo;t</em> work?</p>\n<p>If Obsidian is a &ldquo;Second Brain&rdquo; then perhaps other people can be a &ldquo;third brain&rdquo;, but you have to ask.</p>\n<p>Comments/suggestions/advice is most welcome either here directly, on Mastodon (I&rsquo;m <a href=\"https://fosstodon.org/@jonocarroll\">@jonocarroll@fosstodon.org</a>), or via email (<a href=\"mailto:hello@jcarroll.com.au\">hello@jcarroll.com.au</a>).</p>\n",
				"content_text": "This post is a bit of a brain dump / working-out-loud thing. The posts here on [jcarroll.xyz](https://jcarroll.xyz) are less formal than what I put on my main blog [jcarroll.com.au ](https://jcarroll.com.au) which is more for technical articles on coding. There won't be any code in this post. This was built up from a series of notes captured over a couple of weeks or so, which makes for a somewhat longer, bordering on rambling post, so ... sorry, I guess?\n\nI'll also point out from the get-go that I'm not particularly looking for tool suggestions in this post - I've committed to using Obsidian as my note-capturing tool for at least the next couple of years to give it a fair chance.\n\nI just finished reading [Building a Second Brain](https://www.buildingasecondbrain.com/book) by Tiago Forte and even early into that book I was convinced that I wanted to start better capturing my notes when I'm reading something. That's not to say it was the *only* resource I've read on the topic recently - I haven't quite finished [How to Read a Book](https://en.wikipedia.org/wiki/How_to_Read_a_Book) (the first edition of which was published in the 1960s, and the second edition - the one I'm reading - in the 1970s) which covers the concept of progressive summarisation in great detail, and was the _actual_ first prompt for me to start taking notes, but I only did it while reading _that_ book.\n\nThe idea of progressive summarisation is to distil the essential message of a given paragraph/chapter/article (at some scale) so that if you do ever need to refer to it again you can read _that_ rather than re-reading literally everything in the book. As someone who does a fair amount of knowledge work, that can be important. As a scientist I was trained to use a notebook frequently, but in any process I tried to implement myself I created too much friction to use it regularly in my non-academic life.\n\nI began by taking a lot of notes in paper notebooks - I enjoy the experience of writing and did find that on re-reading what I had written I found a lot of things that I had otherwise forgotten about, even just a week later. Paper is nice, my fountain pens are lovely to write with, and a notebook makes for a targeted place to store those notes, but it wasn't searchable or necessarily legible all the time. \"What good are notes you never read\" comes up a few times around the place. I needed a digital solution both for storage and for searching/organising the notes.\n\nAs it turns out, AI LLMs are _pretty good_ at extracting text from my a photo of my handwriting - not perfect, but way better than what I remember of using a Palm Pilot with a very specific (and temperamental) handwriting recognition system. So, I could \"scan\" all my handwritten notes and... file them? I'd still need to text search through them every time I wanted to look up something.\n\nEnter [Obsidian](https://obsidian.md/) - a free (but not open-source) digital note organisation tool. It's probably familiar to users of Evernote, Notion, etc... but extremely configurable; stores _everything_ in plaintext (mostly markdown) locally; and works on pretty much every platform. Even better, they've very recently dropped the price of their sync offering to US$4/month which seems very reasonable given how well it works. I briefly flirted with the idea of the other free sync options (put it all in git, gDrive, etc...) but getting these to work properly across multiple devices including a phone didn't seem like a saving over the very reasonable price of sync. I'd used Obsidian before for notes, but just for capturing something daily, which I rarely ever referred back to.\n\nBuilding a Second Brain is agnostic to the tool being used, so I figured I'd do what a *lot* of other people on the internet appear to have done and implement that system in Obsidian. I'm learning a lot as I go and trying not to over-complicate things, but I'm interested in hearing about how other people do this.\n\nOne piece I'm still trying to balance is the physical act of notetaking on paper and transferring to a digital tool. My understanding is that there's a nontrivial difference in retention between writing a note and just highlighting something. My notes on Building a Second Brain were physical, while I bought the eBook for the follow-up [The PARA Method](https://www.buildingasecondbrain.com/para) and I'll do highlighting on my Kindle Scribe which I can export to text. I can also physically write on the Scribe and export that converted text (and it still does a pretty good job of the conversion of my handwriting). I suppose we'll see if these notes feel better captured.\n\nTo set up some context about who _I_ am and why I want to take notes at all: I'm not an academic any more but I still do research, both for work and fun. I'm a PhD theoretical physicist who transitioned to software / data engineering in cancer immunology and now autoimmune diseases doing statistics, software development, data curation/parsing/transformation/storage, visualisation, infrastructure, API wrappers and more, primarily using R. I work on a handful of open-source projects including {ggeasy}. I'm learning as many [programming languages as I can](https://github.com/jonocarroll/exercism-solutions) and enjoy polyglot challenges. I'm interested in personal knowledge management and knowledge graphs (and the intersection of these).\n\n<< self-promotion warning >>\n\nFor what it's worth, if that's the profile of someone you're looking for in your organisation, I occasionally have some short-term capacity to take on projects. I can't recall on which podcast I heard it discussed (because I wasn't taking notes... grrr) but there was a point about the fact that orgs don't hire physicists for their physics knowledge; they hire them because getting a PhD in physics is a proxy for \"I can do this work\". I connect up data sources via APIs and create tools to help scientists interrogate their data faster and more robustly. If you're interested, feel free to get in touch with me.\n\nSo, with that context in mind... \"how do you use personal knowledge management?\" Not which tools do you use - I'm less interested in how you ingest notes and more interested in how you _extract knowledge_ from them. We don't insert knowledge into these tools; we use them to offload the heavy lift of storing information so that we can link things up and retrieve them on demand.\n\nThe notes I took for Building a Second Brain were a bit meta - but there were tidbits in there I probably want to refer back to at some point. I work on a set of projects for different clients and need to keep track of certain details, ideas, snippets, and notes in general for those, so a notetaking tool of some sort is essential. \n\nI started expanding my notetaking to the rest of my life and created notes for a project I'm currently in the middle of - building some more storage for my workshop/shed. Then I realised that there were a lot of other 'areas' (not 'projects') for which I could be capturing information. I started capturing all the IT infrastructure in my house and realised how scattered all the information actually is. I remembered this post about '[documenting your house](https://luke.hsiao.dev/blog/housing-documentation/)' and managed to dig it out of Pocket - my current 'read later' tool. I realised I could store the product manuals for all the devices right alongside my config notes, model numbers, purchased date (year at least). It's starting to feel organised, and that's great. These bits aren't linked in any way to my research, but that's fine - I can have disconnected 'landmasses' of notes.\n\nI started capturing the blog posts I found useful/interesting - Obsidian connects up to a web clipping tool that can capture the entire content of a post and insert it as markdown into a new note. I attended a Meetup and noted some things I wanted to look into more later. I now have a note for each Meetup group linking to notes for each of the people I frequently meet. I can write the link for anything and even if the note it points to doesn't exist, if I ever create it the link will be made.\n\nI figured I could probably also capture all the books on my bookshelf, but I really didn't want to sit there and type out all the titles manually. Thankfully, these new fangled AIs are quite good at extracting them from (rotated) images. I had a lot of success with claude.ai (once I got the prompt working). I have a note containing a bullet list of all the books on my shelf, sorted into rough categories. I did the same with my vinyl albums. Fantastic! So organised! Now if I want to refer to those in my notes, it's easy. Plus, I can see what I do and don't already have.\n\nAt this point I tooted on Mastodon about how much I was enjoying Obsidian \n\n<iframe src=\"https://fosstodon.org/@jonocarroll/112267246150060195/embed\" class=\"mastodon-embed\" style=\"max-width: 100%; border: 0\" width=\"400\" allowfullscreen=\"allowfullscreen\"></iframe><script src=\"https://fosstodon.org/embed.js\" async=\"async\"></script>\n\nand was pointed to a convenient plugin to pull in metadata about books/music/etc... from a database\n\n<iframe src=\"https://social.lol/@amerpie/112270020807629471/embed\" class=\"mastodon-embed\" style=\"max-width: 100%; border: 0\" width=\"400\" allowfullscreen=\"allowfullscreen\"></iframe><script src=\"https://fosstodon.org/embed.js\" async=\"async\"></script>\n\nwhich a) is extremely cool! b) made me stop and think about what I was actually doing with this data. Yes, I could pull in all that additional information, but would I ever use it?\n\nOne of the points in Building a Second Brain was about 'data hoarders' - people who note everything in case they'll ever need it, but with no plan to get any of it out again. I'm not accusing Lou here of anything like that - a good suggestion for a plugin is most welcome - but it got me thinking about how I want to landscape my knowledge garden; do I want ornamentals (nice to look at but not productive) or fruiting plants? I've since seen other people with curated vaults full of movies they've seen/reviewed and all that metadata pulled in automatically, so it has a place, but not necessarily for what I want to get out of my own vault.\n\nI've dealt with a similar delicate balance before - I'm frequently tasked with annotating data (most recently of the genomic variety) such as complementing the metadata for samples from a clinical trial with metadata regarding the source subject. Where I've seen this done less reliably, all of that has been shoehorned into the data of the sample itself, but in more sophisticated systems it's a lookup from sample to subject to create a join. Not every piece of metadata is always relevant, though - it depends on what you want to do with it. Some downstream users of the data I curate might be interested in the medical history or concomitant medications of the source subject when analysing a sample, but others may only be interested in the count method used for sequencing.\n\nI _could_ link all sorts of metadata to the entries I've created - I could add the artist to my albums and add a note for them. I could add the record label and the city in which they're headquartered, and the individual band members and the other bands they've been in and their albums... but where does it end, and why do I want that? \n\nSomeone recently created a (knowledge) graph of [\"all of Wikipedia\"](https://youtu.be/JheGL6uSF-4?si=Hx7_NQXlYgpd9WaP) which is both \"trivial\" to do and impressive - trivial because it's just capturing the links between pages, and impressive because it's nearly 200 million links between 63 million (English) articles. This is interesting because it shows which pages link to each other, but not _why_ they link to each other - one of the examples in the section on paths between articles shows that to get from 'Pokemon' to 'Ancient Egypt' it's only one stop along the way - 'pet' appears in the 'Pokemon' article and has a link to 'Ancient Egypt'... but that graph traversal isn't particularly useful or meaningful.\n\nSimilarly, I'm trying to figure out which links _are_ useful to capture, and part of that is figuring out what a 'note' looks like. I think I'm starting to get the idea that a note should capture an atomic 'idea' so that links between _those_ are useful. Finding a link to the entire Building a Second Brain book note isn't helpful, but finding one to a particular quote from there might be. With that in mind, do I need a note capturing the artist, year, runtime, etc... for every album I own? Am I ever going to reference that? More importantly, perhaps, can't I just search the web for that information if I need it?\n\nIn theory, I may want to search my knowledge garden for an artist or an author, but more likely I'll want to create links between _ideas_ in their works, and I should capture _those_. \n\nMore broadly, I'm starting to capture things that I've found troublesome to find via a web search - getting harder and harder these days with all the LLM content, SEO-optimised trash, and even human-generated content that's only purpose is engagement farming. One of the points that stuck with me from Building a Second Brain was that a resource such as one's notes should \"do the heavy lifting ahead of time so that the actual project is easier\". I'm still figuring out what a \"project\" is in my world (sometimes literally a client project, but other times a less-well-defined thing) but having notes to get the ball rolling sounds like a great idea.\n\nAnother salient point that I believe will guide me in the right direction was the idea that \"the notes should form a working environment, not a storage environment\" - notes should be \"actionable\" in the sense that you can do something with them. I think this comes back to the \"ornamental vs productive fruit\" garden framing.\n\nWhen creating a new note, a question I'm trying to keep front-of-mind is \"can I get out what I want from this via a query?\" - that query could be a simple text search, a search for a specific tag, a backlink, or some more sophisticated dataview, but I do want it to be possible to get to the information, and would like to structure my notes to make them more amenable to those queries.\n\nSo far, when reading a paper/blog post, I'm copying in the entire text and bolding/highlighting as a means of progressive summarisation, but I believe I need to (later, not immediately, after reflection) turn any salient points from that source into their own notes. I _think_ the name for the former is \"fleeting notes\" (which don't need to be kept) and/or \"literature notes\" (direct highlights from the source) but these need to be distilled into their own notes at some point. I'm not really looking for the \"right\" solution here, more trying to understand what will be helpful in achieving success.\n\nI'm definitely starting to appreciate the value of notes - all of the ideas for this post were captured in a note as short bullet points, getting as many down as possible, ignoring spelling. When I have a good idea it tends to flow all-too-freely and I worry that I'll forget some interesting point I wanted to make. I feel that most (if not all) of those got captured this time, so if this post is still boring then bad luck - that was 100% of what I've got to offer. All that was left to do was the wordcrafting around the bullet points. I still enjoy that part, but it was reassuring to have the bullet points to guide my thoughts. It was also extremely helpful to have a bunch of recent resources at my disposal to link to - there are hopefully more useful links scattered throughout this post than usual. If you find them overwhelming, though - I made [this javascript snippet](https://jcarroll.com.au/2023/12/17/making-links-a-little-less-hyper/) to help you focus.\n\nI found some comfort in the idea that my notes are for my eyes only - they don't need to be any more polished than I want them to be because they're not to be seen by anyone else. This also meant that I felt more comfortable leaving these 'draft' notes in a 'draft' state - I added to the notes for this post several times over a week (part of why this post is so long). The original reason for using this hosting service ([micro.blog](https://micro.blog)) was to be able to write _faster_ - to get the ideas I had from my head to the world wide web as fast as possible, without going through the pull - write - build - push -check - merge cycle that my main blog requires (Rmarkdown to markdown to html to Netlify via git). \n\nI no longer need to worry (not that I stress over things) that I'll forget a useful point - it's stored in my second brain. I've started doing this for all sorts of things I previously would simply \"hope to remember later\". Thanks to the sync functionality of Obsidian, my second brain is always in my pocket and on all my desktops. I wrote this post in Obsidian because I know it will be synced to all my devices.\n\nI've had more than a couple of instances since I started focusing on notetaking where I've dropped everything to add a note or braindump. Sometimes it's been directly into Obsidian, other times on paper or my Scribe. One venue I haven't quite figured out yet is the shower - I have a large fraction of my best ideas or debugging solutions come to me in the shower or on a walk. This isn't uncommon, I believe - in [10 Things Software Developers Should Learn about Learning](https://dl.acm.org/doi/10.1145/3584859) they discuss \"spreading activation\" and the likelihood of triggering linkages when not actively thinking about a topic. Maybe I need a waterproof tablet device? If you ask me, workplaces enforcing a 'return to office' policy should encourage \"debugging showers\"; it's certainly useful for me working from home. Actually, it looks like I'm not the first to have this idea; there is a [waterproof notepad](https://www.amazon.com.au/Aqua-Notes-Water-Proof-Note/dp/B01AS5I0ZS/) that I might have to try out.\n\nI likely need to get more comfortable taking audio notes both at home and on-the-go. I've figured out how to get my smartwatch to record these, but Obsidian has a native recording facility that I am yet to try out. I haven't explored if it can transcribe those notes, either.\n\nThere are a few other plugins I'm keen to see developed - top of my list is a workable LLM to enable me to \"ask\" questions of my notes. For a while now I've wondered if it's possible for an \"AI\" to find links between ideas I _haven't_ identified. I'm not quite sure what I anticipate that to look like, but perhaps \"you have notes on both iGAN and obesity - a note on [Akkermansia_muciniphila](https://en.wikipedia.org/wiki/Akkermansia_muciniphila) would connect these two.\"\n\nSo far, all of the local LLM plugins I've tried which are able to serve up all the markdown files are either ridiculously slow or fail at the simplest requests. I'm hoping this improves soon.\n\nI'm curious if it's useful to do spaced repetition on some of the notes I create - certainly revisiting them at a later point in time - with added life experience, new takes/perspectives, new opinions - would lead to a refined/expanded summary, but how to invoke that? I've managed to build a dataview search for 'notes created in the last 7 days' which I'll use for a weekly review, but beyond that I may never see any given note again. I added the 'open a random note' tool and occasionally see what it surfaces.\n\nAll of the above was a roundabout way of asking the people who have been using these tools for longer - how do you extract knowledge from your \"second brain\"? with a follow-up of \"what did you put in place that best enabled that?\" What _didn't_ work?\n\nIf Obsidian is a \"Second Brain\" then perhaps other people can be a \"third brain\", but you have to ask.\n\nComments/suggestions/advice is most welcome either here directly, on Mastodon (I'm [@jonocarroll@fosstodon.org](https://fosstodon.org/@jonocarroll)), or via email ([hello@jcarroll.com.au](mailto:hello@jcarroll.com.au)).\n",
				"date_published": "2024-06-01T14:57:26+09:30",
				"url": "https://jcarroll.xyz/2024/06/01/how-do-you.html"
			},
			{
				"id": "http://jonocarroll.micro.blog/2024/05/06/on-content-creation.html",
				"title": "On content creation motivation",
				"content_html": "<p>(this was going to be a (series of) toot(s) but it got too waaay too long)</p>\n<p>I enjoyed reading a blog post this morning, linked there from some other site. As I always do, I checked their other recent posts to see if I want to add them to my RSS feed service, and found one explaining why they&rsquo;re moving to a hosted service:</p>\n<ul>\n<li>their least favourite part of having a blog is dealing with email delivery</li>\n<li>they might get more readers over there</li>\n</ul>\n<p>This perplexed me for a moment, but is probably more common than I assumed. I have zero interest in email delivery of my blog posts - if you want to see posts from my main site as they&rsquo;re available, it <a href=\"https://jcarroll.com.au/index.xml\">has RSS</a> (filtered <a href=\"https://jcarroll.com.au/tags/rstats/index.xml\">by tag</a> if you want to be specific). This secondary site does appear to have <a href=\"https://jcarroll.xyz/feed.xml\">a feed</a>, but even I had to dig into the source to find it. To that other author, though, it was a significant aspect of their blogging experience.</p>\n<p>That desire for &ldquo;distribution&rdquo;, along with the second point about &ldquo;gaining readers&rdquo; is, I think, the reason I&rsquo;m disillusioned with so many people I previously enjoyed following - they&rsquo;re making content solely &ldquo;for the views&rdquo; or &ldquo;to go viral&rdquo; as if that translates to anything useful. Bad content goes viral, too, and any viral content is viral for mere days anyway. I appreciate that freelance developers rely on networking to find work - I do, too - but I don&rsquo;t believe there&rsquo;s a dose-effect relationship between &ldquo;number of views&rdquo; and &ldquo;job opportunities&rdquo;, especially if the &ldquo;views&rdquo; are for a post that only exists for the sake of &ldquo;the views&rdquo;.</p>\n<p>I&rsquo;ve stopped following people who spend 3/4 of their posts complaining about their view count or how to &ldquo;get more followers&rdquo;. I write my posts mainly for an audience of 1 (future me) and because I hope things in there could be useful to other people if they pop up in search results later. I&rsquo;d love if something I&rsquo;ve written sparked a conversation with a reader, but I don&rsquo;t think pushing my content on to more random screens will necessarily help with that.</p>\n<p>I&rsquo;ve seen developers I regularly followed (because they had interesting opinions or insights about things I am interested in) &ldquo;go all in on content creation&rdquo; which invariably meant silly faces in YouTube previews, making &lsquo;reaction videos&rsquo; with 0 novel insights, and focussing on the controversial, because &ldquo;views&rdquo;. The content they previously made &ldquo;because it was interesting to them&rdquo; is way more interesting to viewers, and typically useful, too - it&rsquo;s why I was following them. Maybe that&rsquo;s me being naive, though - maybe that $20 from YouTube for their 20,000 views was an important part of their &ldquo;brand&rdquo;.</p>\n<p>I&rsquo;ve also seen a big rise in the &ldquo;three paragraph blog post submitted to a post aggregator&rdquo; style of blog. I help aggregate <a href=\"https://rweekly.org/\">RWeekly</a> because I believe it&rsquo;s a useful contribution to people discovering interesting content, and that the human curation of it makes it a more valuable resource than an alternative (ad-supported) dump of all the posts. I&rsquo;m seeing more and more posts with very little insight or information titled &lsquo;exploring&rsquo; or &lsquo;guide to&rsquo; which have one call to the function and an explanation of the arguments. Not more help than the help docs themselves, but hey, another post to share&hellip; views!</p>\n<p>This mindset is probably part of why the web is filling up with <a href=\"https://www.rostrum.blog/posts/2024-03-15-ai-garbage/\">AI-generated rubbish</a> - it&rsquo;s &ldquo;easy&rdquo; to make and potentially leads to &ldquo;views&rdquo;. But it&rsquo;s still rubbish in the same way that the human-made &ldquo;for the views&rdquo; stuff is rubbish. It&rsquo;s quantity over quality and I&rsquo;m hardly the first to complain about it.</p>\n<p>A very fair question would then be, <em>&ldquo;Jono, why are you writing and publishing and linking to this if not for &lsquo;the views&rsquo;?&quot;</em> and to that I would say that I&rsquo;m actually interested in the discussion and am trying to initiate it. I have no interest in the readership counts of my posts. This would have been a handful of toots if it wasn&rsquo;t so long. One feature of Mastodon I&rsquo;m really enjoying is the lack of focus on the &lsquo;like count&rsquo; of any posts - you can get to some of that data if you want, but otherwise it&rsquo;s just a &lsquo;star&rsquo; that you&rsquo;ve either clicked or not clicked. I use it as a nod to the author to say &ldquo;hey, I enjoyed this - thanks!&rdquo; but not having an algorithm watching that and relying on it to promote a post is freeing. I&rsquo;ve checked back in on Twitter occasionally and it&rsquo;s almost all &ldquo;for the views&rdquo; content now, while over on Mastodon I&rsquo;ve been enjoying regular updates of someone&rsquo;s train journey across the USA, simply because I follow them.</p>\n<p>Do other people find the &lsquo;web&rsquo; going the same way? Am I just being naive? Would I get more responses to this if it had a cute cat picture? Or a vaguely relevant AI-generated header image? Or wrote something controversial? Have I gone about it all wrong and should I be providing email distribution of my new posts?</p>\n",
				"content_text": "(this was going to be a (series of) toot(s) but it got too waaay too long)\r\n\r\nI enjoyed reading a blog post this morning, linked there from some other site. As I always do, I checked their other recent posts to see if I want to add them to my RSS feed service, and found one explaining why they're moving to a hosted service:\r\n\r\n- their least favourite part of having a blog is dealing with email delivery\r\n- they might get more readers over there\r\n\r\nThis perplexed me for a moment, but is probably more common than I assumed. I have zero interest in email delivery of my blog posts - if you want to see posts from my main site as they're available, it [has RSS](https://jcarroll.com.au/index.xml) (filtered [by tag](https://jcarroll.com.au/tags/rstats/index.xml) if you want to be specific). This secondary site does appear to have [a feed](https://jcarroll.xyz/feed.xml), but even I had to dig into the source to find it. To that other author, though, it was a significant aspect of their blogging experience.\r\n\r\nThat desire for \"distribution\", along with the second point about \"gaining readers\" is, I think, the reason I'm disillusioned with so many people I previously enjoyed following - they're making content solely \"for the views\" or \"to go viral\" as if that translates to anything useful. Bad content goes viral, too, and any viral content is viral for mere days anyway. I appreciate that freelance developers rely on networking to find work - I do, too - but I don't believe there's a dose-effect relationship between \"number of views\" and \"job opportunities\", especially if the \"views\" are for a post that only exists for the sake of \"the views\".\r\n\r\nI've stopped following people who spend 3/4 of their posts complaining about their view count or how to \"get more followers\". I write my posts mainly for an audience of 1 (future me) and because I hope things in there could be useful to other people if they pop up in search results later. I'd love if something I've written sparked a conversation with a reader, but I don't think pushing my content on to more random screens will necessarily help with that.\r\n\r\nI've seen developers I regularly followed (because they had interesting opinions or insights about things I am interested in) \"go all in on content creation\" which invariably meant silly faces in YouTube previews, making 'reaction videos' with 0 novel insights, and focussing on the controversial, because \"views\". The content they previously made \"because it was interesting to them\" is way more interesting to viewers, and typically useful, too - it's why I was following them. Maybe that's me being naive, though - maybe that $20 from YouTube for their 20,000 views was an important part of their \"brand\".\r\n\r\nI've also seen a big rise in the \"three paragraph blog post submitted to a post aggregator\" style of blog. I help aggregate [RWeekly](https://rweekly.org/) because I believe it's a useful contribution to people discovering interesting content, and that the human curation of it makes it a more valuable resource than an alternative (ad-supported) dump of all the posts. I'm seeing more and more posts with very little insight or information titled 'exploring' or 'guide to' which have one call to the function and an explanation of the arguments. Not more help than the help docs themselves, but hey, another post to share... views!\r\n\r\nThis mindset is probably part of why the web is filling up with [AI-generated rubbish](https://www.rostrum.blog/posts/2024-03-15-ai-garbage/) - it's \"easy\" to make and potentially leads to \"views\". But it's still rubbish in the same way that the human-made \"for the views\" stuff is rubbish. It's quantity over quality and I'm hardly the first to complain about it.\r\n\r\nA very fair question would then be, _\"Jono, why are you writing and publishing and linking to this if not for 'the views'?\"_ and to that I would say that I'm actually interested in the discussion and am trying to initiate it. I have no interest in the readership counts of my posts. This would have been a handful of toots if it wasn't so long. One feature of Mastodon I'm really enjoying is the lack of focus on the 'like count' of any posts - you can get to some of that data if you want, but otherwise it's just a 'star' that you've either clicked or not clicked. I use it as a nod to the author to say \"hey, I enjoyed this - thanks!\" but not having an algorithm watching that and relying on it to promote a post is freeing. I've checked back in on Twitter occasionally and it's almost all \"for the views\" content now, while over on Mastodon I've been enjoying regular updates of someone's train journey across the USA, simply because I follow them. \r\n\r\nDo other people find the 'web' going the same way? Am I just being naive? Would I get more responses to this if it had a cute cat picture? Or a vaguely relevant AI-generated header image? Or wrote something controversial? Have I gone about it all wrong and should I be providing email distribution of my new posts?\n",
				"date_published": "2024-05-06T09:18:06+09:30",
				"url": "https://jcarroll.xyz/2024/05/06/on-content-creation.html"
			},
			{
				"id": "http://jonocarroll.micro.blog/2024/03/15/fuzzy-grouping.html",
				"title": "Fuzzy Grouping",
				"content_html": "<p>A former colleague got in touch recently with a question about how one might perform the following grouping operation&hellip;</p>\n<p>Given a set of medical terms that may include typos and variations (as is typically the case for real-world medical data) group &ldquo;similar&rdquo; items together in the {dplyr} sense of &ldquo;group&rdquo; (the data presumably is a table of values which need to be grouped together into medical categories).</p>\n<p>The provided example was the following: &ldquo;gastrointestinal disorders&rdquo;, &ldquo;gastrointestinal tract disorders&rdquo;, &ldquo;gastreinstestinal disorder&rdquo; (sic).</p>\n<p>I wasn&rsquo;t aware of a straightforward way to do it (by all means, please let me know if there <em>is</em> one!) so I figured it was a nice challenge.</p>\n<p>In order to deal with the typos, I figured I needed to &ldquo;spellcheck&rdquo; the entries. A regular spellchecker will complain to no end about medical terms which typically aren&rsquo;t in its &ldquo;standard&rdquo; dictionary, so I needed a &ldquo;ground-truth&rdquo; wordlist to work with.</p>\n<p>To be properly robust, a good cross-check of medical terms should probably use an ontology - and there are various options available with associated R packages - but I found this dataset which works nicely enough. I read it in to the session easily since <code>readLines</code> can take a URL of a file. I lowercased it because I don&rsquo;t want to deal with the differences that casing brings.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">terms <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">tolower</span>(<span style=\"color:#a6e22e\">readLines</span>(<span style=\"color:#e6db74\">&#34;https://raw.githubusercontent.com/socd06/medical-nlp/master/data/vocab.txt&#34;</span>))\n</code></pre></div><p>To make a larger &ldquo;data&rdquo; example I added two other medical groups with similar typos</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">gi <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">c</span>(<span style=\"color:#e6db74\">&#34;gastrointestinal disorders&#34;</span>, <span style=\"color:#e6db74\">&#34;gastrointestinal tract disorders&#34;</span>, <span style=\"color:#e6db74\">&#34;gastreinstestinal disorder&#34;</span>)\nhep <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">c</span>(<span style=\"color:#e6db74\">&#34;hepatic encephalopathy&#34;</span>, <span style=\"color:#e6db74\">&#34;hepatic encephalapathy&#34;</span>, <span style=\"color:#e6db74\">&#34;hepatic encefalopathy&#34;</span>)\nco <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">c</span>(<span style=\"color:#e6db74\">&#34;myocarditis&#34;</span>, <span style=\"color:#e6db74\">&#34;myocardits&#34;</span>, <span style=\"color:#e6db74\">&#34;myocardites&#34;</span>)\n</code></pre></div><p>In order to spellcheck, I first see if the word is in the wordlist verbatim, in which case it doesn&rsquo;t need correcting, otherwise I take the word with the smallest <a href=\"https://en.wikipedia.org/wiki/Levenshtein_distance\">Levenshtein distance</a> (edit) distance to the word, being the most likely &ldquo;correct&rdquo; spelling, via the built-in <code>adist()</code></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">match_word <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(word, wordlist) {\n  word <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">tolower</span>(word)\n  <span style=\"color:#a6e22e\">if </span>(word <span style=\"color:#f92672\">%in%</span> wordlist) <span style=\"color:#a6e22e\">return</span>(word)\n  wordlist<span style=\"color:#a6e22e\">[which.min</span>(<span style=\"color:#a6e22e\">adist</span>(word, wordlist)[1, ])]\n}\n</code></pre></div><p>This checks individual words, not entire phases, so I <code>apply</code> this over each word in a given phrase</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">spellcheck_phrase <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(phrase, wordlist) {\n  <span style=\"color:#a6e22e\">sapply</span>(phrase, <span style=\"color:#a6e22e\">\\</span>(w) <span style=\"color:#a6e22e\">paste</span>(<span style=\"color:#a6e22e\">sapply</span>(<span style=\"color:#a6e22e\">strsplit</span>(w, <span style=\"color:#e6db74\">&#34; &#34;</span>)[[1]], <span style=\"color:#a6e22e\">\\</span>(word) <span style=\"color:#a6e22e\">match_word</span>(word, wordlist)), collapse <span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">&#34; &#34;</span>), USE.NAMES <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">FALSE</span>)\n}\n</code></pre></div><p>As a test, the corrected spellings of the GI terms</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\"><span style=\"color:#a6e22e\">spellcheck_phrase</span>(gi, terms)\n<span style=\"color:#75715e\">#&gt; [1] &#34;gastrointestinal disorders&#34;       &#34;gastrointestinal tract disorders&#34;</span>\n<span style=\"color:#75715e\">#&gt; [3] &#34;gastrointestinal disorder&#34;</span>\n</code></pre></div><p>These still aren&rsquo;t &ldquo;groupable&rdquo; yet; they&rsquo;re spelled correctly but they&rsquo;re not all the same.</p>\n<p>In order to properly reproduce the &lsquo;grouping&rsquo; I created an example dataset with some &ldquo;values&rdquo;</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">meddata <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">data.frame</span>(term <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">c</span>(gi, hep, co), value <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">LETTERS</span>[1<span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">9</span>])\n</code></pre></div><p>Then shuffled them</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">meddata <span style=\"color:#f92672\">&lt;-</span> meddata<span style=\"color:#a6e22e\">[match</span>(meddata<span style=\"color:#f92672\">$</span>value, <span style=\"color:#a6e22e\">strsplit</span>(<span style=\"color:#e6db74\">&#34;FIABDEHCG&#34;</span>, <span style=\"color:#e6db74\">&#34;&#34;</span>)[[1]]), ]\nmeddata\n<span style=\"color:#75715e\">#&gt;                               term value</span>\n<span style=\"color:#75715e\">#&gt; 3       gastreinstestinal disorder     C</span>\n<span style=\"color:#75715e\">#&gt; 4           hepatic encephalopathy     D</span>\n<span style=\"color:#75715e\">#&gt; 8                       myocardits     H</span>\n<span style=\"color:#75715e\">#&gt; 5           hepatic encephalapathy     E</span>\n<span style=\"color:#75715e\">#&gt; 6            hepatic encefalopathy     F</span>\n<span style=\"color:#75715e\">#&gt; 1       gastrointestinal disorders     A</span>\n<span style=\"color:#75715e\">#&gt; 9                      myocardites     I</span>\n<span style=\"color:#75715e\">#&gt; 7                      myocarditis     G</span>\n<span style=\"color:#75715e\">#&gt; 2 gastrointestinal tract disorders     B</span>\n</code></pre></div><p>Next, I added the &ldquo;corrected&rdquo; spellings to this</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">meddata<span style=\"color:#f92672\">$</span>corrected <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">sapply</span>(meddata<span style=\"color:#f92672\">$</span>term, <span style=\"color:#a6e22e\">\\</span>(x) <span style=\"color:#a6e22e\">spellcheck_phrase</span>(x, terms), USE.NAMES <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">FALSE</span>)\nmeddata\n<span style=\"color:#75715e\">#&gt;                               term value                        corrected</span>\n<span style=\"color:#75715e\">#&gt; 3       gastreinstestinal disorder     C        gastrointestinal disorder</span>\n<span style=\"color:#75715e\">#&gt; 4           hepatic encephalopathy     D           hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 8                       myocardits     H                      myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 5           hepatic encephalapathy     E           hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 6            hepatic encefalopathy     F           hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 1       gastrointestinal disorders     A       gastrointestinal disorders</span>\n<span style=\"color:#75715e\">#&gt; 9                      myocardites     I                      myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 7                      myocarditis     G                      myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 2 gastrointestinal tract disorders     B gastrointestinal tract disorders</span>\n</code></pre></div><p>Now for the actual grouping: I used the <a href=\"https://github.com/beniaminogreen/zoomerjoin\">{zoomerjoin}</a> package to calculate the <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\">Jaccard similarity</a> and perform the grouping</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">meddata<span style=\"color:#f92672\">$</span>group <span style=\"color:#f92672\">&lt;-</span> zoomerjoin<span style=\"color:#f92672\">::</span><span style=\"color:#a6e22e\">jaccard_string_group</span>(meddata<span style=\"color:#f92672\">$</span>corrected, threshold <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">0.1</span>)\n<span style=\"color:#75715e\">#&gt; Loading required namespace: igraph</span>\nmeddata\n<span style=\"color:#75715e\">#&gt;                               term value                        corrected</span>\n<span style=\"color:#75715e\">#&gt; 3       gastreinstestinal disorder     C        gastrointestinal disorder</span>\n<span style=\"color:#75715e\">#&gt; 4           hepatic encephalopathy     D           hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 8                       myocardits     H                      myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 5           hepatic encephalapathy     E           hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 6            hepatic encefalopathy     F           hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 1       gastrointestinal disorders     A       gastrointestinal disorders</span>\n<span style=\"color:#75715e\">#&gt; 9                      myocardites     I                      myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 7                      myocarditis     G                      myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 2 gastrointestinal tract disorders     B gastrointestinal tract disorders</span>\n<span style=\"color:#75715e\">#&gt;                       group</span>\n<span style=\"color:#75715e\">#&gt; 3 gastrointestinal disorder</span>\n<span style=\"color:#75715e\">#&gt; 4    hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 8               myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 5    hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 6    hepatic encephalopathy</span>\n<span style=\"color:#75715e\">#&gt; 1 gastrointestinal disorder</span>\n<span style=\"color:#75715e\">#&gt; 9               myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 7               myocarditis</span>\n<span style=\"color:#75715e\">#&gt; 2 gastrointestinal disorder</span>\n</code></pre></div><p>This sets a &ldquo;canonical&rdquo; value for the name of each group, and the threshold may need adjusting with more &ldquo;real&rdquo; data, but it appears to work!</p>\n<p>Performing the actual grouping shows that the &ldquo;values&rdquo; of each group have been recovered</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\"><span style=\"color:#a6e22e\">library</span>(dplyr)\n\nmeddata <span style=\"color:#f92672\">|&gt;</span> \n  <span style=\"color:#a6e22e\">group_by</span>(group) <span style=\"color:#f92672\">|&gt;</span> \n  <span style=\"color:#a6e22e\">summarise</span>(res <span style=\"color:#f92672\">=</span> <span style=\"color:#a6e22e\">toString</span>(<span style=\"color:#a6e22e\">sort</span>(value)))\n<span style=\"color:#75715e\">#&gt; # A tibble: 3 × 2</span>\n<span style=\"color:#75715e\">#&gt;   group                     res    </span>\n<span style=\"color:#75715e\">#&gt;   &lt;chr&gt;                     &lt;chr&gt;  </span>\n<span style=\"color:#75715e\">#&gt; 1 gastrointestinal disorder A, B, C</span>\n<span style=\"color:#75715e\">#&gt; 2 hepatic encephalopathy    D, E, F</span>\n<span style=\"color:#75715e\">#&gt; 3 myocarditis               G, H, I</span>\n</code></pre></div><p>I&rsquo;m curious to know if anyone has a better/different approach? Please let me know either here, on <a href=\"https://fosstodon.org/@jonocarroll\">Mastodon</a>, or any way you can find me. A gist of this code and markup is available <a href=\"https://gist.github.com/jonocarroll/e18f6eeee39875ccd7c4a65411ff1d0a\">here</a>.</p>\n",
				"content_text": "A former colleague got in touch recently with a question about how one might perform the following grouping operation... \r\n\r\nGiven a set of medical terms that may include typos and variations (as is typically the case for real-world medical data) group \"similar\" items together in the {dplyr} sense of \"group\" (the data presumably is a table of values which need to be grouped together into medical categories). \r\n\r\nThe provided example was the following: \"gastrointestinal disorders\", \"gastrointestinal tract disorders\", \"gastreinstestinal disorder\" (sic).\r\n\r\nI wasn't aware of a straightforward way to do it (by all means, please let me know if there _is_ one!) so I figured it was a nice challenge.\r\n\r\nIn order to deal with the typos, I figured I needed to \"spellcheck\" the entries. A regular spellchecker will complain to no end about medical terms which typically aren't in its \"standard\" dictionary, so I needed a \"ground-truth\" wordlist to work with.\r\n\r\nTo be properly robust, a good cross-check of medical terms should probably use an ontology - and there are various options available with associated R packages - but I found this dataset which works nicely enough. I read it in to the session easily since `readLines` can take a URL of a file. I lowercased it because I don't want to deal with the differences that casing brings.\r\n\r\n```r\r\nterms <- tolower(readLines(\"https://raw.githubusercontent.com/socd06/medical-nlp/master/data/vocab.txt\"))\r\n```\r\n\r\nTo make a larger \"data\" example I added two other medical groups with similar typos\r\n\r\n```r\r\ngi <- c(\"gastrointestinal disorders\", \"gastrointestinal tract disorders\", \"gastreinstestinal disorder\")\r\nhep <- c(\"hepatic encephalopathy\", \"hepatic encephalapathy\", \"hepatic encefalopathy\")\r\nco <- c(\"myocarditis\", \"myocardits\", \"myocardites\")\r\n```\r\n\r\nIn order to spellcheck, I first see if the word is in the wordlist verbatim, in which case it doesn't need correcting, otherwise I take the word with the smallest [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) (edit) distance to the word, being the most likely \"correct\" spelling, via the built-in `adist()`\r\n\r\n```r\r\nmatch_word <- function(word, wordlist) {\r\n  word <- tolower(word)\r\n  if (word %in% wordlist) return(word)\r\n  wordlist[which.min(adist(word, wordlist)[1, ])]\r\n}\r\n```\r\n\r\nThis checks individual words, not entire phases, so I `apply` this over each word in a given phrase\r\n\r\n```r\r\nspellcheck_phrase <- function(phrase, wordlist) {\r\n  sapply(phrase, \\(w) paste(sapply(strsplit(w, \" \")[[1]], \\(word) match_word(word, wordlist)), collapse = \" \"), USE.NAMES = FALSE)\r\n}\r\n```\r\n\r\nAs a test, the corrected spellings of the GI terms\r\n\r\n```r\r\nspellcheck_phrase(gi, terms)\r\n#> [1] \"gastrointestinal disorders\"       \"gastrointestinal tract disorders\"\r\n#> [3] \"gastrointestinal disorder\"\r\n```\r\n\r\nThese still aren't \"groupable\" yet; they're spelled correctly but they're not all the same. \r\n\r\nIn order to properly reproduce the 'grouping' I created an example dataset with some \"values\"\r\n\r\n```r\r\nmeddata <- data.frame(term = c(gi, hep, co), value = LETTERS[1:9])\r\n```\r\n\r\nThen shuffled them\r\n\r\n```r\r\nmeddata <- meddata[match(meddata$value, strsplit(\"FIABDEHCG\", \"\")[[1]]), ]\r\nmeddata\r\n#>                               term value\r\n#> 3       gastreinstestinal disorder     C\r\n#> 4           hepatic encephalopathy     D\r\n#> 8                       myocardits     H\r\n#> 5           hepatic encephalapathy     E\r\n#> 6            hepatic encefalopathy     F\r\n#> 1       gastrointestinal disorders     A\r\n#> 9                      myocardites     I\r\n#> 7                      myocarditis     G\r\n#> 2 gastrointestinal tract disorders     B\r\n```\r\n\r\nNext, I added the \"corrected\" spellings to this\r\n\r\n```r\r\nmeddata$corrected <- sapply(meddata$term, \\(x) spellcheck_phrase(x, terms), USE.NAMES = FALSE)\r\nmeddata\r\n#>                               term value                        corrected\r\n#> 3       gastreinstestinal disorder     C        gastrointestinal disorder\r\n#> 4           hepatic encephalopathy     D           hepatic encephalopathy\r\n#> 8                       myocardits     H                      myocarditis\r\n#> 5           hepatic encephalapathy     E           hepatic encephalopathy\r\n#> 6            hepatic encefalopathy     F           hepatic encephalopathy\r\n#> 1       gastrointestinal disorders     A       gastrointestinal disorders\r\n#> 9                      myocardites     I                      myocarditis\r\n#> 7                      myocarditis     G                      myocarditis\r\n#> 2 gastrointestinal tract disorders     B gastrointestinal tract disorders\r\n```\r\n\r\nNow for the actual grouping: I used the [{zoomerjoin}](https://github.com/beniaminogreen/zoomerjoin) package to calculate the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) and perform the grouping\r\n\r\n```r\r\nmeddata$group <- zoomerjoin::jaccard_string_group(meddata$corrected, threshold = 0.1)\r\n#> Loading required namespace: igraph\r\nmeddata\r\n#>                               term value                        corrected\r\n#> 3       gastreinstestinal disorder     C        gastrointestinal disorder\r\n#> 4           hepatic encephalopathy     D           hepatic encephalopathy\r\n#> 8                       myocardits     H                      myocarditis\r\n#> 5           hepatic encephalapathy     E           hepatic encephalopathy\r\n#> 6            hepatic encefalopathy     F           hepatic encephalopathy\r\n#> 1       gastrointestinal disorders     A       gastrointestinal disorders\r\n#> 9                      myocardites     I                      myocarditis\r\n#> 7                      myocarditis     G                      myocarditis\r\n#> 2 gastrointestinal tract disorders     B gastrointestinal tract disorders\r\n#>                       group\r\n#> 3 gastrointestinal disorder\r\n#> 4    hepatic encephalopathy\r\n#> 8               myocarditis\r\n#> 5    hepatic encephalopathy\r\n#> 6    hepatic encephalopathy\r\n#> 1 gastrointestinal disorder\r\n#> 9               myocarditis\r\n#> 7               myocarditis\r\n#> 2 gastrointestinal disorder\r\n```\r\n\r\nThis sets a \"canonical\" value for the name of each group, and the threshold may need adjusting with more \"real\" data, but it appears to work!\r\n\r\nPerforming the actual grouping shows that the \"values\" of each group have been recovered\r\n\r\n```r\r\nlibrary(dplyr)\r\n\r\nmeddata |> \r\n  group_by(group) |> \r\n  summarise(res = toString(sort(value)))\r\n#> # A tibble: 3 × 2\r\n#>   group                     res    \r\n#>   <chr>                     <chr>  \r\n#> 1 gastrointestinal disorder A, B, C\r\n#> 2 hepatic encephalopathy    D, E, F\r\n#> 3 myocarditis               G, H, I\r\n```\r\n\r\nI'm curious to know if anyone has a better/different approach? Please let me know either here, on [Mastodon](https://fosstodon.org/@jonocarroll), or any way you can find me. A gist of this code and markup is available [here](https://gist.github.com/jonocarroll/e18f6eeee39875ccd7c4a65411ff1d0a).\n",
				"date_published": "2024-03-15T12:51:32+09:30",
				"url": "https://jcarroll.xyz/2024/03/15/fuzzy-grouping.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2024/02/10/friends-romans-countrymen.html",
				"title": "Friends, Romans, Countrymen, lend me your debuggers",
				"content_html": "<p>This was just going to be a short toot, but it keeps growing so I moved it over to here.</p>\n<p>I thought I was about to have the shortest &ldquo;cheat&rdquo; solution to an #Exercism problem - <a href=\"https://exercism.org/tracks/r/exercises/roman-numerals\">Roman Numerals</a> - where we need to convert a number to roman. R already has <code>as.roman()</code> though it returns an object of class &lsquo;roman&rsquo; and the tests expect a character result, so fine, <code>as.character(as.roman(arabic))</code>.</p>\n<p>It works great - passes 25 tests with just that. But I get one test failure&hellip; <code>as.roman(3999)</code> produces <code>NA</code> which is documented</p>\n<blockquote>\n<p>Only numbers between 1 and 3899 have a unique representation as roman numbers, and hence others result in as.roman(NA).</p>\n</blockquote>\n<p>The Exercism problem states we can assume input is only up to 3999 which makes sense, because according to <a href=\"https://en.wikipedia.org/wiki/Roman_numerals#:~:text=The%20largest%20number%20that%20can%20be%20represented%20in%20this%20manner\">Wikipedia that&rsquo;s the largest number that has a unique representation</a>. So, why then does R stop at 3899???</p>\n<p>I thought it was just enforced in the code, since <code>utils::as.roman()</code> calls the unexported <code>utils:::.as.roman()</code> which takes a <code>check.range = TRUE</code> argument (this function is not documented). Sure enough, within that function is</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\"><span style=\"color:#a6e22e\">if </span>(check.range) \n        x[x <span style=\"color:#f92672\">&lt;=</span> <span style=\"color:#ae81ff\">0L</span> <span style=\"color:#f92672\">|</span> x <span style=\"color:#f92672\">&gt;=</span> <span style=\"color:#ae81ff\">3900L</span>] <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#66d9ef\">NA</span>\n    <span style=\"color:#a6e22e\">class</span>(x) <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#e6db74\">&#34;roman&#34;</span>\n    x\n</code></pre></div><p>but if I set that argument to <code>FALSE</code> I still get <code>NA</code></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">utils<span style=\"color:#f92672\">:::</span><span style=\"color:#a6e22e\">.as.roman</span>(<span style=\"color:#ae81ff\">3999</span>)\n[1] <span style=\"color:#f92672\">&lt;</span><span style=\"color:#66d9ef\">NA</span><span style=\"color:#f92672\">&gt;</span>\nutils<span style=\"color:#f92672\">:::</span><span style=\"color:#a6e22e\">.as.roman</span>(<span style=\"color:#ae81ff\">3999</span>, check.range <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">FALSE</span>)\n[1] <span style=\"color:#f92672\">&lt;</span><span style=\"color:#66d9ef\">NA</span><span style=\"color:#f92672\">&gt;</span>\n</code></pre></div><p>so what&rsquo;s the point of that?</p>\n<p>I ran through this with my favourite debug tool <code>debugonce()</code> and the only line that actually does anything is immediately following this check</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\"><span style=\"color:#a6e22e\">class</span>(x) <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#e6db74\">&#34;roman&#34;</span>\nx\n</code></pre></div><p>Up until then, <code>x</code> is still the numeric input value. Setting the class on it seems to perform the conversion, but I don&rsquo;t quite see how that is invoked. <code>as.roman()</code> isn&rsquo;t being called - I set a <code>debug</code> flag on that and it wasn&rsquo;t invoked. I set another on <code>utils:::.as.character.roman()</code> and it <em>was</em> invoked, but I don&rsquo;t quite see the path to that yet.</p>\n<p>I also see <code>utils:::.numeric2roman()</code> which seems to do the conversion but sets the value to <code>NA</code> if it&rsquo;s larger than 3899, and has no way to avoid that check.</p>\n<p>So&hellip; bug report? There doesn&rsquo;t appear to be one involving the fact that the values 3900-3999 should be representable - at least not as far as <a href=\"https://bugs.r-project.org/buglist.cgi?bug_status=UNCONFIRMED&amp;bug_status=NEW&amp;bug_status=CONFIRMED&amp;bug_status=ASSIGNED&amp;bug_status=REOPENED&amp;bug_status=RESOLVED&amp;bug_status=VERIFIED&amp;bug_status=CLOSED&amp;f0=OP&amp;f1=OP&amp;f2=product&amp;f3=component&amp;f4=alias&amp;f5=short_desc&amp;f7=content&amp;f8=CP&amp;f9=CP&amp;j1=OR&amp;o2=substring&amp;o3=substring&amp;o4=substring&amp;o5=substring&amp;o6=substring&amp;o7=matches&amp;query_format=advanced&amp;short_desc=roman&amp;short_desc_type=allwordssubstr&amp;v2=roman&amp;v3=roman&amp;v4=roman&amp;v5=roman&amp;v6=roman&amp;v7=%22roman%22\">I can tell in Bugzilla</a>.</p>\n<p>I figured I&rsquo;d make a post about it first and see if anyone has any further insights, otherwise I might work up the courage to email r-devel.</p>\n",
				"content_text": "This was just going to be a short toot, but it keeps growing so I moved it over to here.\r\n\r\nI thought I was about to have the shortest \"cheat\" solution to an #Exercism problem - [Roman Numerals](https://exercism.org/tracks/r/exercises/roman-numerals) - where we need to convert a number to roman. R already has `as.roman()` though it returns an object of class 'roman' and the tests expect a character result, so fine, `as.character(as.roman(arabic))`.\r\n\r\nIt works great - passes 25 tests with just that. But I get one test failure... `as.roman(3999)` produces `NA` which is documented\r\n\r\n> Only numbers between 1 and 3899 have a unique representation as roman numbers, and hence others result in as.roman(NA).\r\n\r\nThe Exercism problem states we can assume input is only up to 3999 which makes sense, because according to [Wikipedia that's the largest number that has a unique representation](https://en.wikipedia.org/wiki/Roman_numerals#:~:text=The%20largest%20number%20that%20can%20be%20represented%20in%20this%20manner). So, why then does R stop at 3899???\r\n\r\nI thought it was just enforced in the code, since `utils::as.roman()` calls the unexported `utils:::.as.roman()` which takes a `check.range = TRUE` argument (this function is not documented). Sure enough, within that function is \r\n\r\n```r\r\nif (check.range) \r\n        x[x <= 0L | x >= 3900L] <- NA\r\n    class(x) <- \"roman\"\r\n    x\r\n```\r\n\r\nbut if I set that argument to `FALSE` I still get `NA`\r\n\r\n```r\r\nutils:::.as.roman(3999)\r\n[1] <NA>\r\nutils:::.as.roman(3999, check.range = FALSE)\r\n[1] <NA>\r\n```\r\nso what's the point of that?\r\n\r\nI ran through this with my favourite debug tool `debugonce()` and the only line that actually does anything is immediately following this check\r\n\r\n```r\r\nclass(x) <- \"roman\"\r\nx\r\n```\r\n\r\nUp until then, `x` is still the numeric input value. Setting the class on it seems to perform the conversion, but I don't quite see how that is invoked. `as.roman()` isn't being called - I set a `debug` flag on that and it wasn't invoked. I set another on `utils:::.as.character.roman()` and it _was_ invoked, but I don't quite see the path to that yet.\r\n\r\nI also see `utils:::.numeric2roman()` which seems to do the conversion but sets the value to `NA` if it's larger than 3899, and has no way to avoid that check.\r\n\r\nSo... bug report? There doesn't appear to be one involving the fact that the values 3900-3999 should be representable - at least not as far as [I can tell in Bugzilla](https://bugs.r-project.org/buglist.cgi?bug_status=UNCONFIRMED&bug_status=NEW&bug_status=CONFIRMED&bug_status=ASSIGNED&bug_status=REOPENED&bug_status=RESOLVED&bug_status=VERIFIED&bug_status=CLOSED&f0=OP&f1=OP&f2=product&f3=component&f4=alias&f5=short_desc&f7=content&f8=CP&f9=CP&j1=OR&o2=substring&o3=substring&o4=substring&o5=substring&o6=substring&o7=matches&query_format=advanced&short_desc=roman&short_desc_type=allwordssubstr&v2=roman&v3=roman&v4=roman&v5=roman&v6=roman&v7=%22roman%22).\r\n\r\nI figured I'd make a post about it first and see if anyone has any further insights, otherwise I might work up the courage to email r-devel.\n",
				"date_published": "2024-02-10T15:57:10+09:30",
				"url": "https://jcarroll.xyz/2024/02/10/friends-romans-countrymen.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/11/16/breaking-down-squared.html",
				"title": "Breaking down squared digits in (Dyalog) APL",
				"content_html": "<p>I love small challenges as a way to learn a language. I spotted <a href=\"https://mastodon.social/@krz/111415569841445388\">this one</a> in the #rstats hashtag on Mastodon</p>\n<blockquote>\n<p>&ldquo;square each digit of an integer (return integer), eg 9113 becomes 81119&rdquo;</p>\n</blockquote>\n<p>and I just <em>had</em> to try it in APL.</p>\n<p>First, extract each of the digits using format</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      <span style=\"color:#66d9ef\">{</span><span style=\"color:#f92672\">⍎</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#f92672\">⍕</span>⍵<span style=\"color:#66d9ef\">}</span><span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#ae81ff\">9</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#ae81ff\">3</span>\n</code></pre></div><p>then square each digit, using the commute operator to make <code>2*x</code> into <code>x*2</code> (<code>*</code> is power)</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      <span style=\"color:#66d9ef\">{</span><span style=\"color:#ae81ff\">2</span><span style=\"color:#f92672\">*</span><span style=\"color:#a6e22e\">⍨</span><span style=\"color:#f92672\">⍎</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#f92672\">⍕</span>⍵<span style=\"color:#66d9ef\">}</span><span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#ae81ff\">81</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#ae81ff\">9</span>\n</code></pre></div><p>The  next bit took me a bit of trying, but I got there (please do let me know if there&rsquo;s an easier way)&hellip; extract each digit as a character</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      <span style=\"color:#66d9ef\">{</span><span style=\"color:#f92672\">⍕</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#ae81ff\">2</span><span style=\"color:#f92672\">*</span><span style=\"color:#a6e22e\">⍨</span><span style=\"color:#f92672\">⍎</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#f92672\">⍕</span>⍵<span style=\"color:#66d9ef\">}</span><span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#960050;background-color:#1e0010\">┌──┬─┬─┬─┐</span>\n<span style=\"color:#960050;background-color:#1e0010\">│</span><span style=\"color:#ae81ff\">81</span><span style=\"color:#960050;background-color:#1e0010\">│</span><span style=\"color:#ae81ff\">1</span><span style=\"color:#960050;background-color:#1e0010\">│</span><span style=\"color:#ae81ff\">1</span><span style=\"color:#960050;background-color:#1e0010\">│</span><span style=\"color:#ae81ff\">9</span><span style=\"color:#960050;background-color:#1e0010\">│</span>\n<span style=\"color:#960050;background-color:#1e0010\">└──┴─┴─┴─┘</span>\n</code></pre></div><p>then catenate-reduce</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      <span style=\"color:#66d9ef\">{</span><span style=\"color:#f92672\">,</span><span style=\"color:#a6e22e\">/</span><span style=\"color:#f92672\">⍕</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#ae81ff\">2</span><span style=\"color:#f92672\">*</span><span style=\"color:#a6e22e\">⍨</span><span style=\"color:#f92672\">⍎</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#f92672\">⍕</span>⍵<span style=\"color:#66d9ef\">}</span><span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#960050;background-color:#1e0010\">┌─────┐</span>\n<span style=\"color:#960050;background-color:#1e0010\">│</span><span style=\"color:#ae81ff\">81119</span><span style=\"color:#960050;background-color:#1e0010\">│</span>\n<span style=\"color:#960050;background-color:#1e0010\">└─────┘</span>\n</code></pre></div><p>and finally enlist down to a scalar and parse as a number again</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      <span style=\"color:#66d9ef\">{</span><span style=\"color:#f92672\">⍎∊,</span><span style=\"color:#a6e22e\">/</span><span style=\"color:#f92672\">⍕</span><span style=\"color:#a6e22e\">¨</span>(<span style=\"color:#ae81ff\">2</span><span style=\"color:#f92672\">*</span><span style=\"color:#a6e22e\">⍨</span><span style=\"color:#f92672\">⍎</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#f92672\">⍕</span>⍵)<span style=\"color:#66d9ef\">}</span><span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#ae81ff\">81119</span>\n</code></pre></div><p>Wrapping it into a named defun, it&rsquo;s not all that bad</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      squaredigits<span style=\"color:#66d9ef\">←</span><span style=\"color:#66d9ef\">{</span><span style=\"color:#f92672\">⍎∊,</span><span style=\"color:#a6e22e\">/</span><span style=\"color:#f92672\">⍕</span><span style=\"color:#a6e22e\">¨</span>(<span style=\"color:#ae81ff\">2</span><span style=\"color:#f92672\">*</span><span style=\"color:#a6e22e\">⍨</span><span style=\"color:#f92672\">⍎</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#f92672\">⍕</span>⍵)<span style=\"color:#66d9ef\">}</span>\n      squaredigits <span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#ae81ff\">81119</span>\n</code></pre></div><p>Is it as nice as the Julia solution?</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-julia\" data-lang=\"julia\">parse(<span style=\"color:#66d9ef\">Int</span>, join(reverse(digits(num))<span style=\"color:#f92672\">.^</span><span style=\"color:#ae81ff\">2</span>))\n</code></pre></div><p>or, as mentioned in a reply</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-julia\" data-lang=\"julia\"><span style=\"color:#66d9ef\">using</span> Chain\n<span style=\"color:#a6e22e\">@chain</span> num digits reverse _<span style=\"color:#f92672\">.^</span><span style=\"color:#ae81ff\">2</span> join parse(<span style=\"color:#66d9ef\">Int</span>, _)\n</code></pre></div><p>No - I think in this case Julia does a much nicer job; easier to reason and interpret but that comes from having nice wrappers for things like <code>digits</code> (which returns them in reverse order, requiring the <code>reverse</code>). I could absolutely do that in APL, I&rsquo;ve just been trying to do it entirely with primitives, but nice wrappers are nice&hellip;</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      digits<span style=\"color:#66d9ef\">←</span><span style=\"color:#f92672\">⍎</span><span style=\"color:#a6e22e\">¨</span><span style=\"color:#f92672\">⍕</span>\n      digits <span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#ae81ff\">9</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#ae81ff\">3</span>\n</code></pre></div><p>same for <code>join</code></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      join<span style=\"color:#66d9ef\">←</span><span style=\"color:#f92672\">⍎</span><span style=\"color:#a6e22e\">∘</span><span style=\"color:#f92672\">∊</span>(<span style=\"color:#f92672\">,</span><span style=\"color:#a6e22e\">/</span><span style=\"color:#f92672\">⍕</span><span style=\"color:#a6e22e\">¨</span>)\n      join <span style=\"color:#ae81ff\">81</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#ae81ff\">1</span> <span style=\"color:#ae81ff\">9</span>\n<span style=\"color:#ae81ff\">81119</span>\n</code></pre></div><p>and heck, square</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      square<span style=\"color:#66d9ef\">←</span><span style=\"color:#66d9ef\">{</span><span style=\"color:#ae81ff\">2</span><span style=\"color:#f92672\">*</span><span style=\"color:#a6e22e\">⍨</span>⍵<span style=\"color:#66d9ef\">}</span>\n      square <span style=\"color:#ae81ff\">9</span>\n<span style=\"color:#ae81ff\">81</span>\n</code></pre></div><p>or, since I no longer need the arguments in that order, simply</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      square<span style=\"color:#66d9ef\">←</span><span style=\"color:#66d9ef\">{</span>⍵<span style=\"color:#f92672\">*</span><span style=\"color:#ae81ff\">2</span><span style=\"color:#66d9ef\">}</span>\n      square <span style=\"color:#ae81ff\">9</span>\n<span style=\"color:#ae81ff\">81</span>\n</code></pre></div><p>so then the APL solution, via composing these three functions, becomes simply</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      squaredigits<span style=\"color:#66d9ef\">←</span>join<span style=\"color:#a6e22e\">∘</span>square<span style=\"color:#a6e22e\">∘</span>digits\n      squaredigits <span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#ae81ff\">81119</span>\n</code></pre></div><p>Now <em>THAT&rsquo;s</em> readable!</p>\n<p>And yes, it&rsquo;s still a number</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">      <span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">+</span>squaredigits <span style=\"color:#ae81ff\">9113</span>\n<span style=\"color:#ae81ff\">81120</span>\n</code></pre></div><p>Want to see how it works? APL prints out the AST for a function like this</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-apl\" data-lang=\"apl\">squaredigits\n      <span style=\"color:#a6e22e\">∘</span>      \n     <span style=\"color:#960050;background-color:#1e0010\">┌┴┐</span>     \n     <span style=\"color:#a6e22e\">∘</span> <span style=\"color:#a6e22e\">¨</span>     \n   <span style=\"color:#960050;background-color:#1e0010\">┌─┴──┐</span>    \n <span style=\"color:#960050;background-color:#1e0010\">┌─┴──┐</span> <span style=\"color:#66d9ef\">{</span>⍵<span style=\"color:#f92672\">*</span><span style=\"color:#ae81ff\">2</span><span style=\"color:#66d9ef\">}</span>\n <span style=\"color:#a6e22e\">∘</span>  <span style=\"color:#960050;background-color:#1e0010\">┌─┴─┐</span>    \n<span style=\"color:#960050;background-color:#1e0010\">┌</span>   <span style=\"color:#a6e22e\">/</span>   <span style=\"color:#a6e22e\">¨</span>    \n<span style=\"color:#f92672\">∊</span> <span style=\"color:#960050;background-color:#1e0010\">┌─┘</span> <span style=\"color:#960050;background-color:#1e0010\">┌─┘</span>    \n  <span style=\"color:#f92672\">,</span>         \n</code></pre></div><p>I&rsquo;m not nearly done loving what APL can do.</p>\n",
				"content_text": "I love small challenges as a way to learn a language. I spotted [this one](https://mastodon.social/@krz/111415569841445388) in the #rstats hashtag on Mastodon\n\n> \"square each digit of an integer (return integer), eg 9113 becomes 81119\"\n\nand I just *had* to try it in APL.\n\nFirst, extract each of the digits using format\n\n```apl\n      {⍎¨⍕⍵}9113\n9 1 1 3\n```\n\nthen square each digit, using the commute operator to make `2*x` into `x*2` (`*` is power)\n\n```apl\n      {2*⍨⍎¨⍕⍵}9113\n81 1 1 9\n```\n\nThe  next bit took me a bit of trying, but I got there (please do let me know if there's an easier way)... extract each digit as a character\n\n```apl\n      {⍕¨2*⍨⍎¨⍕⍵}9113\n┌──┬─┬─┬─┐\n│81│1│1│9│\n└──┴─┴─┴─┘\n```\n\nthen catenate-reduce\n\n```apl\n      {,/⍕¨2*⍨⍎¨⍕⍵}9113\n┌─────┐\n│81119│\n└─────┘\n```\n\nand finally enlist down to a scalar and parse as a number again\n\n```apl\n      {⍎∊,/⍕¨(2*⍨⍎¨⍕⍵)}9113\n81119\n```\n\nWrapping it into a named defun, it's not all that bad\n```apl\n      squaredigits←{⍎∊,/⍕¨(2*⍨⍎¨⍕⍵)}\n      squaredigits 9113\n81119\n```\n\nIs it as nice as the Julia solution? \n\n```julia\nparse(Int, join(reverse(digits(num)).^2))\n```\n\nor, as mentioned in a reply\n\n```julia\nusing Chain\n@chain num digits reverse _.^2 join parse(Int, _)\n```\n\nNo - I think in this case Julia does a much nicer job; easier to reason and interpret but that comes from having nice wrappers for things like `digits` (which returns them in reverse order, requiring the `reverse`). I could absolutely do that in APL, I've just been trying to do it entirely with primitives, but nice wrappers are nice...\n\n```apl\n      digits←⍎¨⍕\n      digits 9113\n9 1 1 3\n```\n\nsame for `join`\n\n```apl\n      join←⍎∘∊(,/⍕¨)\n      join 81 1 1 9\n81119\n```\n\nand heck, square\n\n```apl\n      square←{2*⍨⍵}\n      square 9\n81\n```\n\nor, since I no longer need the arguments in that order, simply\n\n```apl\n      square←{⍵*2}\n      square 9\n81\n```\n\nso then the APL solution, via composing these three functions, becomes simply\n\n```apl\n      squaredigits←join∘square∘digits\n      squaredigits 9113\n81119\n```\nNow *THAT's* readable!\n\nAnd yes, it's still a number\n\n```apl\n      1+squaredigits 9113\n81120\n```\n\nWant to see how it works? APL prints out the AST for a function like this\n\n```apl\nsquaredigits\n      ∘      \n     ┌┴┐     \n     ∘ ¨     \n   ┌─┴──┐    \n ┌─┴──┐ {⍵*2}\n ∘  ┌─┴─┐    \n┌   /   ¨    \n∊ ┌─┘ ┌─┘    \n  ,         \n```\n\nI'm not nearly done loving what APL can do.\n",
				"date_published": "2023-11-16T09:17:56+09:30",
				"url": "https://jcarroll.xyz/2023/11/16/breaking-down-squared.html",
				"tags": ["APL"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/11/10/breaking-down-fizzbuzz.html",
				"title": "Breaking down fizzbuzz in (Dyalog) APL",
				"content_html": "<p>The one-liner solution: <code>{∊(3↑(0=3 5|⍵)∪1)/'Fizz' 'Buzz' ⍵}¨⍳20</code></p>\n<p>The explanation:</p>\n<p>For each value in <code>[1, 2, ..., 20]</code>, find the boolean mask of &ldquo;is this divisible by 3 or 5?&rdquo; (vectorized).</p>\n<p><code>{0=3 5|⍵}¨⍳20</code></p>\n<pre tabindex=\"0\"><code>┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┐\n│0 0│0 0│1 0│0 0│0 1│1 0│0 0│0 0│1 0│0 1│0 0│1 0│0 0│0 0│1 1│0 0│0 0│1 0│0 0│0 1│\n└───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┘\n</code></pre><p>Union this with the value 1; if 1 doesn&rsquo;t already appear, add it. This is now a mask for whether the result\nshould be &ldquo;fizz&rdquo; <code>(1 0)</code>, &ldquo;buzz&rdquo; <code>(0 1)</code>, &ldquo;fizzbuzz&rdquo; <code>(1 1)</code>, or the number itself <code>(0 0 1)</code>.</p>\n<p><code>{(0=3 5|⍵)∪1}¨⍳20</code></p>\n<pre tabindex=\"0\"><code>┌─────┬─────┬───┬─────┬───┬───┬─────┬─────┬───┬───┬─────┬───┬─────┬─────┬───┬─────┬─────┬───┬─────┬───┐\n│0 0 1│0 0 1│1 0│0 0 1│0 1│1 0│0 0 1│0 0 1│1 0│0 1│0 0 1│1 0│0 0 1│0 0 1│1 1│0 0 1│0 0 1│1 0│0 0 1│0 1│\n└─────┴─────┴───┴─────┴───┴───┴─────┴─────┴───┴───┴─────┴───┴─────┴─────┴───┴─────┴─────┴───┴─────┴───┘\n</code></pre><p>Take 3 elements; if there are only 2, extend it to 3 (adding a 0). The combinations are then\n&ldquo;fizz&rdquo; <code>(1 0 0)</code>, &ldquo;buzz&rdquo; <code>(0 1 0)</code>, &ldquo;fizzbuzz&rdquo; <code>(1 1 0)</code>, or the number itself <code>(0 0 1)</code>.</p>\n<p><code>{3↑(0=3 5|⍵)∪1}¨⍳20</code></p>\n<pre tabindex=\"0\"><code>┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n│0 0 1│0 0 1│1 0 0│0 0 1│0 1 0│1 0 0│0 0 1│0 0 1│1 0 0│0 1 0│0 0 1│1 0 0│0 0 1│0 0 1│1 1 0│0 0 1│0 0 1│1 0 0│0 0 1│0 1 0│\n└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘\n</code></pre><p>Filter the vector <code>'Fizz' 'Buzz' ⍵</code> based on this boolean mask.</p>\n<p><code>{(3↑(0=3 5|⍵)∪1)/'Fizz' 'Buzz'⍵}¨⍳20</code></p>\n<pre tabindex=\"0\"><code>┌─┬─┬──────┬─┬──────┬──────┬─┬─┬──────┬──────┬──┬──────┬──┬──┬───────────┬──┬──┬──────┬──┬──────┐\n│1│2│┌────┐│4│┌────┐│┌────┐│7│8│┌────┐│┌────┐│11│┌────┐│13│14│┌────┬────┐│16│17│┌────┐│19│┌────┐│\n│ │ ││Fizz││ ││Buzz│││Fizz││ │ ││Fizz│││Buzz││  ││Fizz││  │  ││Fizz│Buzz││  │  ││Fizz││  ││Buzz││\n│ │ │└────┘│ │└────┘│└────┘│ │ │└────┘│└────┘│  │└────┘│  │  │└────┴────┘│  │  │└────┘│  │└────┘│\n└─┴─┴──────┴─┴──────┴──────┴─┴─┴──────┴──────┴──┴──────┴──┴──┴───────────┴──┴──┴──────┴──┴──────┘\n</code></pre><p>and finally, enlist (flatten) the vector.</p>\n<p><code>{∊(3↑(0=3 5|⍵)∪1)/'Fizz' 'Buzz'⍵}¨⍳20</code></p>\n<pre tabindex=\"0\"><code>┌─┬─┬────┬─┬────┬────┬─┬─┬────┬────┬──┬────┬──┬──┬────────┬──┬──┬────┬──┬────┐\n│1│2│Fizz│4│Buzz│Fizz│7│8│Fizz│Buzz│11│Fizz│13│14│FizzBuzz│16│17│Fizz│19│Buzz│\n└─┴─┴────┴─┴────┴────┴─┴─┴────┴────┴──┴────┴──┴──┴────────┴──┴──┴────┴──┴────┘ \n</code></pre><p>So concise.</p>\n",
				"content_text": "The one-liner solution: `{∊(3↑(0=3 5|⍵)∪1)/'Fizz' 'Buzz' ⍵}¨⍳20`\n\nThe explanation:\n\nFor each value in `[1, 2, ..., 20]`, find the boolean mask of \"is this divisible by 3 or 5?\" (vectorized).\n\n`{0=3 5|⍵}¨⍳20`\n```\n┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┐\n│0 0│0 0│1 0│0 0│0 1│1 0│0 0│0 0│1 0│0 1│0 0│1 0│0 0│0 0│1 1│0 0│0 0│1 0│0 0│0 1│\n└───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┘\n```\nUnion this with the value 1; if 1 doesn't already appear, add it. This is now a mask for whether the result\nshould be \"fizz\" `(1 0)`, \"buzz\" `(0 1)`, \"fizzbuzz\" `(1 1)`, or the number itself `(0 0 1)`.\n\n`{(0=3 5|⍵)∪1}¨⍳20`\n```\n┌─────┬─────┬───┬─────┬───┬───┬─────┬─────┬───┬───┬─────┬───┬─────┬─────┬───┬─────┬─────┬───┬─────┬───┐\n│0 0 1│0 0 1│1 0│0 0 1│0 1│1 0│0 0 1│0 0 1│1 0│0 1│0 0 1│1 0│0 0 1│0 0 1│1 1│0 0 1│0 0 1│1 0│0 0 1│0 1│\n└─────┴─────┴───┴─────┴───┴───┴─────┴─────┴───┴───┴─────┴───┴─────┴─────┴───┴─────┴─────┴───┴─────┴───┘\n```\nTake 3 elements; if there are only 2, extend it to 3 (adding a 0). The combinations are then\n\"fizz\" `(1 0 0)`, \"buzz\" `(0 1 0)`, \"fizzbuzz\" `(1 1 0)`, or the number itself `(0 0 1)`.\n\n`{3↑(0=3 5|⍵)∪1}¨⍳20`\n```\n┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n│0 0 1│0 0 1│1 0 0│0 0 1│0 1 0│1 0 0│0 0 1│0 0 1│1 0 0│0 1 0│0 0 1│1 0 0│0 0 1│0 0 1│1 1 0│0 0 1│0 0 1│1 0 0│0 0 1│0 1 0│\n└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘\n```\nFilter the vector `'Fizz' 'Buzz' ⍵` based on this boolean mask.\n\n`{(3↑(0=3 5|⍵)∪1)/'Fizz' 'Buzz'⍵}¨⍳20`\n```\n┌─┬─┬──────┬─┬──────┬──────┬─┬─┬──────┬──────┬──┬──────┬──┬──┬───────────┬──┬──┬──────┬──┬──────┐\n│1│2│┌────┐│4│┌────┐│┌────┐│7│8│┌────┐│┌────┐│11│┌────┐│13│14│┌────┬────┐│16│17│┌────┐│19│┌────┐│\n│ │ ││Fizz││ ││Buzz│││Fizz││ │ ││Fizz│││Buzz││  ││Fizz││  │  ││Fizz│Buzz││  │  ││Fizz││  ││Buzz││\n│ │ │└────┘│ │└────┘│└────┘│ │ │└────┘│└────┘│  │└────┘│  │  │└────┴────┘│  │  │└────┘│  │└────┘│\n└─┴─┴──────┴─┴──────┴──────┴─┴─┴──────┴──────┴──┴──────┴──┴──┴───────────┴──┴──┴──────┴──┴──────┘\n```\nand finally, enlist (flatten) the vector.\n\n`{∊(3↑(0=3 5|⍵)∪1)/'Fizz' 'Buzz'⍵}¨⍳20`\n```\n┌─┬─┬────┬─┬────┬────┬─┬─┬────┬────┬──┬────┬──┬──┬────────┬──┬──┬────┬──┬────┐\n│1│2│Fizz│4│Buzz│Fizz│7│8│Fizz│Buzz│11│Fizz│13│14│FizzBuzz│16│17│Fizz│19│Buzz│\n└─┴─┴────┴─┴────┴────┴─┴─┴────┴────┴──┴────┴──┴──┴────────┴──┴──┴────┴──┴────┘ \n```\n\nSo concise.\n",
				"date_published": "2023-11-10T21:19:05+09:30",
				"url": "https://jcarroll.xyz/2023/11/10/breaking-down-fizzbuzz.html",
				"tags": ["APL"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/07/03/filtering-vectors.html",
				"title": "Filtering Vectors",
				"content_html": "<p>I saw a YouTube short demonstrating how to use <code>filter()</code> in Python along the lines of</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-python\" data-lang=\"python\">nums<span style=\"color:#f92672\">=</span>range(<span style=\"color:#ae81ff\">1</span>,<span style=\"color:#ae81ff\">30</span>)\n\n<span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">is_prime</span>(num):\n  <span style=\"color:#66d9ef\">for</span> x <span style=\"color:#f92672\">in</span> range(<span style=\"color:#ae81ff\">2</span>,num):\n    <span style=\"color:#66d9ef\">if</span> (num<span style=\"color:#f92672\">%</span>x) <span style=\"color:#f92672\">==</span> <span style=\"color:#ae81ff\">0</span>:\n      <span style=\"color:#66d9ef\">return</span> <span style=\"color:#66d9ef\">False</span>\n    \n  <span style=\"color:#66d9ef\">return</span> <span style=\"color:#66d9ef\">True</span>\n\n  \nprimes<span style=\"color:#f92672\">=</span>list(filter(is_prime, nums))\nprint(primes)\n<span style=\"color:#75715e\"># [1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29]</span>\n</code></pre></div><p>and as always, I like to think about how I&rsquo;d do this in other languages (e.g. R).</p>\n<p>The (deliberately brute-force) <code>is_prime()</code> function translates easily enough</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">is_prime <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(x) {\n  <span style=\"color:#a6e22e\">if </span>(x <span style=\"color:#f92672\">%in%</span> <span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">2</span>) <span style=\"color:#a6e22e\">return</span>(<span style=\"color:#66d9ef\">TRUE</span>)\n  <span style=\"color:#a6e22e\">for </span>(i in <span style=\"color:#ae81ff\">2</span><span style=\"color:#f92672\">:</span>(x<span style=\"color:#ae81ff\">-1</span>)) {\n    <span style=\"color:#a6e22e\">if </span>(x <span style=\"color:#f92672\">%%</span> i <span style=\"color:#f92672\">==</span> <span style=\"color:#ae81ff\">0</span>) <span style=\"color:#a6e22e\">return</span>(<span style=\"color:#66d9ef\">FALSE</span>)\n  }\n  <span style=\"color:#66d9ef\">TRUE</span>\n}\n</code></pre></div><p>and the most common way to run this over a vector of inputs is with something like <code>sapply()</code> returning a vector of logicals that can be used for square-bracket subsetting</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">x <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">30</span>\nprimes <span style=\"color:#f92672\">&lt;-</span> x<span style=\"color:#a6e22e\">[sapply</span>(x, is_prime)]\nprimes\n[1]  <span style=\"color:#ae81ff\">1</span>  <span style=\"color:#ae81ff\">2</span>  <span style=\"color:#ae81ff\">3</span>  <span style=\"color:#ae81ff\">5</span>  <span style=\"color:#ae81ff\">7</span> <span style=\"color:#ae81ff\">11</span> <span style=\"color:#ae81ff\">13</span> <span style=\"color:#ae81ff\">17</span> <span style=\"color:#ae81ff\">19</span> <span style=\"color:#ae81ff\">23</span> <span style=\"color:#ae81ff\">29</span>\n</code></pre></div><p>We can get rid of the need for <code>sapply()</code> by vectorising the function which is done easily with <code>Vectorize()</code></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">is_prime_v <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">Vectorize</span>(is_prime)\nprimes <span style=\"color:#f92672\">&lt;-</span> x<span style=\"color:#a6e22e\">[is_prime_v</span>(x)]\nprimes\n[1]  <span style=\"color:#ae81ff\">1</span>  <span style=\"color:#ae81ff\">2</span>  <span style=\"color:#ae81ff\">3</span>  <span style=\"color:#ae81ff\">5</span>  <span style=\"color:#ae81ff\">7</span> <span style=\"color:#ae81ff\">11</span> <span style=\"color:#ae81ff\">13</span> <span style=\"color:#ae81ff\">17</span> <span style=\"color:#ae81ff\">19</span> <span style=\"color:#ae81ff\">23</span> <span style=\"color:#ae81ff\">29</span>\n</code></pre></div><p>It&rsquo;s usually at this point that I feel bad about having to write <code>x</code> twice and lament that while {dplyr} is great for <code>data.frame</code>s, we don&rsquo;t have something equivalent for filtering of vectors so easily&hellip; except we do (as I usually end up remembering). <code>Filter()</code> is a base function that works on vectors and is the equivalent of what we saw in Python (per the docs, <a href=\"https://hoogle.haskell.org/?hoogle=filter\">&ldquo;Filter corresponds to filter in Haskell&rdquo;</a>)</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">primes <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">Filter</span>(is_prime, x)\nprimes\n[1]  <span style=\"color:#ae81ff\">1</span>  <span style=\"color:#ae81ff\">2</span>  <span style=\"color:#ae81ff\">3</span>  <span style=\"color:#ae81ff\">5</span>  <span style=\"color:#ae81ff\">7</span> <span style=\"color:#ae81ff\">11</span> <span style=\"color:#ae81ff\">13</span> <span style=\"color:#ae81ff\">17</span> <span style=\"color:#ae81ff\">19</span> <span style=\"color:#ae81ff\">23</span> <span style=\"color:#ae81ff\">29</span>\n</code></pre></div><p>In fairness, <code>Filter()</code> expands to much the same as the above</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">Filter\n<span style=\"color:#a6e22e\">function </span>(f, x) \n{\n    f <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">match.fun</span>(f)\n    ind <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">as.logical</span>(<span style=\"color:#a6e22e\">unlist</span>(<span style=\"color:#a6e22e\">lapply</span>(x, f)))\n    x<span style=\"color:#a6e22e\">[which</span>(ind)]\n}\n<span style=\"color:#f92672\">&lt;</span>bytecode<span style=\"color:#f92672\">:</span> <span style=\"color:#ae81ff\">0x12a9db8b0</span><span style=\"color:#f92672\">&gt;</span>\n<span style=\"color:#f92672\">&lt;</span>environment<span style=\"color:#f92672\">:</span> namespace<span style=\"color:#f92672\">:</span>base<span style=\"color:#f92672\">&gt;</span>\n</code></pre></div><p>but it&rsquo;s a nice interface.</p>\n<p>I need to remember to use that (as well as the rest of the gold-mine in the &lsquo;funprog&rsquo; toolbox like <code>Map</code> and <code>Reduce</code>) more often.</p>\n",
				"content_text": "I saw a YouTube short demonstrating how to use `filter()` in Python along the lines of\n\n```python\nnums=range(1,30)\n\ndef is_prime(num):\n  for x in range(2,num):\n    if (num%x) == 0:\n      return False\n    \n  return True\n\n  \nprimes=list(filter(is_prime, nums))\nprint(primes)\n# [1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n```\n\nand as always, I like to think about how I'd do this in other languages (e.g. R). \n\nThe (deliberately brute-force) `is_prime()` function translates easily enough\n\n```r\nis_prime <- function(x) {\n  if (x %in% 1:2) return(TRUE)\n  for (i in 2:(x-1)) {\n    if (x %% i == 0) return(FALSE)\n  }\n  TRUE\n}\n```\n\nand the most common way to run this over a vector of inputs is with something like `sapply()` returning a vector of logicals that can be used for square-bracket subsetting\n\n```r\nx <- 1:30\nprimes <- x[sapply(x, is_prime)]\nprimes\n[1]  1  2  3  5  7 11 13 17 19 23 29\n```\n\nWe can get rid of the need for `sapply()` by vectorising the function which is done easily with `Vectorize()`\n\n```r\nis_prime_v <- Vectorize(is_prime)\nprimes <- x[is_prime_v(x)]\nprimes\n[1]  1  2  3  5  7 11 13 17 19 23 29\n```\n\nIt's usually at this point that I feel bad about having to write `x` twice and lament that while {dplyr} is great for `data.frame`s, we don't have something equivalent for filtering of vectors so easily... except we do (as I usually end up remembering). `Filter()` is a base function that works on vectors and is the equivalent of what we saw in Python (per the docs, [\"Filter corresponds to filter in Haskell\"](https://hoogle.haskell.org/?hoogle=filter))\n\n```r\nprimes <- Filter(is_prime, x)\nprimes\n[1]  1  2  3  5  7 11 13 17 19 23 29\n```\n\nIn fairness, `Filter()` expands to much the same as the above\n\n```r\nFilter\nfunction (f, x) \n{\n    f <- match.fun(f)\n    ind <- as.logical(unlist(lapply(x, f)))\n    x[which(ind)]\n}\n<bytecode: 0x12a9db8b0>\n<environment: namespace:base>\n```\n\nbut it's a nice interface.\n\nI need to remember to use that (as well as the rest of the gold-mine in the 'funprog' toolbox like `Map` and `Reduce`) more often.\n",
				"date_published": "2023-07-03T13:18:59+09:30",
				"url": "https://jcarroll.xyz/2023/07/03/filtering-vectors.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/06/14/around-the-web.html",
				"title": "Around the web 2023W24",
				"content_html": "<p>I think I&rsquo;m happy with my <a href=\"https://feedrabbit.com/\">RSS-to-email</a> setup now but I feel like I&rsquo;m only passively reading things and can do better. I&rsquo;m going to try summarising (on this micro blog) the interesting posts I see (RSS feeds, newsletters, social feeds, general web finds) as a way to share some links, to be able to find them again later, and to reinforce what I&rsquo;m learning. This is informal, largely unstructured, largely unedited, and possibly only interesting to me, but if you don&rsquo;t like it then don&rsquo;t read it. Feel free to comment (here or elsewhere) and start a conversation on any of the topics (or anything).</p>\n<p>I think I&rsquo;ll aim for a weekly post with all the things I read that week. That way I can add to them as I revisit each day. This post already ended up longer than I expected, but I was able to piece it together over many updates via a web text input box rather than re-building my static website.</p>\n<h2 id=\"this-is-valid-python-syntax\">This is valid Python syntax</h2>\n<p><a href=\"https://www.bitecode.dev/p/this-is-valid-python-syntax\">https://www.bitecode.dev/p/this-is-valid-python-syntax</a></p>\n<p>Source: HN toot (now substack subscribed)</p>\n<p>I <em>really</em> like this post for the fact that it teaches a pile of interesting (python) syntax and invites the reader to see if they can unpack all the pieces. Fun little puzzles like <a href=\"https://codegolf.stackexchange.com/\">code-golf</a> are great for this.</p>\n<p>One thing that really struck me was python&rsquo;s ability to use a variable number of arguments in a function</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-python\" data-lang=\"python\"><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">a_lot</span>(<span style=\"color:#f92672\">*</span>of_stuff): <span style=\"color:#75715e\"># accept 0, 1 or more params</span>\n<span style=\"color:#f92672\">...</span>     print(of_stuff)\n<span style=\"color:#f92672\">...</span>\n</code></pre></div><p>which we <em>can</em> do in R with <code>...</code></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\">a_lot <span style=\"color:#f92672\">&lt;-</span> <span style=\"color:#a6e22e\">function</span>(<span style=\"color:#66d9ef\">...</span>) {\n  <span style=\"color:#a6e22e\">print</span>(<span style=\"color:#a6e22e\">list</span>(<span style=\"color:#66d9ef\">...</span>))\n}\n<span style=\"color:#a6e22e\">a_lot</span>(a <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">1</span>, b <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">2</span>)\n<span style=\"color:#f92672\">$</span>a\n[1] <span style=\"color:#ae81ff\">1</span>\n\n<span style=\"color:#f92672\">$</span>b\n[1] <span style=\"color:#ae81ff\">2</span>\n</code></pre></div><p>but python can also unpack (&ldquo;splat&rdquo;?) the arguments</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-python\" data-lang=\"python\"><span style=\"color:#f92672\">&gt;&gt;&gt;</span> <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">square_area</span>(x1, y1, x2, y2):\n<span style=\"color:#f92672\">...</span>     side_length <span style=\"color:#f92672\">=</span> abs(x2 <span style=\"color:#f92672\">-</span> x1)\n<span style=\"color:#f92672\">...</span>     area <span style=\"color:#f92672\">=</span> side_length <span style=\"color:#f92672\">**</span> <span style=\"color:#ae81ff\">2</span>\n<span style=\"color:#f92672\">...</span>     <span style=\"color:#66d9ef\">return</span> area\n<span style=\"color:#f92672\">...</span> coords <span style=\"color:#f92672\">=</span> [<span style=\"color:#ae81ff\">0</span>, <span style=\"color:#ae81ff\">0</span>, <span style=\"color:#ae81ff\">3</span>, <span style=\"color:#ae81ff\">3</span>]\n<span style=\"color:#f92672\">&gt;&gt;&gt;</span> square_area(coords[<span style=\"color:#ae81ff\">0</span>], coords[<span style=\"color:#ae81ff\">1</span>], coords[<span style=\"color:#ae81ff\">2</span>], coords[<span style=\"color:#ae81ff\">3</span>])\n<span style=\"color:#ae81ff\">9</span>\n<span style=\"color:#f92672\">&gt;&gt;&gt;</span> square_area(<span style=\"color:#f92672\">*</span>coords)\n<span style=\"color:#ae81ff\">9</span>\n</code></pre></div><p>which R can&rsquo;t do (well, <code>do.call(what, args)</code> does that, but not as cleanly).</p>\n<p>I bet some interesting results could be made with R syntax - there&rsquo;s an idea for a post.</p>\n<h2 id=\"get-the-most-out-of-python-dicts\">Get the most out of Python dicts</h2>\n<p><a href=\"https://www.bitecode.dev/p/get-the-most-out-of-python-dict\">https://www.bitecode.dev/p/get-the-most-out-of-python-dict</a></p>\n<p>Source: Substack subscription</p>\n<p><code>dict</code> is a structure in python that I suppose the R equivalent of is a named vector or a <code>list</code>, but there&rsquo;s some hashset-like quality to them (the keys are unique) so that&rsquo;s not exact at all. There&rsquo;s a lot of semantics specific to <code>dict</code> that I haven&rsquo;t appreciated yet, so this was a nice little tour.</p>\n<h2 id=\"how-to-write-conditional-statements-in-r-four-methods\">How to Write Conditional Statements in R: Four Methods</h2>\n<p><a href=\"https://towardsdatascience.com/how-to-write-conditional-statements-in-r-four-methods-f9bedbae0683\">https://towardsdatascience.com/how-to-write-conditional-statements-in-r-four-methods-f9bedbae0683</a></p>\n<p>Source: Mastodon boost</p>\n<p>This is a post for R beginners, but I&rsquo;m wary that the <code>ifelse()</code> vector example could be misleading since it will error if the logic is applied back to the <code>if()</code> scenario (length &gt; 1). The post isn&rsquo;t wrong per-se, but it would be worth pointing out. I&rsquo;ve also hit bugs in the past because the <em>shape</em> of the condition needs to match the shape of the result</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-r\" data-lang=\"r\"><span style=\"color:#a6e22e\">ifelse</span>(<span style=\"color:#66d9ef\">TRUE</span>, <span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">:</span><span style=\"color:#ae81ff\">4</span>, <span style=\"color:#ae81ff\">2</span>)\n[1] <span style=\"color:#ae81ff\">1</span>\n</code></pre></div><p>The author also says</p>\n<blockquote>\n<p>This makes ifelse a clean way of evaluating lots of simple conditions without needing slow, messy loops.</p>\n</blockquote>\n<p>and it should be noted that for-loops aren&rsquo;t slow; your code is slow <a href=\"https://youtu.be/TdbweYvwnss\">https://youtu.be/TdbweYvwnss</a></p>\n<h2 id=\"emulated-build-and-test-of-bioconductor-packages-for-linux-arm64\">Emulated build and test of Bioconductor packages for Linux ARM64</h2>\n<p><a href=\"https://bioconductor.github.io/biocblog/posts/2023-06-09-debug-linux-arm64-on-docker/\">https://bioconductor.github.io/biocblog/posts/2023-06-09-debug-linux-arm64-on-docker/</a></p>\n<p>Source: Mastodon follow</p>\n<p>along with rocker/rstudio arm64 builds <a href=\"https://hub.docker.com/r/rocker/rstudio/tags\">https://hub.docker.com/r/rocker/rstudio/tags</a></p>\n<p>I&rsquo;ve been waiting for this for a while! I got an M1 mac when I started my current job over a year ago and while I can deploy docker images in a cloud solution, I couldn&rsquo;t test them locally. Time to build all the things!</p>\n<h2 id=\"feeling-rusty-counting-characters\">Feeling rusty: counting characters</h2>\n<p><a href=\"https://josiahparry.com/posts/2023-04-13-counting-chars/\">https://josiahparry.com/posts/2023-04-13-counting-chars/</a></p>\n<p>Source: the Julia post below</p>\n<p>Just when you think R is sort of okay at performance - not terrible, but by no means the fastest - Rust goes and shows you that you&rsquo;re wasting your time even trying to keep up and has already gone to lunch after completing all its tasks. This was a great intro to &lsquo;Rust as an RCpp replacement&rsquo; via {rextendr} (a fuller intro from the same author is <a href=\"https://youtu.be/tRm-Qq2_Ap0\">https://youtu.be/tRm-Qq2_Ap0</a>).</p>\n<h2 id=\"string-matching-in-julia\">String Matching in Julia</h2>\n<p><a href=\"https://www.ericekholm.com/posts/string-match-jl/\">https://www.ericekholm.com/posts/string-match-jl/</a></p>\n<p>Source: RT</p>\n<p>This reminds me what I like so much about Julia syntax. Those function definitions are just <em>so</em> clean!</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-julia\" data-lang=\"julia\"><span style=\"color:#66d9ef\">function</span> compare_strings(x<span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">String</span>, y<span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">String</span>)\n    s <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">0</span>\n    <span style=\"color:#66d9ef\">for</span> i <span style=\"color:#f92672\">∈</span> eachindex(x)\n        x[i] <span style=\"color:#f92672\">!=</span> y[i] <span style=\"color:#f92672\">?</span> <span style=\"color:#66d9ef\">break</span> <span style=\"color:#f92672\">:</span> s <span style=\"color:#f92672\">+=</span> <span style=\"color:#ae81ff\">1</span>\n    <span style=\"color:#66d9ef\">end</span>\n    <span style=\"color:#66d9ef\">return</span> s\n<span style=\"color:#66d9ef\">end</span>\n</code></pre></div><p>And being able to add more definitions just by <em>writing them out</em> is so compact (a full post on that coming soon).</p>\n<p>As for performance, that looks hard to beat. I plan to run the R, Rust, and Julia functions on one machine and see how they compare, but I&rsquo;m not going to let it get in the way of writing this.</p>\n<h2 id=\"whats-so-special-about-arrays\">What’s so special about arrays?</h2>\n<p><a href=\"https://josiahparry.com/posts/2023-06-11-matrix-bug.html\">https://josiahparry.com/posts/2023-06-11-matrix-bug.html</a></p>\n<p>Source: r-contributors slack</p>\n<p>I commented in the slack thread but my take on this is that <code>matrix</code> is pretty much a vector with a <code>dim</code> attribute, so I&rsquo;m not so surprised that <code>unclass()</code> doesn&rsquo;t really &ldquo;un-class&rdquo; it. <code>?class</code> seems to refer to this as an &ldquo;implicit class&rdquo;.</p>\n<h2 id=\"effective-spaced-repetition\">Effective Spaced Repetition</h2>\n<p><a href=\"https://borretti.me/article/effective-spaced-repetition\">https://borretti.me/article/effective-spaced-repetition</a></p>\n<p>Source: originally via <a href=\"https://borretti.me/article/depth-first-procrastination\">https://borretti.me/article/depth-first-procrastination</a> via RSS Feed</p>\n<p>This is a longer read, but I&rsquo;m on board with the principle.</p>\n<h2 id=\"research-sum-types\">(Research) Sum Types</h2>\n<p>Trigger: <a href=\"https://corecursive.com/048-jared-forsyth-the-reason-for-types/#sum-types-in-reasonml\">Corecursive podcast</a></p>\n<p>I&rsquo;m digging through some older episodes for a once-a-week rare commute somewhere and this term caught my ear so I decided to look into it a bit and it turns out to be super interesting! I haven&rsquo;t dug through all of these, but this serves as a sort of bookmark for me to come back to.</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Algebraic_data_type\">https://en.wikipedia.org/wiki/Algebraic_data_type</a></li>\n<li><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/10jewgp/could_you_explain_why_sum_types_are_so_good/\">https://www.reddit.com/r/ProgrammingLanguages/comments/10jewgp/could_you_explain_why_sum_types_are_so_good/</a></li>\n<li><a href=\"https://shreevatsa.wordpress.com/2015/01/31/boolean-blindness/\">https://shreevatsa.wordpress.com/2015/01/31/boolean-blindness/</a></li>\n<li><a href=\"https://serokell.io/blog/algebraic-data-types-in-haskell\">https://serokell.io/blog/algebraic-data-types-in-haskell</a></li>\n<li><a href=\"https://jrsinclair.com/articles/2019/algebraic-data-types-what-i-wish-someone-had-explained-about-functional-programming/\">https://jrsinclair.com/articles/2019/algebraic-data-types-what-i-wish-someone-had-explained-about-functional-programming/</a></li>\n<li><a href=\"https://docs.rs/sum_type/latest/sum_type/\">https://docs.rs/sum_type/latest/sum_type/</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=LafOj_HrxRQ\">https://www.youtube.com/watch?v=LafOj_HrxRQ</a> aaaand now I&rsquo;m glad I was keeping track of all of these; this is a very timely video on Rust enums that covers some of this topic. Found via <a href=\"https://this-week-in-rust.org/blog/2023/06/14/this-week-in-rust-499/\">This Week in Rust</a>.</li>\n</ul>\n<h2 id=\"is-that-a-compiler-bug\">Is That a Compiler Bug?</h2>\n<p><a href=\"https://blog.regehr.org/archives/26\">https://blog.regehr.org/archives/26</a></p>\n<p>An older post (circa 2010) that I don&rsquo;t recall how I came across. Interesting, though not something I&rsquo;m likely to encounter myself anytime soon.</p>\n<h2 id=\"writing-r-in-vscode-working-with-multiple-r-sessions\">Writing R in VSCode: Working with multiple R sessions</h2>\n<p><a href=\"https://renkun.me/2020/04/14/writing-r-in-vscode-working-with-multiple-r-sessions/\">https://renkun.me/2020/04/14/writing-r-in-vscode-working-with-multiple-r-sessions/</a></p>\n<p>Source: rOpenSci slack</p>\n<p>I&rsquo;ve been starting to use <code>tmux</code> in terminals lately and am really enjoying it, so when a question came up about connecting a VSCode session to it (which could be remotely connected) my interest was piqued. The linked post might be a solution to that, though I&rsquo;m yet to try it myself.</p>\n<h2 id=\"list-one-task-do-it-cross-it-out\">List one task, do it, cross it out</h2>\n<p><a href=\"https://www.oliverburkeman.com/onething\">https://www.oliverburkeman.com/onething</a></p>\n<p>Source: can&rsquo;t recall&hellip; HN? This echos a lot of what the &lsquo;minimalist&rsquo; folks are saying about organising and cleaning; trying to decide which of the dozen semi-important things you need to work on often leads to doing none of them, but by choosing just one you know exactly what you need to do.</p>\n<h2 id=\"finish-your-projects\">Finish your projects</h2>\n<p><a href=\"https://github.com/readme/guides/finish-your-projects\">https://github.com/readme/guides/finish-your-projects</a></p>\n<p>Source: TLDR newsletter</p>\n<p>Similar to the above article in terms of message; pick something and do it.</p>\n<h2 id=\"iterating-on-testing-in-rust\">Iterating on Testing in Rust</h2>\n<p><a href=\"https://epage.github.io/blog/2023/06/iterating-on-test/\">https://epage.github.io/blog/2023/06/iterating-on-test/</a></p>\n<p>Source: HN</p>\n<p>Good overview of how tests work in Rust.</p>\n<h2 id=\"row-relational-operations-with-slice\">Row relational operations with slice()</h2>\n<p><a href=\"https://yjunechoe.github.io/posts/2023-06-11-row-relational-operations/\">https://yjunechoe.github.io/posts/2023-06-11-row-relational-operations/</a></p>\n<p>Source: RWeekly</p>\n<p>Winners: <code>slice()</code> as a <code>filter()</code> being able use row indices with <code>cond + 1</code>; <code>outer()</code> to get non-recycling addition. A very in-depth article that goes through lots of uses of <code>slice()</code>.</p>\n<h2 id=\"on-updating-a-chat-assistant-app-for-the-rstudio-ide\">On updating a chat assistant app for the RStudio IDE</h2>\n<ul>\n<li><a href=\"https://samuelenrique.com/posts/2023-06-02-updating-gptstudio/\">https://samuelenrique.com/posts/2023-06-02-updating-gptstudio/</a></li>\n</ul>\n<p>Source: RWeekly</p>\n<p>The auto-updating streaming output is very cool! I wonder if there&rsquo;s an open streaming resource I could use to try this?</p>\n<p>This also demonstrates adding &lsquo;copy&rsquo; button to code blocks which I don&rsquo;t have on my blog - perhaps I could? They used</p>\n<pre tabindex=\"0\"><code class=\"language- \" data-lang=\" \">$('pre').each(function() {\n  const $codeChunk = $(this);\n  const $copyButton = $('&lt;button&gt;').text('Copy');\n  $codeChunk.prepend($copyButton);\n\n  $copyButton.on('click', function() {\n    const codeText = $codeChunk.text();\n    navigator.clipboard.writeText(codeText);\n  });\n});\n</code></pre><h2 id=\"a-terminal-case-of-linux\">A terminal case of Linux</h2>\n<p><a href=\"https://fasterthanli.me/articles/a-terminal-case-of-linux\">https://fasterthanli.me/articles/a-terminal-case-of-linux</a></p>\n<p>Source: ? (Mastodon?)</p>\n<p>I don&rsquo;t recall where or why I found this 2021 article but as usual, Amos goes reeeeally deep into some Rust code (and libc / Linux in this case). I got hooked on these posts when (re-)doing AoC 2022 and for <a href=\"https://fasterthanli.me/series/advent-of-code-2022/part-1\">day 1</a> Amos went mile-deep (25mins read) on Rust in general when the problem to be solved was &ldquo;straightforward&rdquo;.</p>\n<h2 id=\"friction-baby\">Friction, Baby</h2>\n<p><a href=\"https://tedium.co/2023/06/10/productivity-friction-theory/\">https://tedium.co/2023/06/10/productivity-friction-theory/</a></p>\n<p>Source: HN</p>\n<p>This makes a lot of sense to me and made me consider a lot of the patterns (anti-, dark-, or otherwise) I see in my media consumption. The goal of this micro blog is to have as little friction as possible when publishing some markdown text. Sometimes, some interactions could use a bit more friction.</p>\n",
				"content_text": "I think I'm happy with my [RSS-to-email](https://feedrabbit.com/) setup now but I feel like I'm only passively reading things and can do better. I'm going to try summarising (on this micro blog) the interesting posts I see (RSS feeds, newsletters, social feeds, general web finds) as a way to share some links, to be able to find them again later, and to reinforce what I'm learning. This is informal, largely unstructured, largely unedited, and possibly only interesting to me, but if you don't like it then don't read it. Feel free to comment (here or elsewhere) and start a conversation on any of the topics (or anything).\n\nI think I'll aim for a weekly post with all the things I read that week. That way I can add to them as I revisit each day. This post already ended up longer than I expected, but I was able to piece it together over many updates via a web text input box rather than re-building my static website.\n\n## This is valid Python syntax\n\n[https://www.bitecode.dev/p/this-is-valid-python-syntax](https://www.bitecode.dev/p/this-is-valid-python-syntax)\n\nSource: HN toot (now substack subscribed)\n\nI _really_ like this post for the fact that it teaches a pile of interesting (python) syntax and invites the reader to see if they can unpack all the pieces. Fun little puzzles like [code-golf](https://codegolf.stackexchange.com/) are great for this.\n\nOne thing that really struck me was python's ability to use a variable number of arguments in a function\n\n```python\ndef a_lot(*of_stuff): # accept 0, 1 or more params\n...     print(of_stuff)\n...\n```\n\nwhich we _can_ do in R with `...`\n\n```r\na_lot <- function(...) {\n  print(list(...))\n}\na_lot(a = 1, b = 2)\n$a\n[1] 1\n\n$b\n[1] 2\n```\n\nbut python can also unpack (\"splat\"?) the arguments\n\n```python\n>>> def square_area(x1, y1, x2, y2):\n...     side_length = abs(x2 - x1)\n...     area = side_length ** 2\n...     return area\n... coords = [0, 0, 3, 3]\n>>> square_area(coords[0], coords[1], coords[2], coords[3])\n9\n>>> square_area(*coords)\n9\n```\n\nwhich R can't do (well, `do.call(what, args)` does that, but not as cleanly).\n\nI bet some interesting results could be made with R syntax - there's an idea for a post.\n\n## Get the most out of Python dicts\n\n[https://www.bitecode.dev/p/get-the-most-out-of-python-dict](https://www.bitecode.dev/p/get-the-most-out-of-python-dict)\n\nSource: Substack subscription\n\n`dict` is a structure in python that I suppose the R equivalent of is a named vector or a `list`, but there's some hashset-like quality to them (the keys are unique) so that's not exact at all. There's a lot of semantics specific to `dict` that I haven't appreciated yet, so this was a nice little tour.\n\n## How to Write Conditional Statements in R: Four Methods\n\n[https://towardsdatascience.com/how-to-write-conditional-statements-in-r-four-methods-f9bedbae0683](https://towardsdatascience.com/how-to-write-conditional-statements-in-r-four-methods-f9bedbae0683)\n\nSource: Mastodon boost\n\nThis is a post for R beginners, but I'm wary that the `ifelse()` vector example could be misleading since it will error if the logic is applied back to the `if()` scenario (length > 1). The post isn't wrong per-se, but it would be worth pointing out. I've also hit bugs in the past because the _shape_ of the condition needs to match the shape of the result\n\n```r\nifelse(TRUE, 1:4, 2)\n[1] 1\n```\n\nThe author also says\n\n> This makes ifelse a clean way of evaluating lots of simple conditions without needing slow, messy loops.\n\nand it should be noted that for-loops aren't slow; your code is slow [https://youtu.be/TdbweYvwnss](https://youtu.be/TdbweYvwnss)\n\n## Emulated build and test of Bioconductor packages for Linux ARM64\n\n[https://bioconductor.github.io/biocblog/posts/2023-06-09-debug-linux-arm64-on-docker/](https://bioconductor.github.io/biocblog/posts/2023-06-09-debug-linux-arm64-on-docker/)\n\nSource: Mastodon follow\n\nalong with rocker/rstudio arm64 builds [https://hub.docker.com/r/rocker/rstudio/tags](https://hub.docker.com/r/rocker/rstudio/tags)\n\nI've been waiting for this for a while! I got an M1 mac when I started my current job over a year ago and while I can deploy docker images in a cloud solution, I couldn't test them locally. Time to build all the things!\n\n## Feeling rusty: counting characters\n\n[https://josiahparry.com/posts/2023-04-13-counting-chars/](https://josiahparry.com/posts/2023-04-13-counting-chars/)\n\nSource: the Julia post below\n\nJust when you think R is sort of okay at performance - not terrible, but by no means the fastest - Rust goes and shows you that you're wasting your time even trying to keep up and has already gone to lunch after completing all its tasks. This was a great intro to 'Rust as an RCpp replacement' via {rextendr} (a fuller intro from the same author is [https://youtu.be/tRm-Qq2_Ap0](https://youtu.be/tRm-Qq2_Ap0)).\n\n## String Matching in Julia\n\n[https://www.ericekholm.com/posts/string-match-jl/](https://www.ericekholm.com/posts/string-match-jl/)\n\nSource: RT\n\nThis reminds me what I like so much about Julia syntax. Those function definitions are just _so_ clean!\n\n```julia\nfunction compare_strings(x::String, y::String)\n    s = 0\n    for i ∈ eachindex(x)\n        x[i] != y[i] ? break : s += 1\n    end\n    return s\nend\n```\n\nAnd being able to add more definitions just by _writing them out_ is so compact (a full post on that coming soon).\n\nAs for performance, that looks hard to beat. I plan to run the R, Rust, and Julia functions on one machine and see how they compare, but I'm not going to let it get in the way of writing this.\n\n## What’s so special about arrays?\n\n[https://josiahparry.com/posts/2023-06-11-matrix-bug.html](https://josiahparry.com/posts/2023-06-11-matrix-bug.html) \n\nSource: r-contributors slack\n\nI commented in the slack thread but my take on this is that `matrix` is pretty much a vector with a `dim` attribute, so I'm not so surprised that `unclass()` doesn't really \"un-class\" it. `?class` seems to refer to this as an \"implicit class\".\n\n## Effective Spaced Repetition\n\n[https://borretti.me/article/effective-spaced-repetition](https://borretti.me/article/effective-spaced-repetition)\n\nSource: originally via [https://borretti.me/article/depth-first-procrastination](https://borretti.me/article/depth-first-procrastination) via RSS Feed\n\nThis is a longer read, but I'm on board with the principle.\n\n## (Research) Sum Types\n\nTrigger: [Corecursive podcast](https://corecursive.com/048-jared-forsyth-the-reason-for-types/#sum-types-in-reasonml)\n\nI'm digging through some older episodes for a once-a-week rare commute somewhere and this term caught my ear so I decided to look into it a bit and it turns out to be super interesting! I haven't dug through all of these, but this serves as a sort of bookmark for me to come back to.\n\n- [https://en.wikipedia.org/wiki/Algebraic_data_type](https://en.wikipedia.org/wiki/Algebraic_data_type)\n- [https://www.reddit.com/r/ProgrammingLanguages/comments/10jewgp/could_you_explain_why_sum_types_are_so_good/](https://www.reddit.com/r/ProgrammingLanguages/comments/10jewgp/could_you_explain_why_sum_types_are_so_good/)\n- [https://shreevatsa.wordpress.com/2015/01/31/boolean-blindness/](https://shreevatsa.wordpress.com/2015/01/31/boolean-blindness/)\n- [https://serokell.io/blog/algebraic-data-types-in-haskell](https://serokell.io/blog/algebraic-data-types-in-haskell)\n- [https://jrsinclair.com/articles/2019/algebraic-data-types-what-i-wish-someone-had-explained-about-functional-programming/](https://jrsinclair.com/articles/2019/algebraic-data-types-what-i-wish-someone-had-explained-about-functional-programming/)\n- [https://docs.rs/sum_type/latest/sum_type/](https://docs.rs/sum_type/latest/sum_type/)\n- [https://www.youtube.com/watch?v=LafOj_HrxRQ](https://www.youtube.com/watch?v=LafOj_HrxRQ) aaaand now I'm glad I was keeping track of all of these; this is a very timely video on Rust enums that covers some of this topic. Found via [This Week in Rust](https://this-week-in-rust.org/blog/2023/06/14/this-week-in-rust-499/).\n\n## Is That a Compiler Bug?\n\n[https://blog.regehr.org/archives/26](https://blog.regehr.org/archives/26)\n\nAn older post (circa 2010) that I don't recall how I came across. Interesting, though not something I'm likely to encounter myself anytime soon.\n\n## Writing R in VSCode: Working with multiple R sessions\n\n[https://renkun.me/2020/04/14/writing-r-in-vscode-working-with-multiple-r-sessions/](https://renkun.me/2020/04/14/writing-r-in-vscode-working-with-multiple-r-sessions/)\n\nSource: rOpenSci slack\n\nI've been starting to use `tmux` in terminals lately and am really enjoying it, so when a question came up about connecting a VSCode session to it (which could be remotely connected) my interest was piqued. The linked post might be a solution to that, though I'm yet to try it myself. \n\n## List one task, do it, cross it out\n\n[https://www.oliverburkeman.com/onething](https://www.oliverburkeman.com/onething)\n\nSource: can't recall... HN? This echos a lot of what the 'minimalist' folks are saying about organising and cleaning; trying to decide which of the dozen semi-important things you need to work on often leads to doing none of them, but by choosing just one you know exactly what you need to do.\n\n## Finish your projects\n\n[https://github.com/readme/guides/finish-your-projects](https://github.com/readme/guides/finish-your-projects)\n\nSource: TLDR newsletter\n\nSimilar to the above article in terms of message; pick something and do it.\n\n## Iterating on Testing in Rust\n\n[https://epage.github.io/blog/2023/06/iterating-on-test/](https://epage.github.io/blog/2023/06/iterating-on-test/)\n\nSource: HN\n\nGood overview of how tests work in Rust.\n\n## Row relational operations with slice()\n\n[https://yjunechoe.github.io/posts/2023-06-11-row-relational-operations/](https://yjunechoe.github.io/posts/2023-06-11-row-relational-operations/)\n\nSource: RWeekly\n\nWinners: `slice()` as a `filter()` being able use row indices with `cond + 1`; `outer()` to get non-recycling addition. A very in-depth article that goes through lots of uses of `slice()`.\n\n## On updating a chat assistant app for the RStudio IDE\n\n- [https://samuelenrique.com/posts/2023-06-02-updating-gptstudio/](https://samuelenrique.com/posts/2023-06-02-updating-gptstudio/)\n\nSource: RWeekly\n\nThe auto-updating streaming output is very cool! I wonder if there's an open streaming resource I could use to try this?\n\nThis also demonstrates adding 'copy' button to code blocks which I don't have on my blog - perhaps I could? They used\n\n``` \n$('pre').each(function() {\n  const $codeChunk = $(this);\n  const $copyButton = $('<button>').text('Copy');\n  $codeChunk.prepend($copyButton);\n\n  $copyButton.on('click', function() {\n    const codeText = $codeChunk.text();\n    navigator.clipboard.writeText(codeText);\n  });\n});\n```\n\n## A terminal case of Linux\n\n[https://fasterthanli.me/articles/a-terminal-case-of-linux](https://fasterthanli.me/articles/a-terminal-case-of-linux)\n\nSource: ? (Mastodon?)\n\nI don't recall where or why I found this 2021 article but as usual, Amos goes reeeeally deep into some Rust code (and libc / Linux in this case). I got hooked on these posts when (re-)doing AoC 2022 and for [day 1](https://fasterthanli.me/series/advent-of-code-2022/part-1) Amos went mile-deep (25mins read) on Rust in general when the problem to be solved was \"straightforward\".\n\n## Friction, Baby\n\n[https://tedium.co/2023/06/10/productivity-friction-theory/](https://tedium.co/2023/06/10/productivity-friction-theory/)\n\nSource: HN\n\nThis makes a lot of sense to me and made me consider a lot of the patterns (anti-, dark-, or otherwise) I see in my media consumption. The goal of this micro blog is to have as little friction as possible when publishing some markdown text. Sometimes, some interactions could use a bit more friction.\n",
				"date_published": "2023-06-16T13:20:20+09:30",
				"url": "https://jcarroll.xyz/2023/06/14/around-the-web.html",
				"tags": ["Around the web"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/05/12/fizzbuzz-without-an.html",
				"title": "FizzBuzz without an if",
				"content_html": "<p>Apparently using an <code>if</code> to solve <a href=\"https://en.wikipedia.org/wiki/Fizz_buzz\">FizzBuzz</a> is a sign that you&rsquo;re <a href=\"https://youtu.be/GfNBZ7awHGo?t=545\">not a great programmer</a>&hellip; Well, that&rsquo;s how I would have done it :-(</p>\n<p>I wanted to see what the <code>map</code> alternative looked like, and I found <a href=\"https://builtin.com/software-engineering-perspectives/fizzbuzz-python#:~:text=Programming%20with%20Mosh-,4.%20LAMBDA,-Another%20method%20we\">this python example</a></p>\n<pre tabindex=\"0\"><code>print(*map(lambda i: 'Fizz'*(not i%3)+'Buzz'*(not i%5) or i, range(1,101)),sep='\\n')\r\n</code></pre><p>What&rsquo;s interesting to me is that there&rsquo;s an <code>or</code> in there - if neither of the modulo operations match, then the resulting string is empty, and that can be used in an <code>or</code> (failing the left-hand side). Whoa! That doesn&rsquo;t work in R because characters can&rsquo;t be used in logical operations</p>\n<pre tabindex=\"0\"><code>&quot;&quot; | 2\r\nError in &quot;&quot; | 2 : \r\n  operations are possible only for numeric, logical or complex types\r\n</code></pre><p>The rest I can more or less reproduce in R, using <code>strrep</code> in place of the infix <code>*</code> used for &lsquo;repeat string&rsquo;</p>\n<pre tabindex=\"0\"><code>sapply(1:20, \r\n       \\(x) paste0(\r\n         strrep(&quot;Fizz&quot;, x %% 3 == 0), \r\n         strrep(&quot;Buzz&quot;, x %% 5 == 0), \r\n         strrep(x, x %% 3 != 0 &amp;&amp; x %% 5 != 0)\r\n       )\r\n)\r\n [1] &quot;1&quot;        &quot;2&quot;        &quot;Fizz&quot;     &quot;4&quot;        &quot;Buzz&quot;     &quot;Fizz&quot;    \r\n [7] &quot;7&quot;        &quot;8&quot;        &quot;Fizz&quot;     &quot;Buzz&quot;     &quot;11&quot;       &quot;Fizz&quot;    \r\n[13] &quot;13&quot;       &quot;14&quot;       &quot;FizzBuzz&quot; &quot;16&quot;       &quot;17&quot;       &quot;Fizz&quot;    \r\n[19] &quot;19&quot;       &quot;Buzz&quot;    \r\n</code></pre><p>This also requires the additional comparison for when the number is a divisor of neither number, so kudos to python for working nicer in that case.</p>\n<p>Is there an even better way to solve this without an <code>if</code>?</p>\n",
				"content_text": "Apparently using an `if` to solve [FizzBuzz](https://en.wikipedia.org/wiki/Fizz_buzz) is a sign that you're [not a great programmer](https://youtu.be/GfNBZ7awHGo?t=545)... Well, that's how I would have done it :-(\r\n\r\nI wanted to see what the `map` alternative looked like, and I found [this python example](https://builtin.com/software-engineering-perspectives/fizzbuzz-python#:~:text=Programming%20with%20Mosh-,4.%20LAMBDA,-Another%20method%20we)\r\n\r\n```\r\nprint(*map(lambda i: 'Fizz'*(not i%3)+'Buzz'*(not i%5) or i, range(1,101)),sep='\\n')\r\n```\r\n\r\nWhat's interesting to me is that there's an `or` in there - if neither of the modulo operations match, then the resulting string is empty, and that can be used in an `or` (failing the left-hand side). Whoa! That doesn't work in R because characters can't be used in logical operations\r\n\r\n```\r\n\"\" | 2\r\nError in \"\" | 2 : \r\n  operations are possible only for numeric, logical or complex types\r\n```\r\n\r\nThe rest I can more or less reproduce in R, using `strrep` in place of the infix `*` used for 'repeat string'\r\n\r\n```\r\nsapply(1:20, \r\n       \\(x) paste0(\r\n         strrep(\"Fizz\", x %% 3 == 0), \r\n         strrep(\"Buzz\", x %% 5 == 0), \r\n         strrep(x, x %% 3 != 0 && x %% 5 != 0)\r\n       )\r\n)\r\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \r\n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \r\n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \r\n[19] \"19\"       \"Buzz\"    \r\n```\r\n\r\nThis also requires the additional comparison for when the number is a divisor of neither number, so kudos to python for working nicer in that case.\r\n\r\nIs there an even better way to solve this without an `if`?\n",
				"date_published": "2023-05-12T16:11:17+09:30",
				"url": "https://jcarroll.xyz/2023/05/12/fizzbuzz-without-an.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/04/12/finished-reading-bullshit.html",
				"title": "Finished reading: Bullshit Jobs by David Graeber 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780241267363\">Bullshit Jobs</a> by David Graeber 📚</p>\n<p>I&rsquo;ll preface this with my sympathy for my colleagues and friends currently struggling to find work after corporate layoffs. This book makes their plight even worse as it details people who <em>do</em> have jobs, but don&rsquo;t believe they should as they don&rsquo;t feel they offer any benefit to society. This is the (much) expanded version of a <a href=\"https://www.atlasofplaces.com/essays/on-the-phenomenon-of-bullshit-jobs/\">short essay</a> backed up with references and sidenotes.</p>\n<p>I&rsquo;ve puzzled for a while at the notion of aiming for &ldquo;100% employment&rdquo; both from the perspective that &ldquo;productivity should be improving with technology, so do we still <em>need</em> those jobs?&rdquo; and the more social &ldquo;is the goal really for everyone to be working more?&rdquo; This book spells out those arguments in great detail and points out that, if all was going well, we could all be working 15 hour weeks with the rest of the time left to do whatever we want.</p>\n<p>I don&rsquo;t think I&rsquo;m spoiling much to say that the conclusion of the book leads towards a universal basic income (not without its own issues and complexities) being a way to even the field - people not needing to take on work they really don&rsquo;t want to do or feel they can&rsquo;t leave because the only alternative is poverty and homelessness. The notion that we could take what we spend torturing the poor (by making them apply for housing, food-stamps, assistance, &hellip;, including paying the people who implement those programs, those who manage those people, and the infrastructure to support all of them) and just give everyone enough money to survive is a radical shift, and I suspect too hard to sell to governments who still take pride in having that many people &ldquo;working&rdquo; on helping others (despite most not actually getting any help).</p>\n<p>I don&rsquo;t feel that my own work is as futile as some of the testimonies, but I do see a huge inefficiency of every company hiring someone (or a team of people) to do the exact same thing for <em>their</em> data. The sheer amount of duplication that happens across companies is surely something that wouldn&rsquo;t exist if money weren&rsquo;t such a driver of why companies do what they do.</p>\n<p>The brief mention of open-source work surprised me - <a href=\"https://micro.blog/books/9780578675862\">Working in Public</a> details how poorly this is supported, but this book highlights that when all of the &ldquo;fun&rdquo; work of building tools is open source, no one builds the &ldquo;core&rdquo; pieces, so a lot of companies end up spending a lot of effort &ldquo;duct taping&rdquo; those free, open-source solutions together.</p>\n<p>This book also dives into the political landscape (mostly in the US but also elsewhere) and articulates why it&rsquo;s all washed together now (and doomed, no matter which side you&rsquo;re on).</p>\n<p>As for what I didn&rsquo;t like about this book - the footnotes take up so much space (sometimes an entire page themselves) that they&rsquo;ve been moved to the end of the book, but they are so frequent (sometimes one per sentence) that I read the entire book flipping back and forth between them. I had to use two bookmarks. This was extremely frustrating, particularly when the footnotes were of little importance. Many had URLs which I haven&rsquo;t checked, but are probably stale. Some were interesting, but this made the book much harder to read.</p>\n<p>Definitely an interesting read, but if you hated society before reading it (I did) then you&rsquo;re not going to be happier afterwards (I&rsquo;m not).</p>\n",
				"content_text": "Finished reading: [Bullshit Jobs](https://micro.blog/books/9780241267363) by David Graeber 📚\r\n\r\nI'll preface this with my sympathy for my colleagues and friends currently struggling to find work after corporate layoffs. This book makes their plight even worse as it details people who _do_ have jobs, but don't believe they should as they don't feel they offer any benefit to society. This is the (much) expanded version of a [short essay](https://www.atlasofplaces.com/essays/on-the-phenomenon-of-bullshit-jobs/) backed up with references and sidenotes.\r\n\r\nI've puzzled for a while at the notion of aiming for \"100% employment\" both from the perspective that \"productivity should be improving with technology, so do we still _need_ those jobs?\" and the more social \"is the goal really for everyone to be working more?\" This book spells out those arguments in great detail and points out that, if all was going well, we could all be working 15 hour weeks with the rest of the time left to do whatever we want.\r\n\r\nI don't think I'm spoiling much to say that the conclusion of the book leads towards a universal basic income (not without its own issues and complexities) being a way to even the field - people not needing to take on work they really don't want to do or feel they can't leave because the only alternative is poverty and homelessness. The notion that we could take what we spend torturing the poor (by making them apply for housing, food-stamps, assistance, ..., including paying the people who implement those programs, those who manage those people, and the infrastructure to support all of them) and just give everyone enough money to survive is a radical shift, and I suspect too hard to sell to governments who still take pride in having that many people \"working\" on helping others (despite most not actually getting any help).\r\n\r\nI don't feel that my own work is as futile as some of the testimonies, but I do see a huge inefficiency of every company hiring someone (or a team of people) to do the exact same thing for *their* data. The sheer amount of duplication that happens across companies is surely something that wouldn't exist if money weren't such a driver of why companies do what they do. \r\n\r\nThe brief mention of open-source work surprised me - [Working in Public](https://micro.blog/books/9780578675862) details how poorly this is supported, but this book highlights that when all of the \"fun\" work of building tools is open source, no one builds the \"core\" pieces, so a lot of companies end up spending a lot of effort \"duct taping\" those free, open-source solutions together.\r\n\r\nThis book also dives into the political landscape (mostly in the US but also elsewhere) and articulates why it's all washed together now (and doomed, no matter which side you're on).\r\n\r\nAs for what I didn't like about this book - the footnotes take up so much space (sometimes an entire page themselves) that they've been moved to the end of the book, but they are so frequent (sometimes one per sentence) that I read the entire book flipping back and forth between them. I had to use two bookmarks. This was extremely frustrating, particularly when the footnotes were of little importance. Many had URLs which I haven't checked, but are probably stale. Some were interesting, but this made the book much harder to read.\r\n\r\nDefinitely an interesting read, but if you hated society before reading it (I did) then you're not going to be happier afterwards (I'm not).\n",
				"date_published": "2023-04-12T10:10:58+09:30",
				"url": "https://jcarroll.xyz/2023/04/12/finished-reading-bullshit.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/04/12/finished-reading-soonish.html",
				"title": "Finished reading: Soonish by Kelly and Zach Weinersmith 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781846149009\">Soonish</a> by Kelly and Zach Weinersmith 📚</p>\n<p>Another &lsquo;prominently featured on a library shelf&rsquo; find that caught my attention partly because of the authors' name - I&rsquo;ve spent many hours reading <a href=\"https://www.smbc-comics.com/\">Saturday Morning Breakfast Cereal</a> and there was definitely a time during my PhD when reading the day&rsquo;s new comics (along with <a href=\"https://xkcd.com/\">XKCD</a> and others) was just part of my routine.</p>\n<p>This condenses what was surely a lot of research down into digestible summaries - punctuated with humour - about how some new technologies might change the world. It&rsquo;s almost a shame that it&rsquo;s not longer; I suspect a <em>lot</em> of material had to be cut just to get it down to the already sizeable read it is.</p>\n<p>This is approachable for anyone not already familiar with these fields, and the authors use some clever analogies to explain the concepts.</p>\n<p>Worth a read, for sure. It will be interesting to see how much of it plays out as they predict.</p>\n",
				"content_text": "Finished reading: [Soonish](https://micro.blog/books/9781846149009) by Kelly and Zach Weinersmith 📚\r\n\r\nAnother 'prominently featured on a library shelf' find that caught my attention partly because of the authors' name - I've spent many hours reading [Saturday Morning Breakfast Cereal](https://www.smbc-comics.com/) and there was definitely a time during my PhD when reading the day's new comics (along with [XKCD](https://xkcd.com/) and others) was just part of my routine.\r\n\r\nThis condenses what was surely a lot of research down into digestible summaries - punctuated with humour - about how some new technologies might change the world. It's almost a shame that it's not longer; I suspect a *lot* of material had to be cut just to get it down to the already sizeable read it is.\r\n\r\nThis is approachable for anyone not already familiar with these fields, and the authors use some clever analogies to explain the concepts. \r\n\r\nWorth a read, for sure. It will be interesting to see how much of it plays out as they predict.\n",
				"date_published": "2023-04-12T09:40:51+09:30",
				"url": "https://jcarroll.xyz/2023/04/12/finished-reading-soonish.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/04/12/finished-reading-version.html",
				"title": "Finished reading: Version Zero by David Yoon 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780593190371\">Version Zero</a> by David Yoon 📚</p>\n<p>I thoroughly enjoyed this book. Credit to my local librarians who placed it prominently on a shelf I was casually browsing. The author does a great job of slowly building up the characters and turning up the dial in the last third of the story to the point that I really didn&rsquo;t want to stop reading it.</p>\n<p>The plot is highly relevant to modern social network / business issues and there are some not-so-subtle allusions to the current largest big tech companies.</p>\n<p>I <a href=\"https://jcarroll.com.au/2023/03/31/version-zero-easter-eggs/\">wrote about the easter egg code on my main blog</a> and have heard back from the person who wrote it (update coming soon).</p>\n<p>Definitely recommend. Once you&rsquo;re finished, go back and re-read the first few pages.</p>\n",
				"content_text": "Finished reading: [Version Zero](https://micro.blog/books/9780593190371) by David Yoon 📚\r\n\r\nI thoroughly enjoyed this book. Credit to my local librarians who placed it prominently on a shelf I was casually browsing. The author does a great job of slowly building up the characters and turning up the dial in the last third of the story to the point that I really didn't want to stop reading it. \r\n\r\nThe plot is highly relevant to modern social network / business issues and there are some not-so-subtle allusions to the current largest big tech companies. \r\n\r\nI [wrote about the easter egg code on my main blog](https://jcarroll.com.au/2023/03/31/version-zero-easter-eggs/) and have heard back from the person who wrote it (update coming soon).\r\n\r\nDefinitely recommend. Once you're finished, go back and re-read the first few pages.\n",
				"date_published": "2023-04-12T09:32:53+09:30",
				"url": "https://jcarroll.xyz/2023/04/12/finished-reading-version.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/12/finished-reading-eastern.html",
				"title": "Finished reading: Eastern Standard Tribe by Cory Doctorow 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780007327942\">Eastern Standard Tribe</a> by Cory Doctorow 📚</p>\n<p>I found this one browsing the shelves at the library and recognised the name from my days reading <a href=\"https://boingboing.net/\">boingboing.net</a> (and the <a href=\"https://www.explainxkcd.com/wiki/index.php/Category:Comics_featuring_Cory_Doctorow\">numerous mentions</a> in XKCD, e.g. <a href=\"https://xkcd.com/239/\">this one</a>). I didn&rsquo;t hate this book, but the writing style did remind me a lot of <a href=\"https://micro.blog/books/9780141924045?title=Snow+Crash&amp;author=Neal+Stephenson&amp;cover_id=68854\">Snow Crash</a> (which I didn&rsquo;t like) with all the (unnecessary?) techy/futuristic names for things and places that were only named to highlight how techy/futuristic the world was portrayed to be. The story itself was okay but never <em>really</em> seemed to go anywhere.</p>\n<p>Not a book I&rsquo;d steer away from (I actually gave away my purchased copy of Snow Crash because I don&rsquo;t ever intend to read it again) but not something I&rsquo;ll steer people towards, either.</p>\n",
				"content_text": "Finished reading: [Eastern Standard Tribe](https://micro.blog/books/9780007327942) by Cory Doctorow 📚\r\n\r\nI found this one browsing the shelves at the library and recognised the name from my days reading [boingboing.net](https://boingboing.net/) (and the [numerous mentions](https://www.explainxkcd.com/wiki/index.php/Category:Comics_featuring_Cory_Doctorow) in XKCD, e.g. [this one](https://xkcd.com/239/)). I didn't hate this book, but the writing style did remind me a lot of [Snow Crash](https://micro.blog/books/9780141924045?title=Snow+Crash&author=Neal+Stephenson&cover_id=68854) (which I didn't like) with all the (unnecessary?) techy/futuristic names for things and places that were only named to highlight how techy/futuristic the world was portrayed to be. The story itself was okay but never *really* seemed to go anywhere.\r\n\r\nNot a book I'd steer away from (I actually gave away my purchased copy of Snow Crash because I don't ever intend to read it again) but not something I'll steer people towards, either.\n",
				"date_published": "2023-03-12T13:20:32+09:30",
				"url": "https://jcarroll.xyz/2023/03/12/finished-reading-eastern.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/12/finished-reading-chess.html",
				"title": "Finished reading: 500 Chess Questions Answered by Andrew Soltis 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781849947596\">500 Chess Questions Answered</a> by Andrew Soltis 📚</p>\n<p>Not a novel and not a textbook, but it took long enough to read that it deserves mention. This is full of valuable tips and notes, but aside from reading it cover to cover, it doesn&rsquo;t serve much use as a reference. There were a few points about calculating that I can incorporate into my games, but otherwise, it seems to be focused at beginners despite having some pretty complex examples.</p>\n<p>If you&rsquo;re new to the game, it&rsquo;s worth a read.</p>\n<p>I&rsquo;m jonocarroll on lichess.org and chess.com if anyone wants a game. My Elo is currently around 1100 (rapid).</p>\n",
				"content_text": "Finished reading: [500 Chess Questions Answered](https://micro.blog/books/9781849947596) by Andrew Soltis 📚\r\n\r\nNot a novel and not a textbook, but it took long enough to read that it deserves mention. This is full of valuable tips and notes, but aside from reading it cover to cover, it doesn't serve much use as a reference. There were a few points about calculating that I can incorporate into my games, but otherwise, it seems to be focused at beginners despite having some pretty complex examples.\r\n\r\nIf you're new to the game, it's worth a read.\r\n\r\nI'm jonocarroll on lichess.org and chess.com if anyone wants a game. My Elo is currently around 1100 (rapid).\n",
				"date_published": "2023-03-12T13:09:11+09:30",
				"url": "https://jcarroll.xyz/2023/03/12/finished-reading-chess.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/05/220456.html",
				"title": "Finished reading: The Apocalypse Seven by Gene Doucette 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780358418948\">The Apocalypse Seven</a> by Gene Doucette 📚</p>\n<p>As soon as I finished <a href=\"https://jcarroll.xyz/2023/03/05/finished-reading-the.html\">The Spaceship Next Door</a> I had a look for the author&rsquo;s other books. I got mixed up between a couple and I thought this one was the sequel. It&rsquo;s not, but I borrowed a copy anyway.</p>\n<p>This is a different story, but told with the same narrative style that again feels distinct from most other sci-fi I&rsquo;ve read. Still very enjoyable; I finished this one in just a few evenings.</p>\n<p>My only gripe with this book was that the author clearly had a familiarity with the area (they note that they spent quite a bit of time there in real life) but I&rsquo;ve never been to <a href=\"https://www.google.com/maps/search/Harvard+University,+Cambridge,+MA,+USA/@42.3733004,-71.1183839,14.64z\">Cambridge</a> and so the frequent specific references to real streets and landmarks were all lost on me. Despite that, I could certainly follow along and enjoyed the story.</p>\n<p>Another fun read. I&rsquo;m looking forward to getting a copy of the actual sequel to that first book.</p>\n",
				"content_text": "Finished reading: [The Apocalypse Seven](https://micro.blog/books/9780358418948) by Gene Doucette 📚\r\n\r\nAs soon as I finished [The Spaceship Next Door](https://jcarroll.xyz/2023/03/05/finished-reading-the.html) I had a look for the author's other books. I got mixed up between a couple and I thought this one was the sequel. It's not, but I borrowed a copy anyway.\r\n\r\nThis is a different story, but told with the same narrative style that again feels distinct from most other sci-fi I've read. Still very enjoyable; I finished this one in just a few evenings. \r\n\r\nMy only gripe with this book was that the author clearly had a familiarity with the area (they note that they spent quite a bit of time there in real life) but I've never been to [Cambridge](https://www.google.com/maps/search/Harvard+University,+Cambridge,+MA,+USA/@42.3733004,-71.1183839,14.64z) and so the frequent specific references to real streets and landmarks were all lost on me. Despite that, I could certainly follow along and enjoyed the story.\r\n\r\nAnother fun read. I'm looking forward to getting a copy of the actual sequel to that first book.\n",
				"date_published": "2023-03-05T21:04:56+09:30",
				"url": "https://jcarroll.xyz/2023/03/05/220456.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/05/215624.html",
				"title": "Finished reading: The Phoenix Project by Gene Kim 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781942788294\">The Phoenix Project</a> by Gene Kim 📚</p>\n<p>I was on the fence about this one; I&rsquo;ve managed a tech team before but I&rsquo;m not currently in charge of anyone so I&rsquo;ve about had my fill of &lsquo;mangement&rsquo; information books. With that said, this is a novel that tells a story which happens to describe some best practices and approaches for running (or being in) a tech team.</p>\n<p>The part I really like is <a href=\"https://itrevolution.com/articles/the-three-ways-principles-underpinning-devops/\">The Three Ways</a> principles and I think there&rsquo;s insight there for anyone working in a project of any kind. The story makes a lot of parallels between manufacturing physical goods and &lsquo;knowledge work&rsquo;, focussing on bottlenecks, and planning, and if you haven&rsquo;t thought about your project in that way then I&rsquo;d say definitely pick up this book. &ldquo;Go fast and break things&rdquo; runs counter to a lot of that, but eventually you&rsquo;re going to need to build something a bit more robust, and some planning will be of great benefit.</p>\n<p>The writing itself felt a little forced, but perhaps I just don&rsquo;t deal with people who talk like that (?).</p>\n<p>If you&rsquo;re near the top of a tech-focussed team, I recommend you have a read. There might not be anything new to you in there, but if there is, it&rsquo;ll be useful.</p>\n",
				"content_text": "Finished reading: [The Phoenix Project](https://micro.blog/books/9781942788294) by Gene Kim 📚\r\n\r\nI was on the fence about this one; I've managed a tech team before but I'm not currently in charge of anyone so I've about had my fill of 'mangement' information books. With that said, this is a novel that tells a story which happens to describe some best practices and approaches for running (or being in) a tech team.\r\n\r\nThe part I really like is [The Three Ways](https://itrevolution.com/articles/the-three-ways-principles-underpinning-devops/) principles and I think there's insight there for anyone working in a project of any kind. The story makes a lot of parallels between manufacturing physical goods and 'knowledge work', focussing on bottlenecks, and planning, and if you haven't thought about your project in that way then I'd say definitely pick up this book. \"Go fast and break things\" runs counter to a lot of that, but eventually you're going to need to build something a bit more robust, and some planning will be of great benefit.\r\n\r\nThe writing itself felt a little forced, but perhaps I just don't deal with people who talk like that (?).\r\n\r\nIf you're near the top of a tech-focussed team, I recommend you have a read. There might not be anything new to you in there, but if there is, it'll be useful.\n",
				"date_published": "2023-03-05T20:56:24+09:30",
				"url": "https://jcarroll.xyz/2023/03/05/215624.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/05/finished-reading-jellyfish.html",
				"title": "Finished reading: Jellyfish Age Backwards by Nicklas Brendborg 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781529387926\">Jellyfish Age Backwards</a> by Nicklas Brendborg 📚</p>\n<p>Another recommendation from social media, if I recall. This had a lot of really interesting information about aging across species, some things we&rsquo;ve discovered that influence it, and some things that don&rsquo;t. The focus extended beyond just humans, so this was a really nice broad exploration with neat comparisons and details about unique species.</p>\n<p>Nothing too complex in there, but well worth a read.</p>\n",
				"content_text": "Finished reading: [Jellyfish Age Backwards](https://micro.blog/books/9781529387926) by Nicklas Brendborg 📚\r\n\r\nAnother recommendation from social media, if I recall. This had a lot of really interesting information about aging across species, some things we've discovered that influence it, and some things that don't. The focus extended beyond just humans, so this was a really nice broad exploration with neat comparisons and details about unique species.\r\n\r\nNothing too complex in there, but well worth a read.\n",
				"date_published": "2023-03-05T20:45:44+09:30",
				"url": "https://jcarroll.xyz/2023/03/05/finished-reading-jellyfish.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/05/finished-reading-the.html",
				"title": "Finished reading: The Spaceship Next Door by Gene Doucette 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781328567543\">The Spaceship Next Door</a> by Gene Doucette 📚</p>\n<p>I like to mix in some fiction between more serious topics, and someone recommended this. I couldn&rsquo;t find a physical copy anywhere so I tried out my library&rsquo;s ebook offering (<a href=\"https://www.overdrive.com/apps/libby\">Libby</a>). I&rsquo;d have preferred to use my new Kindle Scribe, but this worked okay enough on my phone.</p>\n<p>The author has a refreshing narrative style for sci-fi; something I can&rsquo;t quite put my finger on, but it was a very enjoyable read. Some twists and turns and none of it felt too forced or strayed too far from the main storyline. I&rsquo;m not quite sure it&rsquo;s the kind of story I&rsquo;d come back to again and again but I genuinely enjoyed it and have a hold on a physical copy of the sequel already (plus another from the author).</p>\n<p>Thumbs up, worth a read.</p>\n",
				"content_text": "Finished reading: [The Spaceship Next Door](https://micro.blog/books/9781328567543) by Gene Doucette 📚\r\n\r\nI like to mix in some fiction between more serious topics, and someone recommended this. I couldn't find a physical copy anywhere so I tried out my library's ebook offering ([Libby](https://www.overdrive.com/apps/libby)). I'd have preferred to use my new Kindle Scribe, but this worked okay enough on my phone.\r\n\r\nThe author has a refreshing narrative style for sci-fi; something I can't quite put my finger on, but it was a very enjoyable read. Some twists and turns and none of it felt too forced or strayed too far from the main storyline. I'm not quite sure it's the kind of story I'd come back to again and again but I genuinely enjoyed it and have a hold on a physical copy of the sequel already (plus another from the author).\r\n\r\nThumbs up, worth a read.\n",
				"date_published": "2023-03-05T20:41:05+09:30",
				"url": "https://jcarroll.xyz/2023/03/05/finished-reading-the.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/05/finished-reading-human.html",
				"title": "Finished reading: 10% Human: How Your Body’s Microbes Hold The Key To Health And Happiness by Alanna Collen 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780007584048\">10% Human: How Your Body’s Microbes Hold The Key To Health And Happiness</a> by Alanna Collen 📚</p>\n<p>I had this one on my shelf for a while and I&rsquo;m very happy I finally got around to reading it. I knew (roughly) that we have a vast microbiome within our gut, but I hadn&rsquo;t really seen most of the details about just how impactful we now know that to be to our health. In particular, I wasn&rsquo;t aware of the connection to eating fiber; my understanding only extended as far as fiber being something indigestible that helped keep things moving right. As it turns out, there&rsquo;s a whole lot more to it, and a lot involves gut bacteria.</p>\n<p>Sure enough, I&rsquo;ve vastly increased my fiber intake (gradually) after reading this. I&rsquo;m also very curious about what we might be able to learn about the intersection of autoimmune (and other) diseases and the microbiome.</p>\n<p>I&rsquo;ve since been recommended a slightly newer <a href=\"https://micro.blog/books/9780062368621\">book</a> on this topic which I now have a copy of.</p>\n<p>Highly recommended - if you read this and don&rsquo;t increase your fiber intake / support your microbiome, I&rsquo;ll be surprised.</p>\n",
				"content_text": "Finished reading: [10% Human: How Your Body’s Microbes Hold The Key To Health And Happiness](https://micro.blog/books/9780007584048) by Alanna Collen 📚\n\nI had this one on my shelf for a while and I'm very happy I finally got around to reading it. I knew (roughly) that we have a vast microbiome within our gut, but I hadn't really seen most of the details about just how impactful we now know that to be to our health. In particular, I wasn't aware of the connection to eating fiber; my understanding only extended as far as fiber being something indigestible that helped keep things moving right. As it turns out, there's a whole lot more to it, and a lot involves gut bacteria.\n\nSure enough, I've vastly increased my fiber intake (gradually) after reading this. I'm also very curious about what we might be able to learn about the intersection of autoimmune (and other) diseases and the microbiome.\n\nI've since been recommended a slightly newer [book](https://micro.blog/books/9780062368621) on this topic which I now have a copy of.\n\nHighly recommended - if you read this and don't increase your fiber intake / support your microbiome, I'll be surprised.\n",
				"date_published": "2023-03-05T20:34:13+09:30",
				"url": "https://jcarroll.xyz/2023/03/05/finished-reading-human.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/05/finished-reading-once.html",
				"title": "Finished reading: Once Upon an Algorithm by Martin Erwig 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780262545297\">Once Upon an Algorithm</a> by Martin Erwig 📚</p>\n<p>I liked the premise of this book - algorithms taught with examples from classic children&rsquo;s tales. I didn&rsquo;t finish it, however - it was just too wordy (I got about halfway through). In fairness, I started with a very similar approach to <a href=\"https://beyondspreadsheetswithr.com/\">my own book</a> which for a while had a working title of &ldquo;The R Handyman&rdquo; with allusions to power tools, tool belts, hand tools, etc&hellip; but the metaphors felt too forced after a while and we changed tack to focus on the programming without them.</p>\n<p>What I did read was well presented and went into some decent detail about software design, algorithms, and complexity. At some point, though, I think some sort of implementations were necessary to bring it all together, and this book lacked that.</p>\n<p>Worth a try, but if you&rsquo;re really into algorithms I&rsquo;d say get a book with implementations (e.g. <a href=\"https://jcarroll.xyz/2023/01/29/finished-reading-the.html\">this one</a>).</p>\n",
				"content_text": "Finished reading: [Once Upon an Algorithm](https://micro.blog/books/9780262545297) by Martin Erwig 📚\r\n\r\nI liked the premise of this book - algorithms taught with examples from classic children's tales. I didn't finish it, however - it was just too wordy (I got about halfway through). In fairness, I started with a very similar approach to [my own book](https://beyondspreadsheetswithr.com/) which for a while had a working title of \"The R Handyman\" with allusions to power tools, tool belts, hand tools, etc... but the metaphors felt too forced after a while and we changed tack to focus on the programming without them.\r\n\r\nWhat I did read was well presented and went into some decent detail about software design, algorithms, and complexity. At some point, though, I think some sort of implementations were necessary to bring it all together, and this book lacked that.\r\n\r\nWorth a try, but if you're really into algorithms I'd say get a book with implementations (e.g. [this one](https://jcarroll.xyz/2023/01/29/finished-reading-the.html)).\n",
				"date_published": "2023-03-05T20:22:04+09:30",
				"url": "https://jcarroll.xyz/2023/03/05/finished-reading-once.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/03/01/finished-reading-the.html",
				"title": "Finished reading: The Book of Why by Judea Pearl 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780241242643\">The Book of Why</a> by Judea Pearl 📚</p>\n<p>This was the topic of a book club at work but I&rsquo;m really glad I read it. My scepticism going in was probably typical of someone not all that familiar with causal analysis, believing that we can just throw all the variables at a regression model and get an answer - anything uncorrelated will have a small coefficient and we can dispose of it. This book - while it takes a slightly arrogant/high-and-mighty approach to getting there - carefully explains that this approach works <em>only</em> if there is no dependency between the variables. This is, of course, structured into the regression model assumptions that the covariates are &ldquo;<em>independent</em> and identically distributed&rdquo; (i.i.d.) but who checks assumptions? It goes into depth about the different ways that covariates can be connected; how to route around some of them; and how to figure out which ones to include.</p>\n<p>Some of the examples seemed a bit too strawman for my liking, but I do think the general foundation is pretty solid. It&rsquo;s a bit odd to have what should really be a textbook in causal analysis as a prose-heavy combination of history and wordy examples, but then again I can&rsquo;t say I&rsquo;d have picked up the textbook and read it cover-to-cover like this.</p>\n<p>Overall, I think this should be on any data scientist&rsquo;s reading list at some point. I have a bunch of follow-on reading to get through now, but I&rsquo;m much less likely to make the simple errors in my own statistical analyses (even if I do need to find an analyst who <em>can</em> work it out).</p>\n",
				"content_text": "Finished reading: [The Book of Why](https://micro.blog/books/9780241242643) by Judea Pearl 📚\r\n\r\nThis was the topic of a book club at work but I'm really glad I read it. My scepticism going in was probably typical of someone not all that familiar with causal analysis, believing that we can just throw all the variables at a regression model and get an answer - anything uncorrelated will have a small coefficient and we can dispose of it. This book - while it takes a slightly arrogant/high-and-mighty approach to getting there - carefully explains that this approach works _only_ if there is no dependency between the variables. This is, of course, structured into the regression model assumptions that the covariates are \"*independent* and identically distributed\" (i.i.d.) but who checks assumptions? It goes into depth about the different ways that covariates can be connected; how to route around some of them; and how to figure out which ones to include.\r\n\r\nSome of the examples seemed a bit too strawman for my liking, but I do think the general foundation is pretty solid. It's a bit odd to have what should really be a textbook in causal analysis as a prose-heavy combination of history and wordy examples, but then again I can't say I'd have picked up the textbook and read it cover-to-cover like this.\r\n\r\nOverall, I think this should be on any data scientist's reading list at some point. I have a bunch of follow-on reading to get through now, but I'm much less likely to make the simple errors in my own statistical analyses (even if I do need to find an analyst who _can_ work it out).\n",
				"date_published": "2023-03-01T17:57:26+09:30",
				"url": "https://jcarroll.xyz/2023/03/01/finished-reading-the.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/01/29/finished-reading-the.html",
				"title": "Finished reading: The Self-Taught Computer Scientist by Cory Althoff 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781119724339\">The Self-Taught Computer Scientist</a> by Cory Althoff 📚</p>\n<p>This is the book I wish I&rsquo;d read before doing <a href=\"https://adventofcode.com/2022\">Advent of Code</a> - a full blog post on that will eventually be on my <a href=\"https://jcarroll.com.au\">main blog</a>; I finished both parts of all 25 exercises in (strictly) base R, and am more than halfway through re-doing all of them in Rust. I was surprised at how much computer science was needed to solve these, but I did enjoy what I learned along the way, so actually reading up on some of it seemed like a good idea.</p>\n<p>This book is the follow-on from <a href=\"https://micro.blog/books/9781472147097\">&lsquo;The Self Taught Programmer&rsquo;</a> (now on my to-read list) and does a really good job of walking through the concepts, partly framed at setting the reader up for being able to solve the typical software engineering interview questions. The code is python, which is approachable enough. There&rsquo;s some minor funkiness of seeing</p>\n<pre tabindex=\"0\"><code class=\"language-{python}\" data-lang=\"{python}\">if condition:\r\n  return True\r\nelse:\r\n  return False\r\n</code></pre><p>rather than just returning <code>condition</code> but otherwise the code is carefully explained. I believe this has the best explanations I&rsquo;ve seen of &lsquo;big O notation&rsquo; for time complexity and how the different variations arise. Similarly, this is the first time I&rsquo;ve understood what a linked list and binary tree are (and how/why someone may want to invert them - I&rsquo;ve only seen the stereotypical interview question).</p>\n<p>As always, the more you learn about programming in one language the more you learn about all the other languages you know, so now I&rsquo;m interested in understanding some of these concepts from an R perspective. I certainly made use of <code>VecDeque</code> and <code>HashMap</code> in my Rust AoC solutions, but in R I was stuck with poorly-performing <code>vector</code> and <code>list</code> objects, which I occasionally improved with an <code>environment</code>. I was very happy to see that R 4.2.2 gains <code>utils::hashtab()</code> (<a href=\"https://stat.ethz.ch/R-manual/R-devel/library/utils/html/hashtab.html\">link</a>)!.</p>\n<p>Overall I was happy with this book. A great introduction to the concepts, and some useful approaches to interview questions (if you&rsquo;re likely to be asked them).</p>\n",
				"content_text": "Finished reading: [The Self-Taught Computer Scientist](https://micro.blog/books/9781119724339) by Cory Althoff 📚\r\n\r\nThis is the book I wish I'd read before doing [Advent of Code](https://adventofcode.com/2022) - a full blog post on that will eventually be on my [main blog](https://jcarroll.com.au); I finished both parts of all 25 exercises in (strictly) base R, and am more than halfway through re-doing all of them in Rust. I was surprised at how much computer science was needed to solve these, but I did enjoy what I learned along the way, so actually reading up on some of it seemed like a good idea.\r\n\r\nThis book is the follow-on from ['The Self Taught Programmer'](https://micro.blog/books/9781472147097) (now on my to-read list) and does a really good job of walking through the concepts, partly framed at setting the reader up for being able to solve the typical software engineering interview questions. The code is python, which is approachable enough. There's some minor funkiness of seeing\r\n\r\n```{python}\r\nif condition:\r\n  return True\r\nelse:\r\n  return False\r\n```\r\n\r\nrather than just returning `condition` but otherwise the code is carefully explained. I believe this has the best explanations I've seen of 'big O notation' for time complexity and how the different variations arise. Similarly, this is the first time I've understood what a linked list and binary tree are (and how/why someone may want to invert them - I've only seen the stereotypical interview question).\r\n\r\nAs always, the more you learn about programming in one language the more you learn about all the other languages you know, so now I'm interested in understanding some of these concepts from an R perspective. I certainly made use of `VecDeque` and `HashMap` in my Rust AoC solutions, but in R I was stuck with poorly-performing `vector` and `list` objects, which I occasionally improved with an `environment`. I was very happy to see that R 4.2.2 gains `utils::hashtab()` ([link](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/hashtab.html))!. \r\n\r\nOverall I was happy with this book. A great introduction to the concepts, and some useful approaches to interview questions (if you're likely to be asked them).\n",
				"date_published": "2023-01-29T14:38:08+09:30",
				"url": "https://jcarroll.xyz/2023/01/29/finished-reading-the.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/01/26/finished-reading-living.html",
				"title": "Finished reading: Living in Data by Jer Thorp 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781250849151\">Living in Data</a> by Jer Thorp 📚</p>\n<p>(<a href=\"https://jcarroll.xyz/2023/01/20/currently-reading-living.html\">previously</a>)</p>\n<p>As a data person, this book spoke to me deeply. As someone who has worked with collected data many times, it offered a fresh insight into understanding nuances of data, where it has come from, how it is <em>never</em> collected without human involvement, and how biases are embedded into every bit and byte.</p>\n<p>Reading this at the same time as <a href=\"https://jcarroll.xyz/2023/01/26/finished-reading-the.html\">The Checklist Manifesto</a> was timely as there is a lot of overlap between the ideas in terms of gathering data.</p>\n<p>There were many great examples of &ldquo;practical visualisations&rdquo; and real-world projects involving an intersection of art and (data) science. One of the most interesting was a project involving planting genetically identical trees across a wide area, the idea being that &ldquo;how they grow&rdquo; reflects the local conditions, and as such they are a proxy for data about each area. It&rsquo;s a <a href=\"https://www.deeproot.com/blog/blog-entries/onetrees-the-forgotten-tree-art-project/\">somewhat lost project</a> it seems, but interesting nonetheless.</p>\n<p>My very minor nitpick about this one was the very common American-ism of assuming everyone is in the Northern Hemisphere, and as such &ldquo;winter&rdquo; and &ldquo;spring&rdquo; refer to unique times of the year. It reminded me of the bias inherent in the construction of the &lsquo;north is up&rsquo; mental model of the world</p>\n<!-- raw HTML omitted -->\n<p>(you do have to choose some direction, and any is as good as any other, but it&rsquo;s a choice).</p>\n<p>I loved reading this book, and I will be strongly recommending it to everyone who works with data. I am very happy that I picked up this one - sometimes browsing the shelves randomly works.</p>\n",
				"content_text": "Finished reading: [Living in Data](https://micro.blog/books/9781250849151) by Jer Thorp 📚\n\n([previously](https://jcarroll.xyz/2023/01/20/currently-reading-living.html))\n\nAs a data person, this book spoke to me deeply. As someone who has worked with collected data many times, it offered a fresh insight into understanding nuances of data, where it has come from, how it is *never* collected without human involvement, and how biases are embedded into every bit and byte.\n\nReading this at the same time as [The Checklist Manifesto](https://jcarroll.xyz/2023/01/26/finished-reading-the.html) was timely as there is a lot of overlap between the ideas in terms of gathering data.\n\nThere were many great examples of \"practical visualisations\" and real-world projects involving an intersection of art and (data) science. One of the most interesting was a project involving planting genetically identical trees across a wide area, the idea being that \"how they grow\" reflects the local conditions, and as such they are a proxy for data about each area. It's a [somewhat lost project](https://www.deeproot.com/blog/blog-entries/onetrees-the-forgotten-tree-art-project/) it seems, but interesting nonetheless.\n\nMy very minor nitpick about this one was the very common American-ism of assuming everyone is in the Northern Hemisphere, and as such \"winter\" and \"spring\" refer to unique times of the year. It reminded me of the bias inherent in the construction of the 'north is up' mental model of the world\n\n<img src=\"uploads/2023/5254ada67e.jpg\" width=\"600\" height=\"366\" alt=\"\">\n\n(you do have to choose some direction, and any is as good as any other, but it's a choice).\n\nI loved reading this book, and I will be strongly recommending it to everyone who works with data. I am very happy that I picked up this one - sometimes browsing the shelves randomly works.\n",
				"date_published": "2023-01-26T14:28:23+09:30",
				"url": "https://jcarroll.xyz/2023/01/26/finished-reading-living.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/01/26/finished-reading-the.html",
				"title": "Finished reading: The Checklist Manifesto by Atul Gawande 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781846683145\">The Checklist Manifesto</a> by Atul Gawande 📚</p>\n<p>I honestly didn&rsquo;t know what I was getting when I placed a library hold on this book - a recommendation from somewhere, but I assumed it was about something like how to write good checklists or manage priorities. This is not that book. This is a wild ride through the ways that some checklists have been used by the author and others throughout history. Not just any checklists, mind you - it&rsquo;s a tour of how a very important checklist was designed, refined, and implemented. The side-stories are often confronting, impactful, and very entertainingly spun together. I clicked &lsquo;confirm purchase&rsquo; on my own copy of this book before I had finished reading it. I suspect I&rsquo;ll come back to this one again.</p>\n<p>Highly recommended to anyone and everyone, but especially anyone who wants to ensure that things actually get done.</p>\n",
				"content_text": "Finished reading: [The Checklist Manifesto](https://micro.blog/books/9781846683145) by Atul Gawande 📚\r\n\r\nI honestly didn't know what I was getting when I placed a library hold on this book - a recommendation from somewhere, but I assumed it was about something like how to write good checklists or manage priorities. This is not that book. This is a wild ride through the ways that some checklists have been used by the author and others throughout history. Not just any checklists, mind you - it's a tour of how a very important checklist was designed, refined, and implemented. The side-stories are often confronting, impactful, and very entertainingly spun together. I clicked 'confirm purchase' on my own copy of this book before I had finished reading it. I suspect I'll come back to this one again.\r\n\r\nHighly recommended to anyone and everyone, but especially anyone who wants to ensure that things actually get done.\n",
				"date_published": "2023-01-26T14:06:38+09:30",
				"url": "https://jcarroll.xyz/2023/01/26/finished-reading-the.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/01/26/finished-reading-our.html",
				"title": "Finished reading: Our Data, Ourselves by Jacqueline D. Lipton 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780520390508\">Our Data, Ourselves</a> by Jacqueline D. Lipton 📚</p>\n<p>I abandoned this book after a few chapters. I wasn&rsquo;t sure how much I really wanted to read a book about technology and data where the author claims on page 10 that</p>\n<blockquote>\n<p>&ldquo;RFID can be monitored at a distance. You do not need a digital reader in the proximity of the device to locate and &lsquo;read&rsquo; its information. RFID microchips are implanted in livestock and pets to help find them if they are lost. They are implanted into digital devices, notably automobiles, to find them if they become stolen or lost, or simply to track them for work or other purposes.&rdquo;</p>\n</blockquote>\n<p>This is&hellip; somewhere between blatantly wrong and misunderstood. RFID <em>is</em> used for these things (microchips in animals, automobile assembly lines, books at a library) but it&rsquo;s used over very short distances, and is entirely passive (the tag is not powered). Someone has apparently managed to <a href=\"https://www.theregister.com/2006/01/30/dutch_biometric_passport_crack/\">read a passport</a> from about 10 metres using special equipment, but that&rsquo;s far from standard usage. I don&rsquo;t think you can just &ldquo;find&rdquo; an RFID tag in the wild with any reader. You <em>can</em> use one to identify a <em>found</em> animal, or which automobile is passing a sensor in a factory, or which book is being checked out, but there&rsquo;s definitely some strong notion of &ldquo;proximity&rdquo; involved, as far as I&rsquo;m aware (please correct me if I&rsquo;m wrong). I believe RFID is used in automated highway toll collection, but it involves significant power, likely not passively.</p>\n<p>That left a bad enough taste in my (mind)mouth that I wasn&rsquo;t particularly open to reading in great depth about (very specifically) American law</p>\n<blockquote>\n<p>This book focuses on the American position on individual privacy</p>\n</blockquote>\n<p>especially with the note that</p>\n<blockquote>\n<p>our powerful First Amendment protections of free speech-that is, speech free from government interference-have been regarded as limiting laws that restrict what we can say about each other.</p>\n</blockquote>\n<p>I can only read so many &ldquo;Someone vs Someone&rdquo; names of legal precedents and US-specific names of agencies, court jurisdictions, etc. before giving up. This may be of more interest to someone in the US interested in specific legal aspects, but it&rsquo;s not for me.</p>\n",
				"content_text": "Finished reading: [Our Data, Ourselves](https://micro.blog/books/9780520390508) by Jacqueline D. Lipton 📚\r\n\r\nI abandoned this book after a few chapters. I wasn't sure how much I really wanted to read a book about technology and data where the author claims on page 10 that\r\n\r\n> \"RFID can be monitored at a distance. You do not need a digital reader in the proximity of the device to locate and 'read' its information. RFID microchips are implanted in livestock and pets to help find them if they are lost. They are implanted into digital devices, notably automobiles, to find them if they become stolen or lost, or simply to track them for work or other purposes.\"\r\n\r\nThis is... somewhere between blatantly wrong and misunderstood. RFID _is_ used for these things (microchips in animals, automobile assembly lines, books at a library) but it's used over very short distances, and is entirely passive (the tag is not powered). Someone has apparently managed to [read a passport](https://www.theregister.com/2006/01/30/dutch_biometric_passport_crack/) from about 10 metres using special equipment, but that's far from standard usage. I don't think you can just \"find\" an RFID tag in the wild with any reader. You _can_ use one to identify a _found_ animal, or which automobile is passing a sensor in a factory, or which book is being checked out, but there's definitely some strong notion of \"proximity\" involved, as far as I'm aware (please correct me if I'm wrong). I believe RFID is used in automated highway toll collection, but it involves significant power, likely not passively.\r\n\r\nThat left a bad enough taste in my (mind)mouth that I wasn't particularly open to reading in great depth about (very specifically) American law \r\n\r\n> This book focuses on the American position on individual privacy\r\n\r\nespecially with the note that \r\n\r\n> our powerful First Amendment protections of free speech-that is, speech free from government interference-have been regarded as limiting laws that restrict what we can say about each other.\r\n\r\nI can only read so many \"Someone vs Someone\" names of legal precedents and US-specific names of agencies, court jurisdictions, etc. before giving up. This may be of more interest to someone in the US interested in specific legal aspects, but it's not for me.\r\n\r\n",
				"date_published": "2023-01-26T14:00:22+09:30",
				"url": "https://jcarroll.xyz/2023/01/26/finished-reading-our.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/01/20/currently-reading-living.html",
				"title": "Currently reading: Living in Data by Jer Thorp 📚",
				"content_html": "<p>Currently reading: <a href=\"https://micro.blog/books/9781250849151\">Living in Data</a> by Jer Thorp 📚</p>\n<p>I picked this up browsing shelves in a (particularly <a href=\"https://playandgo.com.au/dymocks-book-store-rundle-mall-adelaide-review/\">beautiful</a>) brick and mortar book store thanks to a voucher I received for a journal article review. So far I&rsquo;m loving it. This was my first introduction to Johanna Drucker&rsquo;s framing of <a href=\"http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html\">&ldquo;capta&rdquo;</a> rather than &ldquo;data&rdquo;, the former better reflecting the essence of information being &ldquo;taken and constructed&rdquo; rather than given. Lots to get through yet, but so far very entertaining and insightful.</p>\n",
				"content_text": "Currently reading: [Living in Data](https://micro.blog/books/9781250849151) by Jer Thorp 📚\n\nI picked this up browsing shelves in a (particularly [beautiful](https://playandgo.com.au/dymocks-book-store-rundle-mall-adelaide-review/)) brick and mortar book store thanks to a voucher I received for a journal article review. So far I'm loving it. This was my first introduction to Johanna Drucker's framing of [\"capta\"](http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html) rather than \"data\", the former better reflecting the essence of information being \"taken and constructed\" rather than given. Lots to get through yet, but so far very entertaining and insightful.\n",
				"date_published": "2023-01-20T21:11:45+09:30",
				"url": "https://jcarroll.xyz/2023/01/20/currently-reading-living.html"
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/01/08/finished-reading-loonshots.html",
				"title": "Finished reading: Loonshots by Safi Bahcall 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781250185976\">Loonshots</a> by Safi Bahcall 📚</p>\n<p>This was recommended by someone at work and this time I&rsquo;m very glad they did. The author carefully details the history of some of the most significant breakthroughs and, as a physicist, isn&rsquo;t shy with the specifics. It was staggering to me at the start of the book how intertwined the &ldquo;big&rdquo; computer tech (and by extension, media; Lucasfilm, Pixar, &hellip;) companies are and the history they share. By the end of the book these connections are much clearer. I hadn&rsquo;t expected quite the history lesson, but the prevalence of persisting English technology over the much older Chinese inventions becomes well-reasoned. Less fun is the contemplation that maybe we don&rsquo;t currently have an equivalent of Bell Labs fostering such speculative exploration without the need to be profitable in the near future. Even the well-funded but negative revenue companies have the plan to be highly profitable, not a plan to make something better. The <a href=\"https://www.bcorporation.net/en-us/certification\">B corporations</a> are a good exception, but they&rsquo;re still not quite &ldquo;loonshot nurseries&rdquo;.</p>\n<p>One of the key learnings for me, as someone who works in a Silicon-valley-adjacent biotech startup (hence the recommendation), was that the attitude of creating a &ldquo;disruptive&rdquo; technology is backwards. Many technologies can be seen as disruptive in hindsight, but they never start out with that attitude. Incremental improvement with the support to try something different certainly leads there, but it&rsquo;s perhaps too large a leap to try to get there sooner. Having the support to try something different that could be better is the first step towards making something incredible, and it certainly won&rsquo;t work every time, but not trying will absolutely lead to not making it.</p>\n<p>Lastly, I don&rsquo;t think it really sunk in while I was working there for nearly five years since I wasn&rsquo;t so read-up on the history, but seeing Genentech mentioned in many of these recent books always makes me do a double-take.</p>\n<p>I thoroughly enjoyed this book and recommend it to anyone interested in how big ideas come about and survive, and why others don&rsquo;t.</p>\n",
				"content_text": "Finished reading: [Loonshots](https://micro.blog/books/9781250185976) by Safi Bahcall 📚\r\n\r\nThis was recommended by someone at work and this time I'm very glad they did. The author carefully details the history of some of the most significant breakthroughs and, as a physicist, isn't shy with the specifics. It was staggering to me at the start of the book how intertwined the \"big\" computer tech (and by extension, media; Lucasfilm, Pixar, ...) companies are and the history they share. By the end of the book these connections are much clearer. I hadn't expected quite the history lesson, but the prevalence of persisting English technology over the much older Chinese inventions becomes well-reasoned. Less fun is the contemplation that maybe we don't currently have an equivalent of Bell Labs fostering such speculative exploration without the need to be profitable in the near future. Even the well-funded but negative revenue companies have the plan to be highly profitable, not a plan to make something better. The [B corporations](https://www.bcorporation.net/en-us/certification) are a good exception, but they're still not quite \"loonshot nurseries\".\r\n\r\nOne of the key learnings for me, as someone who works in a Silicon-valley-adjacent biotech startup (hence the recommendation), was that the attitude of creating a \"disruptive\" technology is backwards. Many technologies can be seen as disruptive in hindsight, but they never start out with that attitude. Incremental improvement with the support to try something different certainly leads there, but it's perhaps too large a leap to try to get there sooner. Having the support to try something different that could be better is the first step towards making something incredible, and it certainly won't work every time, but not trying will absolutely lead to not making it.\r\n\r\nLastly, I don't think it really sunk in while I was working there for nearly five years since I wasn't so read-up on the history, but seeing Genentech mentioned in many of these recent books always makes me do a double-take.\r\n\r\nI thoroughly enjoyed this book and recommend it to anyone interested in how big ideas come about and survive, and why others don't.\n",
				"date_published": "2023-01-08T13:05:31+09:30",
				"url": "https://jcarroll.xyz/2023/01/08/finished-reading-loonshots.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/01/06/implicit-or-explicit.html",
				"title": "Implicit or Explicit connection object?",
				"content_html": "<p>I&rsquo;m wrapping a (stateless, hit-every-time) REST API and my design was challenged with an alternative opinion - which is great! I get to have a more serious think about design and what might work best. I have an internal function which does the actual talking to the server, e.g. <code>.get_from_API()</code> which needs to know the URL, auth key, and query parameters. I originally designed my package to fetch these from environment variables depending on the API instance (e.g. &lsquo;prod&rsquo; vs &lsquo;dev&rsquo;) and the user (their user-specific key). Individual endpoint wrappers essentially boil down to</p>\n<pre tabindex=\"0\"><code class=\"language-{r}\" data-lang=\"{r}\">get_this &lt;- function(something) {\r\n    .get_from_API(construct_endpoint(something))\r\n}\r\n</code></pre><p>I asked the following question on some of the social sites I use:</p>\n<hr>\n<p>Is there a good reason to use one of these vs the other when wrapping an API?</p>\n<p>A:</p>\n<pre tabindex=\"0\"><code class=\"language-{r}\" data-lang=\"{r}\">get_this(x, ...) # GET\r\nget_that(x, ...) # GET\r\nset_this(x, y, ...) # SET\r\nset_that(x, y, ...) # SET\r\n</code></pre><p>with something like this within each of these</p>\n<pre tabindex=\"0\"><code class=\"language-{r}\" data-lang=\"{r}\">greedy_con &lt;- .connect(Sys.getenv(implicit_api_vars), ...)\r\n</code></pre><p>OR</p>\n<p>B:</p>\n<pre tabindex=\"0\"><code class=\"language-{r}\" data-lang=\"{r}\">lazy_con &lt;- .connect(explicit_api_vars, ...)\r\nthis(lazy_con, x, ...) # GET\r\nthat(lazy_con, x, ...) # GET\r\nthis(lazy_con, x) &lt;- y # SET\r\nthat(lazy_con, x) &lt;- y # SET\r\n</code></pre><p>(Or some third option)?</p>\n<hr>\n<p>The motivation for the second option may have come from python where methods on objects are much more common. Indeed, the canonical python version of this wrapper uses</p>\n<pre tabindex=\"0\"><code class=\"language-{python}\" data-lang=\"{python}\">lazy_con = NameOfAPI(url = {}, key = {})\r\n\r\nlazy_con.this()\r\nlazy_con.that()\r\n</code></pre><p>In R we have dispatch (e.g. S3) so I <em>could</em> assign a class <code>mycon</code> to <code>lazy_con</code> and methods <code>this.mycon()</code> and <code>that.mycon()</code> but this seems very overengineered. Apparently BioConductor also uses this method syntax but there the standard is S4 dispatch (and typically larger data) so a more explicit object might make more sense.</p>\n<p>Methods on classes seems to be a common frustration for me in python and rust where I&rsquo;m constantly trying to use some function (e.g. <code>abs(x-y)</code>) which is actually a method (<code>(x-y).abs()</code>) but I think I understand <em>why</em> they&rsquo;re built that way.</p>\n<p>So far the responses seem to lean entirely towards hiding the complexity of the connection away. That said, adopting the &lsquo;setter&rsquo; assignment syntax would mean I could do away with the explicit &lsquo;get&rsquo; in the getter function names.</p>\n<p>Do you have an opinion on this? Let me know!</p>\n",
				"content_text": "I'm wrapping a (stateless, hit-every-time) REST API and my design was challenged with an alternative opinion - which is great! I get to have a more serious think about design and what might work best. I have an internal function which does the actual talking to the server, e.g. `.get_from_API()` which needs to know the URL, auth key, and query parameters. I originally designed my package to fetch these from environment variables depending on the API instance (e.g. 'prod' vs 'dev') and the user (their user-specific key). Individual endpoint wrappers essentially boil down to\r\n\r\n```{r}\r\nget_this <- function(something) {\r\n    .get_from_API(construct_endpoint(something))\r\n}\r\n```\r\n\r\nI asked the following question on some of the social sites I use:\r\n\r\n---\r\n\r\nIs there a good reason to use one of these vs the other when wrapping an API?\r\n\r\nA: \r\n\r\n```{r}\r\nget_this(x, ...) # GET\r\nget_that(x, ...) # GET\r\nset_this(x, y, ...) # SET\r\nset_that(x, y, ...) # SET\r\n```\r\n\r\nwith something like this within each of these\r\n\r\n```{r}\r\ngreedy_con <- .connect(Sys.getenv(implicit_api_vars), ...)\r\n```\r\n\r\nOR\r\n\r\nB:\r\n\r\n```{r}\r\nlazy_con <- .connect(explicit_api_vars, ...)\r\nthis(lazy_con, x, ...) # GET\r\nthat(lazy_con, x, ...) # GET\r\nthis(lazy_con, x) <- y # SET\r\nthat(lazy_con, x) <- y # SET\r\n```\r\n\r\n(Or some third option)?\r\n\r\n---\r\n\r\nThe motivation for the second option may have come from python where methods on objects are much more common. Indeed, the canonical python version of this wrapper uses\r\n\r\n```{python}\r\nlazy_con = NameOfAPI(url = {}, key = {})\r\n\r\nlazy_con.this()\r\nlazy_con.that()\r\n```\r\n\r\nIn R we have dispatch (e.g. S3) so I *could* assign a class `mycon` to `lazy_con` and methods `this.mycon()` and `that.mycon()` but this seems very overengineered. Apparently BioConductor also uses this method syntax but there the standard is S4 dispatch (and typically larger data) so a more explicit object might make more sense.\r\n\r\nMethods on classes seems to be a common frustration for me in python and rust where I'm constantly trying to use some function (e.g. `abs(x-y)`) which is actually a method (`(x-y).abs()`) but I think I understand *why* they're built that way. \r\n\r\nSo far the responses seem to lean entirely towards hiding the complexity of the connection away. That said, adopting the 'setter' assignment syntax would mean I could do away with the explicit 'get' in the getter function names.\r\n\r\nDo you have an opinion on this? Let me know!\n",
				"date_published": "2023-01-06T14:57:11+09:30",
				"url": "https://jcarroll.xyz/2023/01/06/implicit-or-explicit.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2023/01/04/finished-reading-neuromancer.html",
				"title": "Finished reading: Neuromancer by William Gibson 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9783608504880\">Neuromancer</a> by William Gibson 📚</p>\n<p>I was recommended this on one of the socials and figured it was about time to get around to it. As the &lsquo;first&rsquo; &ldquo;cyber&rdquo; sci-fi book it was well-written and despite being conservative in its predictions for the future, many of these time has proven to be accurate. The pace is reasonable, but by the end it felt a bit like one short story without many offshoots - I&rsquo;m not complaining; perhaps I&rsquo;ve read too many books whose author tried to fill the pages with unnecessary side stories. Worth the read, but probably not one I&rsquo;ll come back to too often.</p>\n",
				"content_text": "Finished reading: [Neuromancer](https://micro.blog/books/9783608504880) by William Gibson 📚\n\nI was recommended this on one of the socials and figured it was about time to get around to it. As the 'first' \"cyber\" sci-fi book it was well-written and despite being conservative in its predictions for the future, many of these time has proven to be accurate. The pace is reasonable, but by the end it felt a bit like one short story without many offshoots - I'm not complaining; perhaps I've read too many books whose author tried to fill the pages with unnecessary side stories. Worth the read, but probably not one I'll come back to too often.\n",
				"date_published": "2023-01-04T17:44:09+09:30",
				"url": "https://jcarroll.xyz/2023/01/04/finished-reading-neuromancer.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/08/06/finished-reading-start.html",
				"title": "Finished reading: Start with Why: How Great Leaders Inspire Everyone to Take Action by Simon Sinek ",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781591846444\">Start with Why: How Great Leaders Inspire Everyone to Take Action</a> by Simon Sinek</p>\n<p>I <a href=\"https://jcarroll.xyz/2022/06/29/currently-reading-start.html\">posted about this one</a> after a few chapters because it started to get on my nerves. I think I hate-finished it. The author makes some more questionable connections (a bow needs to pulled <em>away</em> from the target&hellip; what?) and continues to write like the reader is definitely American. The argument that Apple fans and Harley riders like iPhones and Harleys because of <em>why</em> the respective companies do what they do never convinced me in the slightest.</p>\n<p>The message that the <em>why</em> of a company should be prominent, consistent, and persistent seems fine, I just don&rsquo;t think I liked how that point was made (and taken too far).</p>\n<p>Much of the book, particularly the final chapters are great examples of <a href=\"https://en.wikipedia.org/wiki/Survivorship_bias\">survivorship bias</a> but the author seems to either be deliberately overlooking that.</p>\n<p>I didn&rsquo;t particularly enjoy this book.</p>\n<!-- raw HTML omitted -->\n",
				"content_text": "Finished reading: [Start with Why: How Great Leaders Inspire Everyone to Take Action](https://micro.blog/books/9781591846444) by Simon Sinek \n\nI [posted about this one](https://jcarroll.xyz/2022/06/29/currently-reading-start.html) after a few chapters because it started to get on my nerves. I think I hate-finished it. The author makes some more questionable connections (a bow needs to pulled *away* from the target... what?) and continues to write like the reader is definitely American. The argument that Apple fans and Harley riders like iPhones and Harleys because of *why* the respective companies do what they do never convinced me in the slightest. \n\nThe message that the *why* of a company should be prominent, consistent, and persistent seems fine, I just don't think I liked how that point was made (and taken too far).\n\nMuch of the book, particularly the final chapters are great examples of [survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias) but the author seems to either be deliberately overlooking that.\n\nI didn't particularly enjoy this book.\n\n<img src=\"uploads/2022/34065cbf3c.jpg\" width=\"405\" height=\"600\" alt=\"don't make me tap the sign / survivorship bias\" />\n",
				"date_published": "2022-08-06T15:39:29+09:30",
				"url": "https://jcarroll.xyz/2022/08/06/finished-reading-start.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/08/06/finished-reading-sea.html",
				"title": "Finished reading: Sea of Tranquility: A novel by Emily St. John Mandel ",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780593321454\">Sea of Tranquility: A novel</a> by Emily St. John Mandel</p>\n<p>I don&rsquo;t know if it was timely or unfortunate that I read this book so close to <a href=\"https://jcarroll.xyz/2022/07/09/finished-reading-the.html\">The End of Eternity</a> - both had very similar themes, but Asimov is just so good at weaving the threads together into an engrossing story. I think this was a recommendation from Twitter, and while I enjoyed it, it did feel oddly paced. The start felt quite slow - the first half of the book was spent introducing the characters, yet the conclusion seemed to be squashed into the final chapter. Without spoiling too much, there&rsquo;s some <a href=\"https://youtu.be/u4SEDzynMiQ\">bootstrap paradox</a> stuff (as any book involving time-travel should have) that just gets hand-waved away. It&rsquo;s fine if you don&rsquo;t think about it too hard.</p>\n<p>Overall, not bad. Recommended for a not-too-serious read.</p>\n",
				"content_text": "Finished reading: [Sea of Tranquility: A novel](https://micro.blog/books/9780593321454) by Emily St. John Mandel \n\nI don't know if it was timely or unfortunate that I read this book so close to [The End of Eternity](https://jcarroll.xyz/2022/07/09/finished-reading-the.html) - both had very similar themes, but Asimov is just so good at weaving the threads together into an engrossing story. I think this was a recommendation from Twitter, and while I enjoyed it, it did feel oddly paced. The start felt quite slow - the first half of the book was spent introducing the characters, yet the conclusion seemed to be squashed into the final chapter. Without spoiling too much, there's some [bootstrap paradox](https://youtu.be/u4SEDzynMiQ) stuff (as any book involving time-travel should have) that just gets hand-waved away. It's fine if you don't think about it too hard.\n\nOverall, not bad. Recommended for a not-too-serious read.\n",
				"date_published": "2022-08-06T15:16:57+09:30",
				"url": "https://jcarroll.xyz/2022/08/06/finished-reading-sea.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/07/31/160948.html",
				"title": "Finished reading: 12 Bytes: How We Got Here. Where We Might Go Next by Jeanette Winterson ",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781473578258\">12 Bytes: How We Got Here. Where We Might Go Next</a> by Jeanette Winterson</p>\n<p>My library had several copies of this on the shelf, so I assumed it was popular or new. The latter is certainly true (2021). This is a collection of short essays detailing the journey from the first analytical machine towards AGI (Artificial General Intelligence; c.f. narrow AI such as a digital assistant), specifically noting the significant contributions of women (against the prejudices and biases which are only now slowly being dismantled).</p>\n<p>The stories shouldn&rsquo;t be new to anyone, but more light on them is a good thing. The author provides a great level of detail about the contributions and their historical contexts, but borders on misandry with their side remarks. &ldquo;Turn around is fair play&rdquo; one could argue, but I don&rsquo;t think it&rsquo;s helpful here. The (extremely talented) women who programmed the ENIAC (the domain experts on the mathematical equations they were programming the machine to solve) absolutely deserved the credit for their achievements that was withheld from them, but emphasising &ldquo;the men who built it couldn&rsquo;t program it&rdquo; as if to suggest they just stuck some electronics together and couldn&rsquo;t comprehend what they&rsquo;d built without the women seems disingenuous. Arguing that the &ldquo;I&rsquo;m a Mac / I&rsquo;m a PC&rdquo; ads specifically only had men in order to reinforce the stereotype that women don&rsquo;t use computers seems like a stretch.</p>\n<p>I enjoyed reading the historical content of this book. The commentary not so much. Others may enjoy it more than I did.</p>\n",
				"content_text": "Finished reading: [12 Bytes: How We Got Here. Where We Might Go Next](https://micro.blog/books/9781473578258) by Jeanette Winterson \n\nMy library had several copies of this on the shelf, so I assumed it was popular or new. The latter is certainly true (2021). This is a collection of short essays detailing the journey from the first analytical machine towards AGI (Artificial General Intelligence; c.f. narrow AI such as a digital assistant), specifically noting the significant contributions of women (against the prejudices and biases which are only now slowly being dismantled). \n\nThe stories shouldn't be new to anyone, but more light on them is a good thing. The author provides a great level of detail about the contributions and their historical contexts, but borders on misandry with their side remarks. \"Turn around is fair play\" one could argue, but I don't think it's helpful here. The (extremely talented) women who programmed the ENIAC (the domain experts on the mathematical equations they were programming the machine to solve) absolutely deserved the credit for their achievements that was withheld from them, but emphasising \"the men who built it couldn't program it\" as if to suggest they just stuck some electronics together and couldn't comprehend what they'd built without the women seems disingenuous. Arguing that the \"I'm a Mac / I'm a PC\" ads specifically only had men in order to reinforce the stereotype that women don't use computers seems like a stretch.\n\nI enjoyed reading the historical content of this book. The commentary not so much. Others may enjoy it more than I did.\n",
				"date_published": "2022-07-31T16:09:48+09:30",
				"url": "https://jcarroll.xyz/2022/07/31/160948.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/07/16/finished-reading-the.html",
				"title": "Finished reading: The New Childhood: Raising Kids to Thrive in a Connected World by Jordan Shapiro ",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780316437257\">The New Childhood: Raising Kids to Thrive in a Connected World</a> by Jordan Shapiro</p>\n<p>Another break from biology, certainly more towards psychology. This one I found randomly (promoted) at the library and given that it&rsquo;s my kids' school holidays at the moment and they&rsquo;re constantly asking for screens, I figured it was worth a shot. To my surprise, by the end of this book I&rsquo;m encouraged to give my kids <em>more</em> screen time. The big qualifier is the content - the author makes convincing arguments that collaborative games and online chat are the new <a href=\"https://en.wikipedia.org/wiki/Agora\">agora</a>, and that kids growing into that world will need to understand, appreciate, and be able to &ldquo;read&rdquo; (&ldquo;be literate in&rdquo;) the new medium. I&rsquo;m not quite convinced that watching YouTube videos of someone playing an extremely basic free game has the same benefit as they propose comes from collaborative gaming or online forums, but I am convinced to start playing Minecraft <em>with</em> my kids.</p>\n<p>This book nicely balances historical psychology with up-to-date perspectives. I was surprised to learn that kindergarten (as a concept at all) is only a couple hundred years old. The framing of how previous generations of children have been raised within the applicable social setting (most of us are remnants of the industrial age), plus the entire home vs work distinction breaking down in the new internet-based world makes a lot of sense and encourages me to think about what outdated notions I&rsquo;m imposing on my own kids, especially how those can be limiting. In my current work I make use of mind maps and collaborative documents - why would I not want my children to use the same useful tools for their education?</p>\n<p>The latter sections of the book branched out into much wider social commentary - children are growing up within the evolving technological landscape, so it&rsquo;s entirely relevant - and I liked several well-made points about why &ldquo;uninformed inclusion&rdquo; might actually work against goals; why it&rsquo;s not sufficient to just connect everyone and hope they&rsquo;ll be nice; why some people form exclusions against others due to a lack of self-identity; and why online forums are so useful for connecting people, but to the detriment of serendipity.</p>\n<p>That last one really hit home - the R community largely calls Twitter home and it&rsquo;s been a fruitful source of friends, collaborators, and inspiration for me, but I do wonder how much of an echo chamber I&rsquo;m stuck it. Maybe it&rsquo;s time to follow some more python and javascript devs.</p>\n<p>I highly recommend this book to anyone with children who is uncertain about the amount of screen time they&rsquo;re getting. Also just a great read for better understanding the new digital world.</p>\n",
				"content_text": "Finished reading: [The New Childhood: Raising Kids to Thrive in a Connected World](https://micro.blog/books/9780316437257) by Jordan Shapiro \n\nAnother break from biology, certainly more towards psychology. This one I found randomly (promoted) at the library and given that it's my kids' school holidays at the moment and they're constantly asking for screens, I figured it was worth a shot. To my surprise, by the end of this book I'm encouraged to give my kids *more* screen time. The big qualifier is the content - the author makes convincing arguments that collaborative games and online chat are the new [agora](https://en.wikipedia.org/wiki/Agora), and that kids growing into that world will need to understand, appreciate, and be able to \"read\" (\"be literate in\") the new medium. I'm not quite convinced that watching YouTube videos of someone playing an extremely basic free game has the same benefit as they propose comes from collaborative gaming or online forums, but I am convinced to start playing Minecraft *with* my kids.\n\nThis book nicely balances historical psychology with up-to-date perspectives. I was surprised to learn that kindergarten (as a concept at all) is only a couple hundred years old. The framing of how previous generations of children have been raised within the applicable social setting (most of us are remnants of the industrial age), plus the entire home vs work distinction breaking down in the new internet-based world makes a lot of sense and encourages me to think about what outdated notions I'm imposing on my own kids, especially how those can be limiting. In my current work I make use of mind maps and collaborative documents - why would I not want my children to use the same useful tools for their education?\n\nThe latter sections of the book branched out into much wider social commentary - children are growing up within the evolving technological landscape, so it's entirely relevant - and I liked several well-made points about why \"uninformed inclusion\" might actually work against goals; why it's not sufficient to just connect everyone and hope they'll be nice; why some people form exclusions against others due to a lack of self-identity; and why online forums are so useful for connecting people, but to the detriment of serendipity.\n\nThat last one really hit home - the R community largely calls Twitter home and it's been a fruitful source of friends, collaborators, and inspiration for me, but I do wonder how much of an echo chamber I'm stuck it. Maybe it's time to follow some more python and javascript devs.\n\nI highly recommend this book to anyone with children who is uncertain about the amount of screen time they're getting. Also just a great read for better understanding the new digital world.\n",
				"date_published": "2022-07-16T20:15:27+09:30",
				"url": "https://jcarroll.xyz/2022/07/16/finished-reading-the.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/07/09/finished-reading-the.html",
				"title": "Finished reading: The End of Eternity by Isaac Asimov",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9780593160060\">The End of Eternity</a> by Isaac Asimov</p>\n<p>Easily one of the greatest science fiction writers of all time, and it&rsquo;s easy to see why. I&rsquo;ve loved many of them, but I haven&rsquo;t read anywhere near all of Asimov&rsquo;s works. I saw this one on a shelf and figured it would be a nice break from the much heavier non-fiction biology I&rsquo;ve been reading lately.</p>\n<p>The craftsmanship of Asimov&rsquo;s writing always leaves me pleased. Small callbacks throughout seem effortless; the words never seem to be shoehorned in order to tie together different threads (with other authors, certain sentences have a certain &lsquo;<a href=\"https://en.wikipedia.org/wiki/Chekhov%27s_gun\">Chekhov&rsquo;s gun</a>&rsquo; flavour to them).</p>\n<p>I enjoyed reading this book. Twists and turns, a careful balance of science and hand-waving, and a not-too-overwhelming set of characters. I&rsquo;ll certainly be keeping an eye out for more of Asimov&rsquo;s works to correct my lack of coverage. Recommended to time-travel and general sci-fi enthusiasts.</p>\n",
				"content_text": "Finished reading: [The End of Eternity](https://micro.blog/books/9780593160060) by Isaac Asimov \n\nEasily one of the greatest science fiction writers of all time, and it's easy to see why. I've loved many of them, but I haven't read anywhere near all of Asimov's works. I saw this one on a shelf and figured it would be a nice break from the much heavier non-fiction biology I've been reading lately.\n\nThe craftsmanship of Asimov's writing always leaves me pleased. Small callbacks throughout seem effortless; the words never seem to be shoehorned in order to tie together different threads (with other authors, certain sentences have a certain '[Chekhov's gun](https://en.wikipedia.org/wiki/Chekhov%27s_gun)' flavour to them).\n\nI enjoyed reading this book. Twists and turns, a careful balance of science and hand-waving, and a not-too-overwhelming set of characters. I'll certainly be keeping an eye out for more of Asimov's works to correct my lack of coverage. Recommended to time-travel and general sci-fi enthusiasts.\n",
				"date_published": "2022-07-09T17:07:59+09:30",
				"url": "https://jcarroll.xyz/2022/07/09/finished-reading-the.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/06/29/currently-reading-start.html",
				"title": "Currently reading: Start With Why by Simon Sinek ",
				"content_html": "<p>Currently reading: <a href=\"https://micro.blog/books/9780241958230\">Start With Why</a> by Simon Sinek</p>\n<p>I was recommended this book by someone whose opinion I hold in high regard, but so far I&rsquo;m not enjoying this book. Not necessarily for the material - I think I can appreciate the points being made about having a defined &lsquo;why&rsquo; behind a company and explanations of the various manipulations a company can leverage rather than actually being better than the competition, but rather the extreme &ldquo;American-ness&rdquo; of the author. Especially surprising since the author has a more diverse background than simply &lsquo;American&rsquo;.</p>\n<p>It took 48 pages before a single non-American company was mentioned (Ferarri), and even then it was in the context of</p>\n<blockquote>\n<p>&ldquo;if you have a family of six a two-seater Ferarri is not better. However, if you&rsquo;re looking for a great way to meet women, a Honda minivan is probably not better&rdquo;.</p>\n</blockquote>\n<p>I&rsquo;m long out of the dating scene, but that seems&hellip; like a terrible comparison. Is that part of the purchasing decision? I suppose maybe it is for some, but as a reliable consumer group?</p>\n<p>Multiple references to Apple Macs and iTunes being brilliant innovations &ldquo;because people connected with the why of the company&rdquo;. iTunes was a terrible product that was forced onto users in order to use the iPod (a distinct improvement over the removable media competition). My understanding is that it gained significant market share over CDs because it was easier (and potentially cheaper - if you wanted a single song). For myself and many others (the reason I believe it was actually innovative) it was <em>easier</em> than pirating music. A dollar for a song compared to a handful of dollars for a CD or the <em>hassle</em> of downloading and uploading a file - it solved a problem. I don&rsquo;t attribute that to Apple&rsquo;s &ldquo;why&rdquo; - another company that offered that might just as well have had the same success.</p>\n<p>Chapter 4 seems to end with the explanation that &ldquo;Harley Davidson riders want Harleys&rdquo; and &ldquo;Mac people want something starting with an <em>i</em>&rdquo; and that there&rsquo;s a cult aspect to this based on loyalty above actual product superiority but I don&rsquo;t believe this is grounded in any &ldquo;why&rdquo; of those companies. They&rsquo;ve each done well at convincing buyers to be part of their collective, and they&rsquo;ve each done well at having some features their buyers do appreciate (loud engines or smooth interfaces) but they&rsquo;re both viewed as objectively worse products by people who can be considered unbiased. The example of &ldquo;U2 being iconoclastic&rdquo; and so a joint promotional iPod &ldquo;makes sense&rdquo; got a genuine chuckle from me - did people buy more iPods because U2 were involved? From everything I saw of that time, it was ridiculed. Users had an entire album forced onto their devices that they had no interest in.</p>\n<p>Then more &ldquo;everyone is American&rdquo; - I actually had to put the book down during the chapter explaining &ldquo;the biology of belonging&rdquo; with the sentence</p>\n<blockquote>\n<p>&ldquo;Go abroad and you&rsquo;ll form instant bonds with other Americans you meet&rdquo;</p>\n</blockquote>\n<p>Other? I&rsquo;m Australian.</p>\n<p>I got really upset at repeated references to language structure having some &ldquo;hidden meaning&rdquo;. Is it a coincidence that the phrase is &ldquo;hearts and minds&rdquo; in that order? Or &ldquo;art and science&rdquo;&hellip; No. Not really. Sure, the rules are vague, but it&rsquo;s not particularly meaningful in the way the author hints at. There are <a href=\"https://www.cambridge.org/elt/blog/2017/08/31/chips-and-fish-word-order-in-english-collocations/\">accepted orderings</a> to some combinations of words known as &ldquo;collocations&rdquo; that &ldquo;make sense&rdquo; to a native English speaker - anyone who hears &ldquo;chips and fish&rdquo; will instantly recognise something is wrong. The &ldquo;i&rdquo; (/ɪ/) in both &ldquo;mind&rdquo; (/maɪnd/) and &ldquo;science&rdquo; (/ˈsaɪ.əns/) fits well into the regular pattern.</p>\n<p>The same allusion to the layout of the &ldquo;Golden Circle&rdquo; having some correlation with the physical brain structure reeks of ill-informed motivational speakers and those who say &ldquo;walnuts are good for your brain because they look like a brain&rdquo;.</p>\n<p>I&rsquo;m still going to give the rest of the book a chance, but so far it&rsquo;s not rating high.</p>\n",
				"content_text": "Currently reading: [Start With Why](https://micro.blog/books/9780241958230) by Simon Sinek \n\nI was recommended this book by someone whose opinion I hold in high regard, but so far I'm not enjoying this book. Not necessarily for the material - I think I can appreciate the points being made about having a defined 'why' behind a company and explanations of the various manipulations a company can leverage rather than actually being better than the competition, but rather the extreme \"American-ness\" of the author. Especially surprising since the author has a more diverse background than simply 'American'.\n\nIt took 48 pages before a single non-American company was mentioned (Ferarri), and even then it was in the context of\n\n> \"if you have a family of six a two-seater Ferarri is not better. However, if you're looking for a great way to meet women, a Honda minivan is probably not better\".\n\nI'm long out of the dating scene, but that seems... like a terrible comparison. Is that part of the purchasing decision? I suppose maybe it is for some, but as a reliable consumer group?\n\nMultiple references to Apple Macs and iTunes being brilliant innovations \"because people connected with the why of the company\". iTunes was a terrible product that was forced onto users in order to use the iPod (a distinct improvement over the removable media competition). My understanding is that it gained significant market share over CDs because it was easier (and potentially cheaper - if you wanted a single song). For myself and many others (the reason I believe it was actually innovative) it was *easier* than pirating music. A dollar for a song compared to a handful of dollars for a CD or the *hassle* of downloading and uploading a file - it solved a problem. I don't attribute that to Apple's \"why\" - another company that offered that might just as well have had the same success.\n\nChapter 4 seems to end with the explanation that \"Harley Davidson riders want Harleys\" and \"Mac people want something starting with an _i_\" and that there's a cult aspect to this based on loyalty above actual product superiority but I don't believe this is grounded in any \"why\" of those companies. They've each done well at convincing buyers to be part of their collective, and they've each done well at having some features their buyers do appreciate (loud engines or smooth interfaces) but they're both viewed as objectively worse products by people who can be considered unbiased. The example of \"U2 being iconoclastic\" and so a joint promotional iPod \"makes sense\" got a genuine chuckle from me - did people buy more iPods because U2 were involved? From everything I saw of that time, it was ridiculed. Users had an entire album forced onto their devices that they had no interest in.\n\nThen more \"everyone is American\" - I actually had to put the book down during the chapter explaining \"the biology of belonging\" with the sentence\n\n> \"Go abroad and you'll form instant bonds with other Americans you meet\"\n\nOther? I'm Australian.\n\nI got really upset at repeated references to language structure having some \"hidden meaning\". Is it a coincidence that the phrase is \"hearts and minds\" in that order? Or \"art and science\"... No. Not really. Sure, the rules are vague, but it's not particularly meaningful in the way the author hints at. There are [accepted orderings](https://www.cambridge.org/elt/blog/2017/08/31/chips-and-fish-word-order-in-english-collocations/) to some combinations of words known as \"collocations\" that \"make sense\" to a native English speaker - anyone who hears \"chips and fish\" will instantly recognise something is wrong. The \"i\" (/ɪ/) in both \"mind\" (/maɪnd/) and \"science\" (/ˈsaɪ.əns/) fits well into the regular pattern.\n\nThe same allusion to the layout of the \"Golden Circle\" having some correlation with the physical brain structure reeks of ill-informed motivational speakers and those who say \"walnuts are good for your brain because they look like a brain\".\n\nI'm still going to give the rest of the book a chance, but so far it's not rating high.\n",
				"date_published": "2022-06-29T22:45:19+09:30",
				"url": "https://jcarroll.xyz/2022/06/29/currently-reading-start.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/06/28/finished-reading-the.html",
				"title": "Finished reading: The Cell by Joshua Z. Rappoport 📚",
				"content_html": "<p>Finished reading: <a href=\"https://micro.blog/books/9781944648978\">The Cell</a> by Joshua Z. Rappoport 📚</p>\n<p>I enjoyed this book - it started with a good overview of the cellular biology but did move on to more organ-based systems, which was perfect for me. The explanations of the lab and microscopy techniques, advancements, innovations, and discoveries were particularly nice. The detour to examples of academic fraud took a dark turn for such a pleasant book. Recommended for those looking for a nice balance of in-depth science and casual explanations.</p>\n",
				"content_text": "Finished reading: [The Cell](https://micro.blog/books/9781944648978) by Joshua Z. Rappoport 📚\r\n\r\nI enjoyed this book - it started with a good overview of the cellular biology but did move on to more organ-based systems, which was perfect for me. The explanations of the lab and microscopy techniques, advancements, innovations, and discoveries were particularly nice. The detour to examples of academic fraud took a dark turn for such a pleasant book. Recommended for those looking for a nice balance of in-depth science and casual explanations.\n",
				"date_published": "2022-06-28T14:16:18+09:30",
				"url": "https://jcarroll.xyz/2022/06/28/finished-reading-the.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/06/11/currently-reading-the.html",
				"title": "Currently reading: The Cell: Discovering the Microscopic World that Determines our Health, our Consciousness, and our Future",
				"content_html": "<p>Currently reading: <a href=\"https://micro.blog/books/9781944648978\">The Cell: Discovering the Microscopic World that Determines our Health, our Consciousness, and our Future</a> by Joshua Z. Rappoport</p>\n<p>Continuing with the technical theme, so far this is a deep dive into the very specific mechanisms of molecular biology without (yet) too much emphasis on particular cells.</p>\n<p>What surprised me so far was that <a href=\"https://en.wikipedia.org/wiki/Polymerase_chain_reaction\">PCR</a> (as a technology) is only as old as I am. It&rsquo;s gained a lot of attention thanks to a particular virus we&rsquo;ve all heard too much about. I&rsquo;d never run one of these reactions myself (having not passed through a university biology department) and wasn&rsquo;t aware of the technical details. I had heard various claims by armchair biologists that the COVID PCR test was &ldquo;just detecting the flu&rdquo; and that &ldquo;if you run enough cycles you can find anything&rdquo; but with a better explanation of how it works - primers and temperature cycling - it&rsquo;s clear that&rsquo;s all just a load of nonsense. I&rsquo;m enjoying this one so far.</p>\n<p>Also news to me was that RT-PCR (Reverse Transcription-Polymerase Chain Reaction) is the name applicable to PCR of an RNA virus (as <a href=\"https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome_coronavirus_2\">SARS-CoV-2</a> is) and not a meaningfully different version to PCR in that case, despite <a href=\"https://www.flysfo.com/travel-well/covid-19-testing\">some listings having both</a>.</p>\n",
				"content_text": "Currently reading: [The Cell: Discovering the Microscopic World that Determines our Health, our Consciousness, and our Future](https://micro.blog/books/9781944648978) by Joshua Z. Rappoport \n\nContinuing with the technical theme, so far this is a deep dive into the very specific mechanisms of molecular biology without (yet) too much emphasis on particular cells. \n\nWhat surprised me so far was that [PCR](https://en.wikipedia.org/wiki/Polymerase_chain_reaction) (as a technology) is only as old as I am. It's gained a lot of attention thanks to a particular virus we've all heard too much about. I'd never run one of these reactions myself (having not passed through a university biology department) and wasn't aware of the technical details. I had heard various claims by armchair biologists that the COVID PCR test was \"just detecting the flu\" and that \"if you run enough cycles you can find anything\" but with a better explanation of how it works - primers and temperature cycling - it's clear that's all just a load of nonsense. I'm enjoying this one so far.\n\nAlso news to me was that RT-PCR (Reverse Transcription-Polymerase Chain Reaction) is the name applicable to PCR of an RNA virus (as [SARS-CoV-2](https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome_coronavirus_2) is) and not a meaningfully different version to PCR in that case, despite [some listings having both](https://www.flysfo.com/travel-well/covid-19-testing).\n",
				"date_published": "2022-06-11T17:06:26+09:30",
				"url": "https://jcarroll.xyz/2022/06/11/currently-reading-the.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/06/11/finished-reading-life.html",
				"title": "Finished reading: Life Unfolding: How the Human Body Creates Itself",
				"content_html": "<p><a href=\"https://micro.blog/books/9780199673537\">Life Unfolding: How the Human Body Creates Itself</a> by Jamie A. Davies</p>\n<p>How a single cell develops into a full human, and a lot of the molecular biology along the way. I thoroughly enjoyed this read - every chapter had highly interesting points about the particular pathways involved and how the cells end up &ldquo;choosing&rdquo; to do all the things they do; move where they need to move, align along directions, and proliferate/die. I spent a lot of time pausing my reading, looking up branches of other information and going down other rabbit holes. I read this as a library loan, but I enjoyed it so much I&rsquo;ve bought my own permanent copy.</p>\n<p>What surprised me the most (having taken a non-traditional route into biology via physics and programming) - but probably shouldn&rsquo;t have - was how the entire system efficiently re-use mechanisms and pathways for both very early development and for ongoing functionality. I recognised several genes from my cancer immunology work, but I always regarded these as &lsquo;just part of the genetic makeup of a person&rsquo;, not paying attention to how they were critical to actually creating the fully grown person, and are now being repurposed. It&rsquo;s perhaps obvious in hindsight, but there aren&rsquo;t a set of genes for pre-natal growth and another set for adult life. It makes the fact that sometimes these mechanisms fail to work perfectly all the more understandable.</p>\n<p>Highly recommend to anyone interested in the very technical details, but a well-presented resource for those generally interested.</p>\n<!-- raw HTML omitted -->\n",
				"content_text": "[Life Unfolding: How the Human Body Creates Itself](https://micro.blog/books/9780199673537) by Jamie A. Davies \n\nHow a single cell develops into a full human, and a lot of the molecular biology along the way. I thoroughly enjoyed this read - every chapter had highly interesting points about the particular pathways involved and how the cells end up \"choosing\" to do all the things they do; move where they need to move, align along directions, and proliferate/die. I spent a lot of time pausing my reading, looking up branches of other information and going down other rabbit holes. I read this as a library loan, but I enjoyed it so much I've bought my own permanent copy.\n\nWhat surprised me the most (having taken a non-traditional route into biology via physics and programming) - but probably shouldn't have - was how the entire system efficiently re-use mechanisms and pathways for both very early development and for ongoing functionality. I recognised several genes from my cancer immunology work, but I always regarded these as 'just part of the genetic makeup of a person', not paying attention to how they were critical to actually creating the fully grown person, and are now being repurposed. It's perhaps obvious in hindsight, but there aren't a set of genes for pre-natal growth and another set for adult life. It makes the fact that sometimes these mechanisms fail to work perfectly all the more understandable.\n\nHighly recommend to anyone interested in the very technical details, but a well-presented resource for those generally interested.\n\n<img src=\"uploads/2022/d82072c1a2.jpg\" width=\"600\" height=\"800\" alt=\"\" />\n",
				"date_published": "2022-06-11T16:49:17+09:30",
				"url": "https://jcarroll.xyz/2022/06/11/finished-reading-life.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/06/11/163214.html",
				"title": "Finished reading: Life from an RNA World: The Ancestor Within",
				"content_html": "<p>(backlog from March 2022)</p>\n<p><a href=\"https://micro.blog/books/9780674050754\">Life from an RNA World: The Ancestor Within</a> by Michael Yarus</p>\n<p>This was a nice tour of how the complex mechanisms of DNA replication came to be, and how the process works to produce proteins and phenotypes. This was the first time I really felt I understood the difference between DNA and RNA, and how such a mechanism evolved. Highly recommend to anyone interested in genomics/genetics at a technical level.</p>\n",
				"content_text": "(backlog from March 2022)\n\n[Life from an RNA World: The Ancestor Within](https://micro.blog/books/9780674050754) by Michael Yarus\n\nThis was a nice tour of how the complex mechanisms of DNA replication came to be, and how the process works to produce proteins and phenotypes. This was the first time I really felt I understood the difference between DNA and RNA, and how such a mechanism evolved. Highly recommend to anyone interested in genomics/genetics at a technical level.\n",
				"date_published": "2022-06-11T16:48:07+09:30",
				"url": "https://jcarroll.xyz/2022/06/11/163214.html",
				"tags": ["Books"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/04/07/interpolation-animation-in.html",
				"title": "Interpolation animation in Julia",
				"content_html": "<p>I <em>love</em> small projects for helping me learn, especially programming. I&rsquo;m still learning Julia, and have found myself wanting more &ldquo;little silly things&rdquo; I can digest and learn from. A lot of the projects I see in Julia are big mathematical models, and I&rsquo;m just not ready to dive that deep yet.</p>\n<p><a href=\"https://twitter.com/ted_dunning/status/1435027697386721280?s=20&amp;t=cDVb0XOQRJeOjXoTrOz54w\">This series of tweets</a> caught my eye, partly because of the cool animation, but also the bite-sized amount of information it was conveying - that interpolation in Julia can be specified so easily, thanks in large part to the multiple dispatch design of the language.</p>\n<p>&ldquo;Surely I could get those 7 lines of code to run&rdquo; I thought.</p>\n<p>Entering the code into VScode was straightforward enough, no problems there. I could define the interpolation function</p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">interpolate(a, b) = t -&gt; ((1.0-t)*a + t*b)\n</code></pre><p>however extending the <code>*</code> and <code>+</code> methods did require me to <code>import Base:*</code> and <code>import Base:+</code> which I think I knew but had forgotten.</p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">+(f::Function, g::Function) = x -&gt; f(x) + g(x)\n*(t::Number, g::Function) = x -&gt; t * g(x)\n</code></pre><p>Defining the secondary and tertiary interpolations, also straightforward</p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">bz1(p1, p2) = interpolate(p1, p2)\nbz2(p1, p2, p3) = interpolate(bz1(p1, p2), bz1(p2, p3))\nbz3(p1, p2, p3, p4) = interpolate(bz2(p1, p2, p3), bz2(p2, p3, p4))\n</code></pre><p>Now the tricky part - evaluating some of these. I knew that <code>a</code> and <code>b</code> represent points, but how to do that here? They&rsquo;re not single numbers, but coordinates. I tried a <code>Tuple</code> as <code>(1, 2)</code> but that doesn&rsquo;t seem to work. I do need to remember that <code>interpolate</code> is itself a function of <code>t</code>, so that needs to be specified as well. If I try to interpolate halfway between two &ldquo;points&rdquo; with <code>Tuple</code>s</p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">interpolate((0,1), (1,2))(0.5)\nERROR: MethodError: no method matching *(::Float64, ::Tuple{Int64,Int64})\nClosest candidates are:\n  *(::Any, ::Any, ::Any, ::Any...) at operators.jl:538\n  *(::Float64, ::Float64) at float.jl:405\n  *(::AbstractFloat, ::Bool) at bool.jl:112\n</code></pre><p>Okay, how about <code>Array</code>s?</p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">interpolate([0,1], [1,2])(0.5)\n2-element Array{Float64,1}:\n 0.5\n 1.5\n</code></pre><p>Huzzah!</p>\n<p>After that, it&rsquo;s a matter of generating the points specified by</p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">bz3(p1, p2, p3, p4)(t)(t)(t)\n</code></pre><p>for various values of <code>t</code>. I did that with a <code>map</code> and joined the results back into a single <code>Array</code></p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">dots = map(i -&gt; bz3(p1, p2, p3, p4)(i)(i)(i),collect(0:0.1:1))\ndots = hcat(dots...)\ndots\n2×11 Array{Float64,2}:\n 0.5  0.47535  0.5368  0.66245  …  1.36905  1.4872  1.53815  1.5\n 1.0  1.3124   1.5312  1.6588      1.3052   1.0128  0.6436   0.2\n</code></pre><p>That was, I&rsquo;d say, a success.</p>\n<p>Drunk with confidence, I wanted to try to reproduce the animation from the tweet, so I dug into the documentation. It didn&rsquo;t seem too bad, and I think I&rsquo;ve managed to reproduce it pretty well</p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">anim = @animate for t in collect(vcat(0:0.01:1,1:-0.01:0))\n    a = bz3(p1, p2, p3, p4)(t)(t)(t);\n    b1 = bz2(p1, p2, p3)(t)(t);\n    b2 = bz2(p2, p3, p4)(t)(t);\n    c1 = bz1(p1, p2)(t);\n    c2 = bz1(p2, p3)(t);\n    c3 = bz1(p3, p4)(t);\n    stars = hcat(p1, p2, p3, p4);\n    diamond1 = hcat(c1, c2);\n    diamond2 = hcat(c2, c3);\n    square = hcat(b1, b2);\n    plot(xlim = (-0.1,2.5), ylim = (-0.1,2.5), legend = false)\n    scatter!(dots[1,:], dots[2,:], markersize = 2)\n    plot!(diamond1[1,:], diamond1[2,:], markersize = 10, markershape = :diamond, color = :green)\n    plot!(diamond2[1,:], diamond2[2,:], markersize = 10, markershape = :diamond, color = :green)\n    plot!(square[1,:], square[2,:], markersize = 10, markershape = :square, color = :blue)\n    plot!(stars[1,:], stars[2,:], markersize = 10, markershape = :star, color = :purple)\n    scatter!(Tuple(a), markersize = 10, markershape = :circle, markercolor = :red)\nend\n\ngif(anim, fps = 24)\n</code></pre><p><img src=\"https://jcarroll.xyz/uploads/2022/ea5f75012f.gif\" alt=\"\"></p>\n<p>Moving the points around, I get a new version all of my own</p>\n<p><img src=\"https://jcarroll.xyz/uploads/2022/b776bf8259.gif\" alt=\"\"></p>\n<p>I&rsquo;m very happy with how these turned out, and I&rsquo;ve learned a lot! A gist of the code to make these is hosted here: <a href=\"https://gist.github.com/jonocarroll/27f9b57332424ea50ec2970e74d8e3b3\">https://gist.github.com/jonocarroll/27f9b57332424ea50ec2970e74d8e3b3</a></p>\n<p>If there are better ways to do any of the steps (there surely are) please feel free to let me know!</p>\n<p>Was this fun? You Bezier ass!</p>\n",
				"content_text": "I *love* small projects for helping me learn, especially programming. I'm still learning Julia, and have found myself wanting more \"little silly things\" I can digest and learn from. A lot of the projects I see in Julia are big mathematical models, and I'm just not ready to dive that deep yet.\n\n[This series of tweets](https://twitter.com/ted_dunning/status/1435027697386721280?s=20&t=cDVb0XOQRJeOjXoTrOz54w) caught my eye, partly because of the cool animation, but also the bite-sized amount of information it was conveying - that interpolation in Julia can be specified so easily, thanks in large part to the multiple dispatch design of the language.\n\n\"Surely I could get those 7 lines of code to run\" I thought.\n\nEntering the code into VScode was straightforward enough, no problems there. I could define the interpolation function \n```{julia}\ninterpolate(a, b) = t -> ((1.0-t)*a + t*b)\n```\nhowever extending the `*` and `+` methods did require me to `import Base:*` and `import Base:+` which I think I knew but had forgotten.\n```{julia}\n+(f::Function, g::Function) = x -> f(x) + g(x)\n*(t::Number, g::Function) = x -> t * g(x)\n```\nDefining the secondary and tertiary interpolations, also straightforward\n```{julia}\nbz1(p1, p2) = interpolate(p1, p2)\nbz2(p1, p2, p3) = interpolate(bz1(p1, p2), bz1(p2, p3))\nbz3(p1, p2, p3, p4) = interpolate(bz2(p1, p2, p3), bz2(p2, p3, p4))\n```\nNow the tricky part - evaluating some of these. I knew that `a` and `b` represent points, but how to do that here? They're not single numbers, but coordinates. I tried a `Tuple` as `(1, 2)` but that doesn't seem to work. I do need to remember that `interpolate` is itself a function of `t`, so that needs to be specified as well. If I try to interpolate halfway between two \"points\" with `Tuple`s\n```{julia}\ninterpolate((0,1), (1,2))(0.5)\nERROR: MethodError: no method matching *(::Float64, ::Tuple{Int64,Int64})\nClosest candidates are:\n  *(::Any, ::Any, ::Any, ::Any...) at operators.jl:538\n  *(::Float64, ::Float64) at float.jl:405\n  *(::AbstractFloat, ::Bool) at bool.jl:112\n```\nOkay, how about `Array`s?\n```{julia}\ninterpolate([0,1], [1,2])(0.5)\n2-element Array{Float64,1}:\n 0.5\n 1.5\n```\nHuzzah!\n\nAfter that, it's a matter of generating the points specified by\n```{julia}\nbz3(p1, p2, p3, p4)(t)(t)(t)\n```\nfor various values of `t`. I did that with a `map` and joined the results back into a single `Array`\n```{julia}\ndots = map(i -> bz3(p1, p2, p3, p4)(i)(i)(i),collect(0:0.1:1))\ndots = hcat(dots...)\ndots\n2×11 Array{Float64,2}:\n 0.5  0.47535  0.5368  0.66245  …  1.36905  1.4872  1.53815  1.5\n 1.0  1.3124   1.5312  1.6588      1.3052   1.0128  0.6436   0.2\n```\nThat was, I'd say, a success.\n\nDrunk with confidence, I wanted to try to reproduce the animation from the tweet, so I dug into the documentation. It didn't seem too bad, and I think I've managed to reproduce it pretty well\n```{julia}\nanim = @animate for t in collect(vcat(0:0.01:1,1:-0.01:0))\n    a = bz3(p1, p2, p3, p4)(t)(t)(t);\n    b1 = bz2(p1, p2, p3)(t)(t);\n    b2 = bz2(p2, p3, p4)(t)(t);\n    c1 = bz1(p1, p2)(t);\n    c2 = bz1(p2, p3)(t);\n    c3 = bz1(p3, p4)(t);\n    stars = hcat(p1, p2, p3, p4);\n    diamond1 = hcat(c1, c2);\n    diamond2 = hcat(c2, c3);\n    square = hcat(b1, b2);\n    plot(xlim = (-0.1,2.5), ylim = (-0.1,2.5), legend = false)\n    scatter!(dots[1,:], dots[2,:], markersize = 2)\n    plot!(diamond1[1,:], diamond1[2,:], markersize = 10, markershape = :diamond, color = :green)\n    plot!(diamond2[1,:], diamond2[2,:], markersize = 10, markershape = :diamond, color = :green)\n    plot!(square[1,:], square[2,:], markersize = 10, markershape = :square, color = :blue)\n    plot!(stars[1,:], stars[2,:], markersize = 10, markershape = :star, color = :purple)\n    scatter!(Tuple(a), markersize = 10, markershape = :circle, markercolor = :red)\nend\n\ngif(anim, fps = 24)\n```\n![](https://jcarroll.xyz/uploads/2022/ea5f75012f.gif)\n\nMoving the points around, I get a new version all of my own\n\n![](https://jcarroll.xyz/uploads/2022/b776bf8259.gif)\n\nI'm very happy with how these turned out, and I've learned a lot! A gist of the code to make these is hosted here: [https://gist.github.com/jonocarroll/27f9b57332424ea50ec2970e74d8e3b3](https://gist.github.com/jonocarroll/27f9b57332424ea50ec2970e74d8e3b3)\n\nIf there are better ways to do any of the steps (there surely are) please feel free to let me know!\n\nWas this fun? You Bezier ass!\n",
				"date_published": "2022-04-07T21:07:00+09:30",
				"url": "https://jcarroll.xyz/2022/04/07/interpolation-animation-in.html",
				"tags": ["Julia"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/03/25/r-challenge-contour.html",
				"title": "R challenge - contour in a matrix",
				"content_html": "<p>As part of what will hopefully become a larger post, I&rsquo;m interested in finding an R way to achieve the following: given an <code>n x n</code> matrix of zeroes with a single non-zero element of some value <code>v</code>, fill the surrounding entries such that each other element is at most one less than those surrounding it (up or down). For example, with an <code>8x8</code> matrix with a value of <code>5</code> at <code>c(5, 5)</code>, the result would be</p>\n<pre tabindex=\"0\"><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    0    0    0    1    0    0    0\n[2,]    0    0    0    1    2    1    0    0\n[3,]    0    0    1    2    3    2    1    0\n[4,]    0    0    2    3    4    3    2    1\n[5,]    1    2    3    4    5    4    3    2\n[6,]    0    1    2    3    4    3    2    1\n[7,]    0    0    1    2    3    2    1    0\n[8,]    0    0    0    1    2    1    0    0\n</code></pre><p>This is somewhat akin to imposing a contour density on top of a single peak, but I really can&rsquo;t find any suitable approaches. Convolutions came to mind, but I can&rsquo;t think of or find the appropriate kernel.</p>\n<p>Let me know if you have one!</p>\n<h2 id=\"update\">Update:</h2>\n<p>Thanks to <a href=\"https://twitter.com/yjunechoe/status/1507344665514848258?s=20&amp;t=27rn8zNl-36D-3ppsslAjw\">June Choe</a>, this code using <code>outer()</code> produces the desired matrix for a point at <code>c(vx, vy)</code> with value <code>vv</code> in a <code>n x n</code> matrix</p>\n<pre tabindex=\"0\"><code>vx &lt;- 4\nvy &lt;- 3\nvv &lt;- 5\nn &lt;- 8\nouter(1:n, 1:n, function(x, y) pmax(vv - abs(x - vx) - abs(y - vy), 0))\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    1    2    1    0    0    0    0\n[2,]    1    2    3    2    1    0    0    0\n[3,]    2    3    4    3    2    1    0    0\n[4,]    3    4    5    4    3    2    1    0\n[5,]    2    3    4    3    2    1    0    0\n[6,]    1    2    3    2    1    0    0    0\n[7,]    0    1    2    1    0    0    0    0\n[8,]    0    0    1    0    0    0    0    0\n</code></pre>",
				"content_text": "As part of what will hopefully become a larger post, I'm interested in finding an R way to achieve the following: given an `n x n` matrix of zeroes with a single non-zero element of some value `v`, fill the surrounding entries such that each other element is at most one less than those surrounding it (up or down). For example, with an `8x8` matrix with a value of `5` at `c(5, 5)`, the result would be\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    0    0    0    1    0    0    0\n[2,]    0    0    0    1    2    1    0    0\n[3,]    0    0    1    2    3    2    1    0\n[4,]    0    0    2    3    4    3    2    1\n[5,]    1    2    3    4    5    4    3    2\n[6,]    0    1    2    3    4    3    2    1\n[7,]    0    0    1    2    3    2    1    0\n[8,]    0    0    0    1    2    1    0    0\n````\n\nThis is somewhat akin to imposing a contour density on top of a single peak, but I really can't find any suitable approaches. Convolutions came to mind, but I can't think of or find the appropriate kernel.\n\nLet me know if you have one!\n\n## Update:\n\nThanks to [June Choe](https://twitter.com/yjunechoe/status/1507344665514848258?s=20&t=27rn8zNl-36D-3ppsslAjw), this code using `outer()` produces the desired matrix for a point at `c(vx, vy)` with value `vv` in a `n x n` matrix \n```\nvx <- 4\nvy <- 3\nvv <- 5\nn <- 8\nouter(1:n, 1:n, function(x, y) pmax(vv - abs(x - vx) - abs(y - vy), 0))\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    1    2    1    0    0    0    0\n[2,]    1    2    3    2    1    0    0    0\n[3,]    2    3    4    3    2    1    0    0\n[4,]    3    4    5    4    3    2    1    0\n[5,]    2    3    4    3    2    1    0    0\n[6,]    1    2    3    2    1    0    0    0\n[7,]    0    1    2    1    0    0    0    0\n[8,]    0    0    1    0    0    0    0    0\n```\n",
				"date_published": "2022-03-25T22:02:00+09:30",
				"url": "https://jcarroll.xyz/2022/03/25/r-challenge-contour.html",
				"tags": ["R"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/03/20/rowwise-optimizations-in.html",
				"title": "ByRow optimizations in Julia",
				"content_html": "<p>I&rsquo;m still fairly new to Julia, even though I&rsquo;ve been trying to learn it for a few years. It&rsquo;s <em>extremely</em> powerful (fast, expressive, &hellip; whatever metric you want to use) but with that comes some complexity.</p>\n<p>I saw <a href=\"https://bkamins.github.io/julialang/2022/02/25/anyall.html\">this post</a> in my feed and it seemed like a great bite-sized chunk of code to learn from. I <em>think</em> I understand everything that&rsquo;s happening, even if I certainly couldn&rsquo;t write that myself, with one exception.</p>\n<p>The connection that for <code>Bool</code> data, <code>all()</code> is equivalent to <code>minimum()</code> (it&rsquo;s false as soon as there is one 0, otherwise it&rsquo;s true) and <code>any()</code> is equivalent to <code>maximum()</code> (if there&rsquo;s a 1 it&rsquo;s true) took me a moment, but seems pretty cool. That wasn&rsquo;t the problem I had.</p>\n<p>The bit that surprised me was that for <code>ByRow</code> calculations on a <code>DataFrame</code>, <code>minimum()</code> is <strong>faster</strong> than <code>all()</code>. The reason this is so surprising for me is that I understand <code>all()</code> from an R-perspective and my understanding was that <code>all()</code> could short-circuit because as soon as it sees a <code>FALSE</code> it can ignore any other values - the result is guaranteed to be <code>FALSE</code> (yes, yes, up to missingness). Surely, a calculation of <code>minimum()</code> needs to evaluate every value at least once (?). Where this might (must?) fall apart is that I&rsquo;m thinking purely of vectors. Sure enough, checking out some timings on a vector in Julia shows <code>all()</code> is near-instantaneous (after compilation)</p>\n<pre tabindex=\"0\"><code class=\"language-{julia}\" data-lang=\"{julia}\">x = rand(Bool, 100_000_000)\n\n@time all(x)\n  0.009047 seconds (218 allocations: 9.531 KiB, 99.85% compilation time)\nfalse\n\n@time all(x)\n  0.000002 seconds\nfalse\n\n@time minimum(x)\n  0.091183 seconds (85.03 k allocations: 4.461 MiB, 41.98% compilation time)\nfalse\n\n@time minimum(x)\n  0.052287 seconds\nfalse\n</code></pre><p>I get similar results, expectedly, from R</p>\n<pre tabindex=\"0\"><code class=\"language-{r}\" data-lang=\"{r}\">x &lt;- sample(c(TRUE, FALSE), 1e8, replace = TRUE)\nmicrobenchmark::microbenchmark(\n  min = max(x),\n  any = any(x),\n  times = 10\n)\n# Unit: nanoseconds\n#  expr       min        lq        mean    median        uq       max neval\n#   min 208741173 210539351 223219500.3 212388892 222673528 285974960    10\n#   any       160       187      2403.4       295      5095      7451    10\n</code></pre><p>So, what&rsquo;s going on? I <em>think</em> the answer is that we&rsquo;re not dealing with just a vector, it&rsquo;s rows from a <code>DataFrame</code>, right? Now, from the R side, that&rsquo;s complicated enough - <code>rowwise()</code> is a <a href=\"https://speakerdeck.com/jennybc/row-oriented-workflows-in-r-with-the-tidyverse\">necessary thing</a> because R stores a <code>data.frame</code> as a list of vectors representing <em>columns</em>, so extracting a row means slicing across those.</p>\n<p>I can reproduce the speedup in Julia (and honestly, I struggle to find a clean and fast way to do it in R) but the statement &ldquo;<a href=\"https://bkamins.github.io/julialang/2022/02/25/anyall.html#:~:text=This%20time%20things%20are%20very%20fast%2C%20as%20row%2Dwise%20aggregation%20for%20maximum%20and%20minimum%20is%20optimized.\">This time things are very fast, as row-wise aggregation for maximum and minimum is optimized.</a>&rdquo; got me thinking - where should I have learned that? Google isn&rsquo;t showing me any relevant results, so is this just a known thing? I can imagine that such an optimization for doing this might exist, but can anyone provide a reference or guide?? The author of the blog post used this optimization in a <a href=\"https://stackoverflow.com/a/71209103/4168169\">StackOverflow answer</a> without challenge (no reference provided) so I feel like it&rsquo;s potentially just something I should know.</p>\n",
				"content_text": "I'm still fairly new to Julia, even though I've been trying to learn it for a few years. It's *extremely* powerful (fast, expressive, ... whatever metric you want to use) but with that comes some complexity. \n\nI saw [this post](https://bkamins.github.io/julialang/2022/02/25/anyall.html) in my feed and it seemed like a great bite-sized chunk of code to learn from. I *think* I understand everything that's happening, even if I certainly couldn't write that myself, with one exception.\n\nThe connection that for `Bool` data, `all()` is equivalent to `minimum()` (it's false as soon as there is one 0, otherwise it's true) and `any()` is equivalent to `maximum()` (if there's a 1 it's true) took me a moment, but seems pretty cool. That wasn't the problem I had.\n\nThe bit that surprised me was that for `ByRow` calculations on a `DataFrame`, `minimum()` is **faster** than `all()`. The reason this is so surprising for me is that I understand `all()` from an R-perspective and my understanding was that `all()` could short-circuit because as soon as it sees a `FALSE` it can ignore any other values - the result is guaranteed to be `FALSE` (yes, yes, up to missingness). Surely, a calculation of `minimum()` needs to evaluate every value at least once (?). Where this might (must?) fall apart is that I'm thinking purely of vectors. Sure enough, checking out some timings on a vector in Julia shows `all()` is near-instantaneous (after compilation)\n```{julia}\nx = rand(Bool, 100_000_000)\n\n@time all(x)\n  0.009047 seconds (218 allocations: 9.531 KiB, 99.85% compilation time)\nfalse\n\n@time all(x)\n  0.000002 seconds\nfalse\n\n@time minimum(x)\n  0.091183 seconds (85.03 k allocations: 4.461 MiB, 41.98% compilation time)\nfalse\n\n@time minimum(x)\n  0.052287 seconds\nfalse\n```\nI get similar results, expectedly, from R\n```{r}\nx <- sample(c(TRUE, FALSE), 1e8, replace = TRUE)\nmicrobenchmark::microbenchmark(\n  min = max(x),\n  any = any(x),\n  times = 10\n)\n# Unit: nanoseconds\n#  expr       min        lq        mean    median        uq       max neval\n#   min 208741173 210539351 223219500.3 212388892 222673528 285974960    10\n#   any       160       187      2403.4       295      5095      7451    10\n```\nSo, what's going on? I *think* the answer is that we're not dealing with just a vector, it's rows from a `DataFrame`, right? Now, from the R side, that's complicated enough - `rowwise()` is a [necessary thing](https://speakerdeck.com/jennybc/row-oriented-workflows-in-r-with-the-tidyverse) because R stores a `data.frame` as a list of vectors representing *columns*, so extracting a row means slicing across those. \n\nI can reproduce the speedup in Julia (and honestly, I struggle to find a clean and fast way to do it in R) but the statement \"[This time things are very fast, as row-wise aggregation for maximum and minimum is optimized.](https://bkamins.github.io/julialang/2022/02/25/anyall.html#:~:text=This%20time%20things%20are%20very%20fast%2C%20as%20row%2Dwise%20aggregation%20for%20maximum%20and%20minimum%20is%20optimized.)\" got me thinking - where should I have learned that? Google isn't showing me any relevant results, so is this just a known thing? I can imagine that such an optimization for doing this might exist, but can anyone provide a reference or guide?? The author of the blog post used this optimization in a [StackOverflow answer](https://stackoverflow.com/a/71209103/4168169) without challenge (no reference provided) so I feel like it's potentially just something I should know.\n",
				"date_published": "2022-03-20T13:11:00+09:30",
				"url": "https://jcarroll.xyz/2022/03/20/rowwise-optimizations-in.html",
				"tags": ["R","Julia"]
			},
			{
				"id": "http://jonocarroll.micro.blog/2022/03/20/first-post-on.html",
				"title": "First post on jcarroll.xyz",
				"content_html": "<p>I like blogging, but in the spirit of lowering the resistance to getting posts out, I&rsquo;ve started a micro blog <a href=\"https://jcarroll.xyz\">jcarroll.xyz</a> where I&rsquo;ll capture shorter, less polished pieces and random thoughts / snippets.</p>\n<p>This is my first post, testing all the functionality. DNS might still take a little while, so don&rsquo;t worry if you see my full blog when you click the link.</p>\n",
				"content_text": "I like blogging, but in the spirit of lowering the resistance to getting posts out, I've started a micro blog [jcarroll.xyz](https://jcarroll.xyz) where I'll capture shorter, less polished pieces and random thoughts / snippets.\n\nThis is my first post, testing all the functionality. DNS might still take a little while, so don't worry if you see my full blog when you click the link.\n",
				"date_published": "2022-03-20T11:28:39+09:30",
				"url": "https://jcarroll.xyz/2022/03/20/first-post-on.html"
			}
	]
}
